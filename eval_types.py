# generated by datamodel-codegen:
#   filename:  eval.schema.json
#   timestamp: 2025-12-13T15:05:41+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional, Union

from pydantic import BaseModel, ConfigDict, Field


class SourceData(BaseModel):
    dataset_name: str = Field(..., description='Name of the source dataset')
    hf_repo: Optional[str] = Field(
        None, description='HuggingFace repository identifier'
    )
    hf_split: Optional[str] = Field(None, description='One of train, val or test.')
    samples_number: Optional[int] = Field(
        None, description='Number of samples in the dataset'
    )
    sample_ids: Optional[List[Union[int, str]]] = Field(
        None, description='Array of sample ids used for evaluation'
    )
    additional_details: Optional[Dict[str, Any]] = Field(
        None, description='Additional dataset info parameters'
    )


class SourceType(Enum):
    documentation = 'documentation'
    evaluation_run = 'evaluation_run'


class EvaluatorRelationship(Enum):
    first_party = 'first_party'
    third_party = 'third_party'
    collaborative = 'collaborative'
    other = 'other'


class SourceMetadata(BaseModel):
    source_name: Optional[str] = Field(
        None,
        description='Name of the source (e.g. title of the source leaderboard or name of the platform used for the evaluation).',
    )
    source_type: SourceType = Field(
        ...,
        description='Whether the data comes from a direct evaluation run or from documentation',
    )
    source_organization_name: str = Field(
        ..., description='Name of the organization that provides the data'
    )
    source_organization_url: Optional[str] = Field(
        None, description='URL for the organization that provides the data'
    )
    source_organization_logo_url: Optional[str] = Field(
        None, description='URL for the Logo for the organization that provides the data'
    )
    evaluator_relationship: EvaluatorRelationship = Field(
        ..., description='Relationship between the evaluator and the model'
    )


class ModelInfo(BaseModel):
    name: str = Field(..., description='Model name provided by evaluation source')
    id: str = Field(
        ...,
        description='Model name in HuggingFace format (e.g. meta-llama/Llama-3.1-8B-Instruct for models available on HuggingFace or openai/azure/gpt-4o-mini-2024-07-18 for closed API models)',
    )
    developer: Optional[str] = Field(
        None, description="Name of organization that provides the model (e.g. 'OpenAI')"
    )
    inference_platform: Optional[str] = Field(
        None,
        description='Name of inference platform which provides an access to models by API to run the evaluations or provides models weights to run them locally (e.g. HuggingFace, Bedrock, Together AI)',
    )
    inference_engine: Optional[str] = Field(
        None,
        description='Name of inference engine which provides an access to optimized models to use them for local evaluations (e.g. vLLM, Ollama).',
    )
    additional_details: Optional[Dict[str, Any]] = Field(
        None, description='Additional model configuration parameters'
    )


class ScoreType(Enum):
    binary = 'binary'
    continuous = 'continuous'
    levels = 'levels'


class MetricConfig(BaseModel):
    evaluation_description: Optional[str] = Field(
        None, description='Description of the evaluation'
    )
    lower_is_better: bool = Field(..., description='Whether a lower score is better')
    score_type: Optional[ScoreType] = Field(None, description='Type of score')
    level_names: Optional[List[str]] = Field(
        None, description='Names of the score levels'
    )
    level_metadata: Optional[List[str]] = Field(
        None, description='Additional Description for each Score Level'
    )
    has_unknown_level: Optional[bool] = Field(
        None,
        description='Indicates whether there is an Unknown Level - if True, then a score of -1 will be treated as Unknown',
    )
    min_score: Optional[float] = Field(
        None, description='Minimum possible score for continuous metric'
    )
    max_score: Optional[float] = Field(
        None, description='Maximum possible score for continuous metric'
    )


class ScoreDetails(BaseModel):
    score: float = Field(..., description='The score for the evaluation')
    details: Optional[Dict[str, Any]] = Field(
        None, description='Any additional details about the score'
    )


class EvaluationResult(BaseModel):
    evaluation_name: str = Field(..., description='Name of the evaluation')
    evaluation_timestamp: Optional[str] = Field(
        None, description='Timestamp for when the evaluations were run'
    )
    metric_config: MetricConfig = Field(..., description='Details about the metric')
    score_details: ScoreDetails = Field(
        ..., description='The score for the evaluation and related details'
    )
    detailed_evaluation_results_url: Optional[str] = Field(
        None, description='Link to detailed evaluation data'
    )
    generation_config: Optional[Dict[str, Any]] = None


class FullLogprob(BaseModel):
    token_id: float = Field(
        ..., description='Id of token for which we keep its logprob'
    )
    logprob: float = Field(..., description='Log probability of the token')
    decoded_token: str = Field(
        ..., description='The decoded string representation of the token'
    )


class DetailedEvaluationResultsPerSample(BaseModel):
    sample_id: str = Field(..., description='Simple sample ID')
    input: str = Field(..., description='Raw input for the model')
    prompt: Optional[str] = Field(None, description='Full prompt for the model')
    ground_truth: Union[str, List[str]] = Field(
        ...,
        description='Target response that may include one or multiple correct answers.',
    )
    response: str = Field(..., description='Response from the model')
    choices: Optional[Union[List[str], List[List[str]]]] = Field(
        None,
        description='Either an array of possible responses (list of strings) or an array of string pairs [choice, response].',
    )
    full_logprobs: Optional[List[List[FullLogprob]]] = Field(
        None, description='Full log probabilities generated for this sample'
    )


class EvaluationLog(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    schema_version: str = Field(
        ..., description='Version of the schema used for this evaluation data'
    )
    evaluation_id: str = Field(
        ...,
        description='Unique identifier for this specific evaluation run. Use eval_name/model_id/retrieved_timestamp format',
    )
    retrieved_timestamp: str = Field(
        ...,
        description='Timestamp for when this record was created - using Unix Epoch time format',
    )
    source_data: Union[List[str], SourceData] = Field(
        ...,
        description='Source of dataset used for evaluation. There are two options supported: HuggingFace dataset or url for other data source.',
    )
    source_metadata: SourceMetadata = Field(
        ..., description='Metadata about the source of the leaderboard data'
    )
    model_info: ModelInfo = Field(
        ...,
        description='Complete model specification including basic information, technical configuration and inference settings',
    )
    evaluation_results: List[EvaluationResult] = Field(
        ..., description='Array of evaluation results'
    )
    detailed_evaluation_results_per_samples: Optional[
        Union[str, List[DetailedEvaluationResultsPerSample]]
    ] = Field(
        None,
        description='Detailed eval results for all individual samples in the evaluation. This can be provided as source link or list of DetailedEvaluationResultsPerSample objects.',
    )
