# generated by datamodel-codegen:
#   filename:  eval.schema.json
#   timestamp: 2026-02-09T17:16:13+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Literal

from pydantic import BaseModel, ConfigDict, Field, conint


class SourceType(Enum):
    documentation = 'documentation'
    evaluation_run = 'evaluation_run'


class EvaluatorRelationship(Enum):
    first_party = 'first_party'
    third_party = 'third_party'
    collaborative = 'collaborative'
    other = 'other'


class SourceMetadata(BaseModel):
    source_name: str | None = Field(
        None,
        description='Name of the source (e.g. title of the source leaderboard or name of the platform used for the evaluation).',
    )
    source_type: SourceType = Field(
        ...,
        description='Whether the data comes from a direct evaluation run or from documentation',
    )
    source_organization_name: str = Field(
        ..., description='Name of the organization that provides the data'
    )
    source_organization_url: str | None = Field(
        None, description='URL for the organization that provides the data'
    )
    source_organization_logo_url: str | None = Field(
        None, description='URL for the Logo for the organization that provides the data'
    )
    evaluator_relationship: EvaluatorRelationship = Field(
        ..., description='Relationship between the evaluator and the model'
    )


class ScoreType(Enum):
    binary = 'binary'
    continuous = 'continuous'
    levels = 'levels'


class AggregationMethod(Enum):
    majority_vote = 'majority_vote'
    average = 'average'
    weighted_average = 'weighted_average'
    median = 'median'


class ConfidenceInterval(BaseModel):
    lower: float | None = Field(
        None, description='Lower bound of the confidence interval'
    )
    upper: float | None = Field(
        None, description='Upper bound of the confidence interval'
    )
    method: str | None = Field(
        None, description='How it was computed (e.g. bootstrap, holdout)'
    )


class EvalLimits(BaseModel):
    time_limit: Any | None = Field(None, description='Time limit for evaluation.')
    message_limit: Any | None = Field(None, description='Message limit for evaluation.')
    token_limit: Any | None = Field(None, description='Token limit for evaluation.')


class Sandbox(BaseModel):
    type: str | None = Field(None, description='Type of sandbox e.g. docker')
    config: str | None = Field(
        None,
        description='Config file name/path e.g. compose.yaml. TODO or full config? Not sure based on the Inspect docs',
    )


class Format(Enum):
    jsonl = 'jsonl'
    json = 'json'


class HashAlgorithm(Enum):
    sha256 = 'sha256'
    md5 = 'md5'


class DetailedEvaluationResults(BaseModel):
    format: Format | None = Field(
        None, description='Format of the detailed evaluation results'
    )
    file_path: str | None = Field(
        None, description='Path to the detailed evaluation results file'
    )
    hash_algorithm: HashAlgorithm | None = Field(
        None,
        description='Hash algorithm used for checksum and sample_hash in instance-level data',
    )
    checksum: str | None = Field(None, description='Checksum value of the file')
    total_rows: int | None = Field(
        None, description='Total number of rows in the detailed evaluation results file'
    )


class AdditionalPropertiesObject(BaseModel):
    model_config = ConfigDict(
        extra='allow',
    )


class InferenceEngine(BaseModel):
    name: str | None = Field(None, description='Name of the inference engine')
    version: str | None = Field(None, description='Version of the inference engine')


class ModelInfo(BaseModel):
    name: str = Field(..., description='Model name provided by evaluation source')
    id: str = Field(
        ...,
        description='Model name in HuggingFace format (e.g. meta-llama/Llama-3.1-8B-Instruct for models available on HuggingFace or openai/azure/gpt-4o-mini-2024-07-18 for closed API models)',
    )
    developer: str | None = Field(
        None, description="Name of organization that provides the model (e.g. 'OpenAI')"
    )
    inference_platform: str | None = Field(
        None,
        description='Name of inference platform which provides an access to models by API to run the evaluations or provides models weights to run them locally (e.g. HuggingFace, Bedrock, Together AI)',
    )
    inference_engine: InferenceEngine | None = Field(
        None,
        description='Name of inference engine which provides an access to optimized models to use them for local evaluations (e.g. vLLM, Ollama).',
    )
    additional_details: AdditionalPropertiesObject | None = None


class SourceDataUrl(BaseModel):
    model_config = ConfigDict(
        extra='allow',
    )
    dataset_name: str = Field(..., description='Name of the source dataset')
    source_type: Literal['url']
    url: list[str] = Field(
        ..., description='URL(s) for the source of the evaluation data', min_length=1
    )
    additional_details: AdditionalPropertiesObject | None = None


class SourceDataHf(BaseModel):
    model_config = ConfigDict(
        extra='allow',
    )
    dataset_name: str = Field(..., description='Name of the source dataset')
    source_type: Literal['hf_dataset']
    hf_repo: str | None = Field(None, description='HuggingFace repository identifier')
    hf_split: str | None = Field(None, description='One of train, val or test.')
    samples_number: int | None = Field(
        None, description='Number of samples in the dataset'
    )
    sample_ids: list[int | str] | None = Field(
        None, description='Array of sample ids used for evaluation'
    )
    additional_details: AdditionalPropertiesObject | None = None


class SourceDataPrivate(BaseModel):
    dataset_name: str = Field(..., description='Name of the source dataset')
    source_type: Literal['other']
    additional_details: AdditionalPropertiesObject | None = None


class ScoreDetails(BaseModel):
    score: float = Field(..., description='The score for the evaluation')
    details: AdditionalPropertiesObject | None = None
    confidence_interval: ConfidenceInterval | None = Field(
        None, description='Confidence interval for the score'
    )


class AvailableTool(BaseModel):
    name: str | None = Field(None, description='e.g. bash, calculator, ...')
    description: str | None = None
    parameters: AdditionalPropertiesObject | None = None


class AgenticEvalConfig(BaseModel):
    available_tools: list[AvailableTool] | None = Field(
        None, description='List of all available tools with their configurations'
    )
    additional_details: AdditionalPropertiesObject | None = None


class EvalPlan(BaseModel):
    name: str | None = None
    steps: list[Any] | None = Field(None, description='Array of evaluation plan steps')
    config: AdditionalPropertiesObject | None = None


class GenerationArgs(BaseModel):
    model_config = ConfigDict(
        extra='allow',
    )
    temperature: float | None = Field(None, description='Sampling temperature')
    top_p: float | None = Field(None, description='Nucleus sampling parameter')
    top_k: float | None = Field(None, description='Top-k sampling parameter')
    max_tokens: conint(ge=1) | None = Field(
        None, description='Maximum number of tokens to generate'
    )
    execution_command: str | None = Field(
        None, description='Command used to run the model to generate results'
    )
    reasoning: bool | None = Field(
        None,
        description='Whether reasoning orchain-of-thought was used to generate results',
    )
    prompt_template: str | None = Field(
        None,
        description='Input prompt template for task (should contain agentic info if needed).',
    )
    agentic_eval_config: AgenticEvalConfig | None = Field(
        None, description='General configuration for agentic evaluations.'
    )
    eval_plan: EvalPlan | None = Field(
        None,
        description='Plan (solvers) used in evaluation. Solvers are crucial parts of Inspect evaluations which can serve a wide variety of purposes like providing system prompts, prompt engineering, model generation or multi-turn dialog.',
    )
    eval_limits: EvalLimits | None = Field(
        None,
        description='Listed evaluation limits like time limit, message limit, token limit.',
    )
    sandbox: Sandbox | None = None
    max_attempts: int | None = Field(
        1, description='Maximum number of submission attempts (default 1).'
    )
    incorrect_attempt_feedback: str | None = Field(
        None, description='Feedback from the model after incorrect attempt.'
    )


class GenerationConfig(BaseModel):
    generation_args: GenerationArgs | None = Field(
        None,
        description='Parameters used to generate results - properties may vary by model type',
    )
    additional_details: AdditionalPropertiesObject | None = None


class JudgeConfig(BaseModel):
    model_info: ModelInfo
    temperature: float | None = None
    weight: float | None = Field(
        None, description="Weight of this judge's score in aggregation (used in jury)"
    )


class LlmScoring(BaseModel):
    model_config = ConfigDict(
        extra='allow',
    )
    judges: list[JudgeConfig] = Field(
        ...,
        description='LLM judge(s) - single item for judge, multiple for jury',
        min_length=1,
    )
    input_prompt: str = Field(..., description='Prompt template used for judging')
    aggregation_method: AggregationMethod | None = Field(
        None, description='How to aggregate scores when multiple judges'
    )
    expert_baseline: float | None = Field(
        None, description='Expert/human baseline score for comparison'
    )
    additional_details: AdditionalPropertiesObject | None = None


class MetricConfig(BaseModel):
    evaluation_description: str | None = Field(
        None, description='Description of the evaluation'
    )
    lower_is_better: bool = Field(..., description='Whether a lower score is better')
    score_type: ScoreType | None = Field(None, description='Type of score')
    level_names: list[str] | None = Field(None, description='Names of the score levels')
    level_metadata: list[str] | None = Field(
        None, description='Additional Description for each Score Level'
    )
    has_unknown_level: bool | None = Field(
        None,
        description='Indicates whether there is an Unknown Level - if True, then a score of -1 will be treated as Unknown',
    )
    min_score: float | None = Field(
        None, description='Minimum possible score for continuous metric'
    )
    max_score: float | None = Field(
        None, description='Maximum possible score for continuous metric'
    )
    llm_scoring: LlmScoring | None = Field(
        None, description='Configuration when LLM is used as scorer/judge'
    )


class EvaluationResult(BaseModel):
    evaluation_name: str = Field(..., description='Name of the evaluation')
    source_data: SourceDataUrl | SourceDataHf | SourceDataPrivate = Field(
        ...,
        description='Source of dataset for this evaluation: URL, HuggingFace dataset, or private/custom dataset.',
    )
    evaluation_timestamp: str | None = Field(
        None, description='Timestamp for when the evaluations were run'
    )
    metric_config: MetricConfig = Field(..., description='Details about the metric')
    score_details: ScoreDetails = Field(
        ..., description='The score for the evaluation and related details'
    )
    generation_config: GenerationConfig | None = None


class EvaluationLog(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    schema_version: str = Field(
        ..., description='Version of the schema used for this evaluation data'
    )
    evaluation_id: str = Field(
        ...,
        description='Unique identifier for this specific evaluation run. Use eval_name/model_id/retrieved_timestamp format',
    )
    evaluation_timestamp: str | None = Field(
        None, description='Timestamp for when the evaluation was run'
    )
    retrieved_timestamp: str = Field(
        ...,
        description='Timestamp for when this record was created - using Unix Epoch time format',
    )
    source_metadata: SourceMetadata = Field(
        ..., description='Metadata about the source of the leaderboard data'
    )
    model_info: ModelInfo
    evaluation_results: list[EvaluationResult] = Field(
        ..., description='Array of evaluation results'
    )
    detailed_evaluation_results: DetailedEvaluationResults | None = Field(
        None,
        description='Reference to the evaluation results for all individual samples in the evaluation',
    )
