{
  "schema_version": "0.2.0",
  "evaluation_id": "hfopenllm_v2/win10_llama3-13.45b-Instruct/1770682486.623709",
  "retrieved_timestamp": "1770682486.623709",
  "source_metadata": {
    "source_name": "HF Open LLM v2",
    "source_type": "documentation",
    "source_organization_name": "Hugging Face",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "llama3-13.45b-Instruct",
    "id": "win10/llama3-13.45b-Instruct",
    "developer": "win10",
    "inference_platform": "unknown",
    "additional_details": {
      "precision": "bfloat16",
      "architecture": "LlamaForCausalLM",
      "params_billions": 13.265
    }
  },
  "evaluation_results": [
    {
      "evaluation_name": "IFEval",
      "source_data": {
        "dataset_name": "IFEval",
        "source_type": "hf_dataset",
        "hf_repo": "google/IFEval"
      },
      "metric_config": {
        "evaluation_description": "Accuracy on IFEval",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.4144
      }
    },
    {
      "evaluation_name": "BBH",
      "source_data": {
        "dataset_name": "BBH",
        "source_type": "hf_dataset",
        "hf_repo": "SaylorTwift/bbh"
      },
      "metric_config": {
        "evaluation_description": "Accuracy on BBH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.4865
      }
    },
    {
      "evaluation_name": "MATH Level 5",
      "source_data": {
        "dataset_name": "MATH Level 5",
        "source_type": "hf_dataset",
        "hf_repo": "DigitalLearningGmbH/MATH-lighteval"
      },
      "metric_config": {
        "evaluation_description": "Exact Match on MATH Level 5",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.0242
      }
    },
    {
      "evaluation_name": "GPQA",
      "source_data": {
        "dataset_name": "GPQA",
        "source_type": "hf_dataset",
        "hf_repo": "Idavidrein/gpqa"
      },
      "metric_config": {
        "evaluation_description": "Accuracy on GPQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.2584
      }
    },
    {
      "evaluation_name": "MUSR",
      "source_data": {
        "dataset_name": "MUSR",
        "source_type": "hf_dataset",
        "hf_repo": "TAUR-Lab/MuSR"
      },
      "metric_config": {
        "evaluation_description": "Accuracy on MUSR",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.3848
      }
    },
    {
      "evaluation_name": "MMLU-PRO",
      "source_data": {
        "dataset_name": "MMLU-PRO",
        "source_type": "hf_dataset",
        "hf_repo": "TIGER-Lab/MMLU-Pro"
      },
      "metric_config": {
        "evaluation_description": "Accuracy on MMLU-PRO",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.3345
      }
    }
  ]
}