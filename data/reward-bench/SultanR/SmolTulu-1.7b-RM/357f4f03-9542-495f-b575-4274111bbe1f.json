{
  "schema_version": "0.2.0",
  "evaluation_id": "reward-bench/SultanR_SmolTulu-1.7b-RM/1766412838.146816",
  "retrieved_timestamp": "1766412838.146816",
  "source_metadata": {
    "source_name": "RewardBench",
    "source_type": "documentation",
    "source_organization_name": "Allen Institute for AI",
    "source_organization_url": "https://allenai.org",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "SultanR/SmolTulu-1.7b-RM",
    "id": "SultanR/SmolTulu-1.7b-RM",
    "developer": "SultanR",
    "additional_details": {
      "model_type": "Seq. Classifier"
    }
  },
  "evaluation_results": [
    {
      "evaluation_name": "Score",
      "metric_config": {
        "evaluation_description": "Overall RewardBench Score",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.5094
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Chat",
      "metric_config": {
        "evaluation_description": "Chat accuracy - includes easy chat subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.743
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Chat Hard",
      "metric_config": {
        "evaluation_description": "Chat Hard accuracy - includes hard chat subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.4408
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Safety",
      "metric_config": {
        "evaluation_description": "Safety accuracy - includes safety subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.5716
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Reasoning",
      "metric_config": {
        "evaluation_description": "Reasoning accuracy - includes code and math subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.2821
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    }
  ]
}
