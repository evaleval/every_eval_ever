{
  "schema_version": "0.2.0",
  "evaluation_id": "reward-bench/Ray2333_GRM-llama3-8B-sftreg/1766412838.146816",
  "retrieved_timestamp": "1766412838.146816",
  "source_metadata": {
    "source_name": "RewardBench",
    "source_type": "documentation",
    "source_organization_name": "Allen Institute for AI",
    "source_organization_url": "https://allenai.org",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Ray2333/GRM-llama3-8B-sftreg",
    "id": "Ray2333/GRM-llama3-8B-sftreg",
    "developer": "Ray2333",
    "additional_details": {
      "model_type": "Seq. Classifier"
    }
  },
  "evaluation_results": [
    {
      "evaluation_name": "Score",
      "metric_config": {
        "evaluation_description": "Overall RewardBench Score",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.8542
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Chat",
      "metric_config": {
        "evaluation_description": "Chat accuracy - includes easy chat subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.986
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Chat Hard",
      "metric_config": {
        "evaluation_description": "Chat Hard accuracy - includes hard chat subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.6776
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Safety",
      "metric_config": {
        "evaluation_description": "Safety accuracy - includes safety subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.8919
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Reasoning",
      "metric_config": {
        "evaluation_description": "Reasoning accuracy - includes code and math subsets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.9229
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    },
    {
      "evaluation_name": "Prior Sets (0.5 weight)",
      "metric_config": {
        "evaluation_description": "Prior Sets score (weighted 0.5) - includes test sets",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.7309
      },
      "source_data": {
        "dataset_name": "RewardBench",
        "source_type": "hf_dataset",
        "hf_repo": "allenai/reward-bench"
      }
    }
  ]
}
