{
  "schema_version": "0.1.0",
  "evaluation_id": "global-mmlu-lite/google_gemma-3-4b/1764290504.009719",
  "retrieved_timestamp": "1764290504.009719",
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
  ],
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "source_organization_url": "https://www.kaggle.com/organizations/cohere-labs",
    "evaluator_relationship": "third_party",
    "source_type": "documentation",
    "source_name": "Kaggle Global MMLU Lite Leaderboard"
  },
  "model_info": {
    "name": "Gemma 3 4B",
    "id": "google/gemma-3-4b",
    "developer": "Google",
    "inference_platform": "Kaggle"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite"
      },
      "score_details": {
        "score": 0.6510937500000001
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Sensitive",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Sensitive"
      },
      "score_details": {
        "score": 0.6115625
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Agnostic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Agnostic"
      },
      "score_details": {
        "score": 0.690625
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Arabic"
      },
      "score_details": {
        "score": 0.6525,
        "details": {
          "confidence_interval": 0.046664,
          "stddev": 0.046664
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task English"
      },
      "score_details": {
        "score": 0.67,
        "details": {
          "confidence_interval": 0.04608,
          "stddev": 0.04608
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Bengali"
      },
      "score_details": {
        "score": 0.68,
        "details": {
          "confidence_interval": 0.045714,
          "stddev": 0.045714
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task German"
      },
      "score_details": {
        "score": 0.6525,
        "details": {
          "confidence_interval": 0.046664,
          "stddev": 0.046664
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task French"
      },
      "score_details": {
        "score": 0.6575,
        "details": {
          "confidence_interval": 0.046505,
          "stddev": 0.046505
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Hindi"
      },
      "score_details": {
        "score": 0.6475,
        "details": {
          "confidence_interval": 0.046819,
          "stddev": 0.046819
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Indonesian"
      },
      "score_details": {
        "score": 0.6775,
        "details": {
          "confidence_interval": 0.045808,
          "stddev": 0.045808
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Italian"
      },
      "score_details": {
        "score": 0.6675,
        "details": {
          "confidence_interval": 0.046168,
          "stddev": 0.046168
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Japanese"
      },
      "score_details": {
        "score": 0.6325,
        "details": {
          "confidence_interval": 0.047247,
          "stddev": 0.047247
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Korean"
      },
      "score_details": {
        "score": 0.66,
        "details": {
          "confidence_interval": 0.046423,
          "stddev": 0.046423
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Portuguese"
      },
      "score_details": {
        "score": 0.68,
        "details": {
          "confidence_interval": 0.045714,
          "stddev": 0.045714
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Spanish"
      },
      "score_details": {
        "score": 0.6725,
        "details": {
          "confidence_interval": 0.045991,
          "stddev": 0.045991
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Swahili"
      },
      "score_details": {
        "score": 0.6075,
        "details": {
          "confidence_interval": 0.047853,
          "stddev": 0.047853
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Yoruba"
      },
      "score_details": {
        "score": 0.5825,
        "details": {
          "confidence_interval": 0.048327,
          "stddev": 0.048327
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Chinese"
      },
      "score_details": {
        "score": 0.6475,
        "details": {
          "confidence_interval": 0.046819,
          "stddev": 0.046819
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Burmese"
      },
      "score_details": {
        "score": 0.63,
        "details": {
          "confidence_interval": 0.047314,
          "stddev": 0.047314
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    }
  ]
}