{
  "schema_version": "0.0.1",
  "evaluation_id": "cohere/global-mmlu-lite/2025-11-28T00:36:05.012239Z",
  "evaluation_source": {
    "evaluation_source_name": "Kaggle Global MMLU Lite Leaderboard",
    "evaluation_source_type": "leaderboard"
  },
  "retrieved_timestamp": "2025-11-28T00:36:05.012239Z",
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
  ],
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "source_organization_url": "https://www.kaggle.com/organizations/cohere-labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Command A",
    "id": "cohere/command-a",
    "developer": "Cohere",
    "inference_platform": "Kaggle"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite"
      },
      "score_details": {
        "score": 0.838546365914787
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Sensitive",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Sensitive"
      },
      "score_details": {
        "score": 0.7993200376884423
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Agnostic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Agnostic"
      },
      "score_details": {
        "score": 0.8777732412060302
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Arabic"
      },
      "score_details": {
        "score": 0.8425,
        "details": {
          "confidence_interval": 0.035698,
          "stddev": 0.035698
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task English"
      },
      "score_details": {
        "score": 0.855,
        "details": {
          "confidence_interval": 0.034505,
          "stddev": 0.034505
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Bengali"
      },
      "score_details": {
        "score": 0.8225,
        "details": {
          "confidence_interval": 0.037444,
          "stddev": 0.037444
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task German"
      },
      "score_details": {
        "score": 0.8425,
        "details": {
          "confidence_interval": 0.035698,
          "stddev": 0.035698
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task French"
      },
      "score_details": {
        "score": 0.8375,
        "details": {
          "confidence_interval": 0.036152,
          "stddev": 0.036152
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Hindi"
      },
      "score_details": {
        "score": 0.842105,
        "details": {
          "confidence_interval": 0.035779,
          "stddev": 0.035779
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Indonesian"
      },
      "score_details": {
        "score": 0.854637,
        "details": {
          "confidence_interval": 0.034584,
          "stddev": 0.034584
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Italian"
      },
      "score_details": {
        "score": 0.8375,
        "details": {
          "confidence_interval": 0.036152,
          "stddev": 0.036152
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Japanese"
      },
      "score_details": {
        "score": 0.845,
        "details": {
          "confidence_interval": 0.035466,
          "stddev": 0.035466
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Korean"
      },
      "score_details": {
        "score": 0.85,
        "details": {
          "confidence_interval": 0.034992,
          "stddev": 0.034992
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Portuguese"
      },
      "score_details": {
        "score": 0.84,
        "details": {
          "confidence_interval": 0.035927,
          "stddev": 0.035927
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Spanish"
      },
      "score_details": {
        "score": 0.8525,
        "details": {
          "confidence_interval": 0.034751,
          "stddev": 0.034751
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Swahili"
      },
      "score_details": {
        "score": 0.8275,
        "details": {
          "confidence_interval": 0.037025,
          "stddev": 0.037025
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Yoruba"
      },
      "score_details": {
        "score": 0.815,
        "details": {
          "confidence_interval": 0.038052,
          "stddev": 0.038052
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Chinese"
      },
      "score_details": {
        "score": 0.835,
        "details": {
          "confidence_interval": 0.036375,
          "stddev": 0.036375
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Burmese"
      },
      "score_details": {
        "score": 0.8175,
        "details": {
          "confidence_interval": 0.037852,
          "stddev": 0.037852
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    }
  ]
}