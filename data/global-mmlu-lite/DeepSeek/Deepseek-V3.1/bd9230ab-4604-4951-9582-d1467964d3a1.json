{
  "schema_version": "0.0.1",
  "evaluation_id": "deepseek/global-mmlu-lite/2025-11-28T00:36:05.013462Z",
  "evaluation_source": {
    "evaluation_source_name": "Kaggle Global MMLU Lite Leaderboard",
    "evaluation_source_type": "leaderboard"
  },
  "retrieved_timestamp": "2025-11-28T00:36:05.013462Z",
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
  ],
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "source_organization_url": "https://www.kaggle.com/organizations/cohere-labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Deepseek V3.1",
    "id": "deepseek/deepseek-v31",
    "developer": "DeepSeek",
    "inference_platform": "Kaggle"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite"
      },
      "score_details": {
        "score": 0.8043661366877002
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Sensitive",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Sensitive"
      },
      "score_details": {
        "score": 0.7793102525957433
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Agnostic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Agnostic"
      },
      "score_details": {
        "score": 0.8294756436687251
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Arabic"
      },
      "score_details": {
        "score": 0.805,
        "details": {
          "confidence_interval": 0.038827,
          "stddev": 0.038827
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task English"
      },
      "score_details": {
        "score": 0.825,
        "details": {
          "confidence_interval": 0.037236,
          "stddev": 0.037236
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Bengali"
      },
      "score_details": {
        "score": 0.815657,
        "details": {
          "confidence_interval": 0.038192,
          "stddev": 0.038192
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task German"
      },
      "score_details": {
        "score": 0.7925,
        "details": {
          "confidence_interval": 0.03974,
          "stddev": 0.03974
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task French"
      },
      "score_details": {
        "score": 0.8175,
        "details": {
          "confidence_interval": 0.037852,
          "stddev": 0.037852
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Hindi"
      },
      "score_details": {
        "score": 0.756892,
        "details": {
          "confidence_interval": 0.04209,
          "stddev": 0.04209
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Indonesian"
      },
      "score_details": {
        "score": 0.776382,
        "details": {
          "confidence_interval": 0.040935,
          "stddev": 0.040935
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Italian"
      },
      "score_details": {
        "score": 0.8075,
        "details": {
          "confidence_interval": 0.038637,
          "stddev": 0.038637
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Japanese"
      },
      "score_details": {
        "score": 0.831169,
        "details": {
          "confidence_interval": 0.037419,
          "stddev": 0.037419
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Korean"
      },
      "score_details": {
        "score": 0.8125,
        "details": {
          "confidence_interval": 0.03825,
          "stddev": 0.03825
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Portuguese"
      },
      "score_details": {
        "score": 0.824561,
        "details": {
          "confidence_interval": 0.037319,
          "stddev": 0.037319
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Spanish"
      },
      "score_details": {
        "score": 0.8125,
        "details": {
          "confidence_interval": 0.03825,
          "stddev": 0.03825
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Swahili"
      },
      "score_details": {
        "score": 0.801008,
        "details": {
          "confidence_interval": 0.039273,
          "stddev": 0.039273
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Yoruba"
      },
      "score_details": {
        "score": 0.783069,
        "details": {
          "confidence_interval": 0.041549,
          "stddev": 0.041549
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Chinese"
      },
      "score_details": {
        "score": 0.816121,
        "details": {
          "confidence_interval": 0.038106,
          "stddev": 0.038106
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Burmese"
      },
      "score_details": {
        "score": 0.7925,
        "details": {
          "confidence_interval": 0.03974,
          "stddev": 0.03974
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    }
  ]
}