{
  "schema_version": "0.0.1",
  "evaluation_id": "openai/global-mmlu-lite/2025-11-28T00:36:05.014679Z",
  "evaluation_source": {
    "evaluation_source_name": "Kaggle Global MMLU Lite Leaderboard",
    "evaluation_source_type": "leaderboard"
  },
  "retrieved_timestamp": "2025-11-28T00:36:05.014679Z",
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
  ],
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "source_organization_url": "https://www.kaggle.com/organizations/cohere-labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "o3 mini",
    "id": "openai/o3-mini",
    "developer": "OpenAI",
    "inference_platform": "Kaggle"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite"
      },
      "score_details": {
        "score": 0.7799999999999999
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Sensitive",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Sensitive"
      },
      "score_details": {
        "score": 0.7650000000000001
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Agnostic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Agnostic"
      },
      "score_details": {
        "score": 0.795
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Arabic"
      },
      "score_details": {
        "score": 0.7725,
        "details": {
          "confidence_interval": 0.041083,
          "stddev": 0.041083
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task English"
      },
      "score_details": {
        "score": 0.8025,
        "details": {
          "confidence_interval": 0.039014,
          "stddev": 0.039014
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Bengali"
      },
      "score_details": {
        "score": 0.77,
        "details": {
          "confidence_interval": 0.041241,
          "stddev": 0.041241
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task German"
      },
      "score_details": {
        "score": 0.7525,
        "details": {
          "confidence_interval": 0.042292,
          "stddev": 0.042292
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task French"
      },
      "score_details": {
        "score": 0.74,
        "details": {
          "confidence_interval": 0.042985,
          "stddev": 0.042985
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Hindi"
      },
      "score_details": {
        "score": 0.7525,
        "details": {
          "confidence_interval": 0.042292,
          "stddev": 0.042292
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Indonesian"
      },
      "score_details": {
        "score": 0.7425,
        "details": {
          "confidence_interval": 0.04285,
          "stddev": 0.04285
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Italian"
      },
      "score_details": {
        "score": 0.8,
        "details": {
          "confidence_interval": 0.039199,
          "stddev": 0.039199
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Japanese"
      },
      "score_details": {
        "score": 0.81,
        "details": {
          "confidence_interval": 0.038445,
          "stddev": 0.038445
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Korean"
      },
      "score_details": {
        "score": 0.8075,
        "details": {
          "confidence_interval": 0.038637,
          "stddev": 0.038637
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Portuguese"
      },
      "score_details": {
        "score": 0.7975,
        "details": {
          "confidence_interval": 0.039382,
          "stddev": 0.039382
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Spanish"
      },
      "score_details": {
        "score": 0.775,
        "details": {
          "confidence_interval": 0.040922,
          "stddev": 0.040922
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Swahili"
      },
      "score_details": {
        "score": 0.765,
        "details": {
          "confidence_interval": 0.041551,
          "stddev": 0.041551
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Yoruba"
      },
      "score_details": {
        "score": 0.7725,
        "details": {
          "confidence_interval": 0.041083,
          "stddev": 0.041083
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Chinese"
      },
      "score_details": {
        "score": 0.8125,
        "details": {
          "confidence_interval": 0.03825,
          "stddev": 0.03825
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Burmese"
      },
      "score_details": {
        "score": 0.8075,
        "details": {
          "confidence_interval": 0.038637,
          "stddev": 0.038637
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    }
  ]
}