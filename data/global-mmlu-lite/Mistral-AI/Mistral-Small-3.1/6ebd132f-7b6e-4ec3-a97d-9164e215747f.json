{
  "schema_version": "0.0.1",
  "evaluation_id": "mistral-ai/global-mmlu-lite/2025-11-28T00:36:05.014106Z",
  "evaluation_source": {
    "evaluation_source_name": "Kaggle Global MMLU Lite Leaderboard",
    "evaluation_source_type": "leaderboard"
  },
  "retrieved_timestamp": "2025-11-28T00:36:05.014106Z",
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
  ],
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "source_organization_url": "https://www.kaggle.com/organizations/cohere-labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Mistral Small 3.1",
    "id": "mistral-ai/mistral-small-31",
    "developer": "Mistral AI",
    "inference_platform": "Kaggle"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite"
      },
      "score_details": {
        "score": 0.78515625
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Sensitive",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Sensitive"
      },
      "score_details": {
        "score": 0.7537499999999999
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Global MMLU Lite Culturally Agnostic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Global MMLU Lite Culturally Agnostic"
      },
      "score_details": {
        "score": 0.8165625
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Arabic"
      },
      "score_details": {
        "score": 0.7875,
        "details": {
          "confidence_interval": 0.040089,
          "stddev": 0.040089
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task English"
      },
      "score_details": {
        "score": 0.8,
        "details": {
          "confidence_interval": 0.039199,
          "stddev": 0.039199
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Bengali"
      },
      "score_details": {
        "score": 0.7725,
        "details": {
          "confidence_interval": 0.041083,
          "stddev": 0.041083
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task German"
      },
      "score_details": {
        "score": 0.7975,
        "details": {
          "confidence_interval": 0.039382,
          "stddev": 0.039382
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task French"
      },
      "score_details": {
        "score": 0.8,
        "details": {
          "confidence_interval": 0.039199,
          "stddev": 0.039199
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Hindi"
      },
      "score_details": {
        "score": 0.795,
        "details": {
          "confidence_interval": 0.039562,
          "stddev": 0.039562
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Indonesian"
      },
      "score_details": {
        "score": 0.785,
        "details": {
          "confidence_interval": 0.04026,
          "stddev": 0.04026
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Italian"
      },
      "score_details": {
        "score": 0.805,
        "details": {
          "confidence_interval": 0.038827,
          "stddev": 0.038827
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Japanese"
      },
      "score_details": {
        "score": 0.77,
        "details": {
          "confidence_interval": 0.041241,
          "stddev": 0.041241
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Korean"
      },
      "score_details": {
        "score": 0.79,
        "details": {
          "confidence_interval": 0.039915,
          "stddev": 0.039915
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Portuguese"
      },
      "score_details": {
        "score": 0.7925,
        "details": {
          "confidence_interval": 0.03974,
          "stddev": 0.03974
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Spanish"
      },
      "score_details": {
        "score": 0.7825,
        "details": {
          "confidence_interval": 0.040429,
          "stddev": 0.040429
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Swahili"
      },
      "score_details": {
        "score": 0.775,
        "details": {
          "confidence_interval": 0.040922,
          "stddev": 0.040922
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Yoruba"
      },
      "score_details": {
        "score": 0.735,
        "details": {
          "confidence_interval": 0.04325,
          "stddev": 0.04325
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Chinese"
      },
      "score_details": {
        "score": 0.7925,
        "details": {
          "confidence_interval": 0.03974,
          "stddev": 0.03974
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0,
        "evaluation_description": "Global MMLU Lite accuracy for task Burmese"
      },
      "score_details": {
        "score": 0.7825,
        "details": {
          "confidence_interval": 0.040429,
          "stddev": 0.040429
        }
      },
      "detailed_evaluation_results_url": "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite"
    }
  ]
}