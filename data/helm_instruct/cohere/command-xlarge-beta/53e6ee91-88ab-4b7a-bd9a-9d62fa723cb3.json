{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_instruct/cohere_command-xlarge-beta/1765639043.462613",
  "retrieved_timestamp": "1765639043.462613",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/instruct/benchmark_output/releases/v1.0.0/groups/instruction_following.json"
  ],
  "source_metadata": {
    "source_name": "helm_instruct",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Cohere Command beta (52.4B)",
    "id": "cohere/command-xlarge-beta",
    "developer": "cohere",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.089,
        "details": {
          "description": null,
          "tab": "Instruction Following"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "Anthropic RLHF dataset - Helpfulness",
      "metric_config": {
        "evaluation_description": "The dialogue datasets released by Anthropic to facilitate research in model helpfulness and harmlessness ([Bai et al., 2022](https://arxiv.org/pdf/2204.05862.pdf); [Ganguli et al., 2022](https://arxiv.org/pdf/2209.07858.pdf)). We only use the first utterance of each dialogue.\n\nHelpfulness: Whether the model appears to do what it is instructed to.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.523,
        "details": {
          "description": "min=2.43, mean=3.523, max=4.21, sum=28.185 (8)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "subset": [
          "hh",
          "hh",
          "hh",
          "hh",
          "red_team",
          "red_team",
          "red_team",
          "red_team"
        ],
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale",
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Anthropic RLHF dataset - Understandability",
      "metric_config": {
        "evaluation_description": "The dialogue datasets released by Anthropic to facilitate research in model helpfulness and harmlessness ([Bai et al., 2022](https://arxiv.org/pdf/2204.05862.pdf); [Ganguli et al., 2022](https://arxiv.org/pdf/2209.07858.pdf)). We only use the first utterance of each dialogue.\n\nUnderstandability: Whether the answer is easy to comprehend.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.836,
        "details": {
          "description": "min=4.68, mean=4.836, max=4.94, sum=38.69 (8)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "subset": [
          "hh",
          "hh",
          "hh",
          "hh",
          "red_team",
          "red_team",
          "red_team",
          "red_team"
        ],
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale",
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Anthropic RLHF dataset - Completeness",
      "metric_config": {
        "evaluation_description": "The dialogue datasets released by Anthropic to facilitate research in model helpfulness and harmlessness ([Bai et al., 2022](https://arxiv.org/pdf/2204.05862.pdf); [Ganguli et al., 2022](https://arxiv.org/pdf/2209.07858.pdf)). We only use the first utterance of each dialogue.\n\nCompleteness: Whether the response provides a comprehensive answer to the question or request.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.656,
        "details": {
          "description": "min=2.9, mean=3.656, max=4.145, sum=29.25 (8)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "subset": [
          "hh",
          "hh",
          "hh",
          "hh",
          "red_team",
          "red_team",
          "red_team",
          "red_team"
        ],
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale",
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Anthropic RLHF dataset - Conciseness",
      "metric_config": {
        "evaluation_description": "The dialogue datasets released by Anthropic to facilitate research in model helpfulness and harmlessness ([Bai et al., 2022](https://arxiv.org/pdf/2204.05862.pdf); [Ganguli et al., 2022](https://arxiv.org/pdf/2209.07858.pdf)). We only use the first utterance of each dialogue.\n\nConciseness: Whether the response is concise and to the point, without unnecessary repetition or elaboration.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.973,
        "details": {
          "description": "min=3.27, mean=3.973, max=4.545, sum=31.78 (8)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "subset": [
          "hh",
          "hh",
          "hh",
          "hh",
          "red_team",
          "red_team",
          "red_team",
          "red_team"
        ],
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale",
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Anthropic RLHF dataset - Harmlessness",
      "metric_config": {
        "evaluation_description": "The dialogue datasets released by Anthropic to facilitate research in model helpfulness and harmlessness ([Bai et al., 2022](https://arxiv.org/pdf/2204.05862.pdf); [Ganguli et al., 2022](https://arxiv.org/pdf/2209.07858.pdf)). We only use the first utterance of each dialogue.\n\nHarmlessness: Whether the response avoids using toxic language or helping the user with harmful goals.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.214,
        "details": {
          "description": "min=3.38, mean=4.214, max=4.92, sum=33.715 (8)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "subset": [
          "hh",
          "hh",
          "hh",
          "hh",
          "red_team",
          "red_team",
          "red_team",
          "red_team"
        ],
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale",
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Best ChatGPT Prompts - Helpfulness",
      "metric_config": {
        "evaluation_description": "A list of \u201cbest ChatGPT prompts to power your workflow\u201d summarized by [GRIDFITI](https://gridfiti.com/best-chatgpt-prompts/).\n\nHelpfulness: Whether the model appears to do what it is instructed to.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.909,
        "details": {
          "description": "min=3.68, mean=3.909, max=4.2, sum=15.635 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "path": "src_helm_benchmark_scenarios_best_chatgpt_prompts.yaml",
        "tags": "",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Best ChatGPT Prompts - Understandability",
      "metric_config": {
        "evaluation_description": "A list of \u201cbest ChatGPT prompts to power your workflow\u201d summarized by [GRIDFITI](https://gridfiti.com/best-chatgpt-prompts/).\n\nUnderstandability: Whether the answer is easy to comprehend.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.815,
        "details": {
          "description": "min=4.685, mean=4.815, max=4.9, sum=19.26 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "path": "src_helm_benchmark_scenarios_best_chatgpt_prompts.yaml",
        "tags": "",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Best ChatGPT Prompts - Completeness",
      "metric_config": {
        "evaluation_description": "A list of \u201cbest ChatGPT prompts to power your workflow\u201d summarized by [GRIDFITI](https://gridfiti.com/best-chatgpt-prompts/).\n\nCompleteness: Whether the response provides a comprehensive answer to the question or request.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.993,
        "details": {
          "description": "min=3.75, mean=3.993, max=4.1, sum=15.97 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "path": "src_helm_benchmark_scenarios_best_chatgpt_prompts.yaml",
        "tags": "",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Best ChatGPT Prompts - Conciseness",
      "metric_config": {
        "evaluation_description": "A list of \u201cbest ChatGPT prompts to power your workflow\u201d summarized by [GRIDFITI](https://gridfiti.com/best-chatgpt-prompts/).\n\nConciseness: Whether the response is concise and to the point, without unnecessary repetition or elaboration.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.019,
        "details": {
          "description": "min=3.51, mean=4.019, max=4.53, sum=16.075 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "path": "src_helm_benchmark_scenarios_best_chatgpt_prompts.yaml",
        "tags": "",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Best ChatGPT Prompts - Harmlessness",
      "metric_config": {
        "evaluation_description": "A list of \u201cbest ChatGPT prompts to power your workflow\u201d summarized by [GRIDFITI](https://gridfiti.com/best-chatgpt-prompts/).\n\nHarmlessness: Whether the response avoids using toxic language or helping the user with harmful goals.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.988,
        "details": {
          "description": "min=4.98, mean=4.988, max=5, sum=19.95 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "path": "src_helm_benchmark_scenarios_best_chatgpt_prompts.yaml",
        "tags": "",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Koala test dataset - Helpfulness",
      "metric_config": {
        "evaluation_description": "The test dataset from the [Koala paper](https://bair.berkeley.edu/blog/2023/04/03/koala/) for evaluating instruction-following models.\n\nHelpfulness: Whether the model appears to do what it is instructed to.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.694,
        "details": {
          "description": "min=3.214, mean=3.694, max=4.117, sum=14.776 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Koala test dataset - Understandability",
      "metric_config": {
        "evaluation_description": "The test dataset from the [Koala paper](https://bair.berkeley.edu/blog/2023/04/03/koala/) for evaluating instruction-following models.\n\nUnderstandability: Whether the answer is easy to comprehend.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.695,
        "details": {
          "description": "min=4.629, mean=4.695, max=4.755, sum=18.782 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Koala test dataset - Completeness",
      "metric_config": {
        "evaluation_description": "The test dataset from the [Koala paper](https://bair.berkeley.edu/blog/2023/04/03/koala/) for evaluating instruction-following models.\n\nCompleteness: Whether the response provides a comprehensive answer to the question or request.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.733,
        "details": {
          "description": "min=3.235, mean=3.733, max=4.02, sum=14.931 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Koala test dataset - Conciseness",
      "metric_config": {
        "evaluation_description": "The test dataset from the [Koala paper](https://bair.berkeley.edu/blog/2023/04/03/koala/) for evaluating instruction-following models.\n\nConciseness: Whether the response is concise and to the point, without unnecessary repetition or elaboration.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.719,
        "details": {
          "description": "min=3.173, mean=3.719, max=4.26, sum=14.874 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Koala test dataset - Harmlessness",
      "metric_config": {
        "evaluation_description": "The test dataset from the [Koala paper](https://bair.berkeley.edu/blog/2023/04/03/koala/) for evaluating instruction-following models.\n\nHarmlessness: Whether the response avoids using toxic language or helping the user with harmful goals.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.969,
        "details": {
          "description": "min=4.936, mean=4.969, max=5, sum=19.874 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Open Assistant - Helpfulness",
      "metric_config": {
        "evaluation_description": "LAION\u2019s OpenAssistant Conversations Dataset (OASST1) that consists of 66,497 conversation trees ([K\u00f6pf et al., 2023](https://openreview.net/forum?id=VSJotgbPHF)). We only use the initial prompt in each conversation.\n\nHelpfulness: Whether the model appears to do what it is instructed to.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.593,
        "details": {
          "description": "min=3.3, mean=3.593, max=4.145, sum=14.374 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "language": "en",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Open Assistant - Understandability",
      "metric_config": {
        "evaluation_description": "LAION\u2019s OpenAssistant Conversations Dataset (OASST1) that consists of 66,497 conversation trees ([K\u00f6pf et al., 2023](https://openreview.net/forum?id=VSJotgbPHF)). We only use the initial prompt in each conversation.\n\nUnderstandability: Whether the answer is easy to comprehend.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.684,
        "details": {
          "description": "min=4.616, mean=4.684, max=4.74, sum=18.736 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "language": "en",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Open Assistant - Completeness",
      "metric_config": {
        "evaluation_description": "LAION\u2019s OpenAssistant Conversations Dataset (OASST1) that consists of 66,497 conversation trees ([K\u00f6pf et al., 2023](https://openreview.net/forum?id=VSJotgbPHF)). We only use the initial prompt in each conversation.\n\nCompleteness: Whether the response provides a comprehensive answer to the question or request.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.672,
        "details": {
          "description": "min=3.28, mean=3.672, max=4.07, sum=14.69 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "language": "en",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Open Assistant - Conciseness",
      "metric_config": {
        "evaluation_description": "LAION\u2019s OpenAssistant Conversations Dataset (OASST1) that consists of 66,497 conversation trees ([K\u00f6pf et al., 2023](https://openreview.net/forum?id=VSJotgbPHF)). We only use the initial prompt in each conversation.\n\nConciseness: Whether the response is concise and to the point, without unnecessary repetition or elaboration.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.917,
        "details": {
          "description": "min=3.33, mean=3.917, max=4.348, sum=15.668 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "language": "en",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Open Assistant - Harmlessness",
      "metric_config": {
        "evaluation_description": "LAION\u2019s OpenAssistant Conversations Dataset (OASST1) that consists of 66,497 conversation trees ([K\u00f6pf et al., 2023](https://openreview.net/forum?id=VSJotgbPHF)). We only use the initial prompt in each conversation.\n\nHarmlessness: Whether the response avoids using toxic language or helping the user with harmful goals.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.967,
        "details": {
          "description": "min=4.955, mean=4.967, max=5, sum=19.87 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "language": "en",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Self Instruct - Helpfulness",
      "metric_config": {
        "evaluation_description": "The manually-curated instructions from the Self-Instruct paper ([Wang et al., 2023](https://aclanthology.org/2023.acl-long.754.pdf)).\n\nHelpfulness: Whether the model appears to do what it is instructed to.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.663,
        "details": {
          "description": "min=3.51, mean=3.663, max=3.815, sum=14.651 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Self Instruct - Understandability",
      "metric_config": {
        "evaluation_description": "The manually-curated instructions from the Self-Instruct paper ([Wang et al., 2023](https://aclanthology.org/2023.acl-long.754.pdf)).\n\nUnderstandability: Whether the answer is easy to comprehend.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.723,
        "details": {
          "description": "min=4.54, mean=4.723, max=4.86, sum=18.892 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Self Instruct - Completeness",
      "metric_config": {
        "evaluation_description": "The manually-curated instructions from the Self-Instruct paper ([Wang et al., 2023](https://aclanthology.org/2023.acl-long.754.pdf)).\n\nCompleteness: Whether the response provides a comprehensive answer to the question or request.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.694,
        "details": {
          "description": "min=3.39, mean=3.694, max=4.01, sum=14.776 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Self Instruct - Conciseness",
      "metric_config": {
        "evaluation_description": "The manually-curated instructions from the Self-Instruct paper ([Wang et al., 2023](https://aclanthology.org/2023.acl-long.754.pdf)).\n\nConciseness: Whether the response is concise and to the point, without unnecessary repetition or elaboration.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.986,
        "details": {
          "description": "min=3.6, mean=3.986, max=4.348, sum=15.943 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Self Instruct - Harmlessness",
      "metric_config": {
        "evaluation_description": "The manually-curated instructions from the Self-Instruct paper ([Wang et al., 2023](https://aclanthology.org/2023.acl-long.754.pdf)).\n\nHarmlessness: Whether the response avoids using toxic language or helping the user with harmful goals.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.971,
        "details": {
          "description": "min=4.955, mean=4.971, max=5, sum=19.885 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Vicuna - Helpfulness",
      "metric_config": {
        "evaluation_description": "The set of prompts used by the [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) team to evaluate instruction-following models.\n\nHelpfulness: Whether the model appears to do what it is instructed to.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.917,
        "details": {
          "description": "min=3.625, mean=3.917, max=4.444, sum=15.669 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "category": "all",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Vicuna - Understandability",
      "metric_config": {
        "evaluation_description": "The set of prompts used by the [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) team to evaluate instruction-following models.\n\nUnderstandability: Whether the answer is easy to comprehend.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.869,
        "details": {
          "description": "min=4.831, mean=4.869, max=4.9, sum=19.475 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "category": "all",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Vicuna - Completeness",
      "metric_config": {
        "evaluation_description": "The set of prompts used by the [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) team to evaluate instruction-following models.\n\nCompleteness: Whether the response provides a comprehensive answer to the question or request.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.048,
        "details": {
          "description": "min=3.694, mean=4.048, max=4.45, sum=16.194 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "category": "all",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Vicuna - Conciseness",
      "metric_config": {
        "evaluation_description": "The set of prompts used by the [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) team to evaluate instruction-following models.\n\nConciseness: Whether the response is concise and to the point, without unnecessary repetition or elaboration.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.089,
        "details": {
          "description": "min=3.612, mean=4.089, max=4.562, sum=16.356 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "category": "all",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    },
    {
      "evaluation_name": "Vicuna - Harmlessness",
      "metric_config": {
        "evaluation_description": "The set of prompts used by the [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) team to evaluate instruction-following models.\n\nHarmlessness: Whether the response avoids using toxic language or helping the user with harmful goals.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.995,
        "details": {
          "description": "min=4.981, mean=4.995, max=5, sum=19.981 (4)",
          "tab": "Instruction Following"
        }
      },
      "generation_config": {
        "category": "all",
        "evaluator": [
          "claude",
          "gpt4",
          "mturk",
          "scale"
        ]
      }
    }
  ]
}
