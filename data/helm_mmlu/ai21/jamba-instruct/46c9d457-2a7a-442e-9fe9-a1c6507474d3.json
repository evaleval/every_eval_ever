{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_mmlu/ai21_jamba-instruct/1767656407.235096",
  "retrieved_timestamp": "1767656407.235096",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
  ],
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Jamba Instruct",
    "id": "ai21/jamba-instruct",
    "developer": "ai21",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.659,
        "details": {
          "description": "min=0.341, mean=0.659, max=0.91, sum=75.114 (114)",
          "tab": "Accuracy",
          "MMLU All Subjects - Observed inference time (s)": {
            "description": "min=0.233, mean=0.277, max=0.519, sum=31.585 (114)",
            "tab": "Efficiency",
            "score": 0.2770578114829593
          },
          "MMLU All Subjects - # eval": {
            "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
            "tab": "General information",
            "score": 246.35087719298247
          },
          "MMLU All Subjects - # train": {
            "description": "min=5, mean=5, max=5, sum=570 (114)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU All Subjects - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU All Subjects - # prompt tokens": {
            "description": "min=223.731, mean=490.686, max=2081.679, sum=55938.26 (114)",
            "tab": "General information",
            "score": 490.6864895752317
          },
          "MMLU All Subjects - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - EM",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.36,
        "details": {
          "description": "min=0.36, mean=0.36, max=0.36, sum=0.72 (2)",
          "tab": "Accuracy",
          "Abstract Algebra - Observed inference time (s)": {
            "description": "min=0.275, mean=0.275, max=0.275, sum=0.55 (2)",
            "tab": "Efficiency",
            "score": 0.27479029655456544
          },
          "Abstract Algebra - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Abstract Algebra - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Abstract Algebra - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Abstract Algebra - # prompt tokens": {
            "description": "min=373.44, mean=373.44, max=373.44, sum=746.88 (2)",
            "tab": "General information",
            "score": 373.44
          },
          "Abstract Algebra - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - EM",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.615,
        "details": {
          "description": "min=0.615, mean=0.615, max=0.615, sum=1.23 (2)",
          "tab": "Accuracy",
          "Anatomy - Observed inference time (s)": {
            "description": "min=0.236, mean=0.236, max=0.236, sum=0.473 (2)",
            "tab": "Efficiency",
            "score": 0.2363892325648555
          },
          "Anatomy - # eval": {
            "description": "min=135, mean=135, max=135, sum=270 (2)",
            "tab": "General information",
            "score": 135.0
          },
          "Anatomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Anatomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Anatomy - # prompt tokens": {
            "description": "min=270.2, mean=270.2, max=270.2, sum=540.4 (2)",
            "tab": "General information",
            "score": 270.2
          },
          "Anatomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Physics - EM",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.422,
        "details": {
          "description": "min=0.422, mean=0.422, max=0.422, sum=0.843 (2)",
          "tab": "Accuracy",
          "College Chemistry - Observed inference time (s)": {
            "description": "min=0.275, mean=0.275, max=0.275, sum=0.55 (2)",
            "tab": "Efficiency",
            "score": 0.2747657370567322
          },
          "College Biology - Observed inference time (s)": {
            "description": "min=0.26, mean=0.26, max=0.26, sum=0.519 (2)",
            "tab": "Efficiency",
            "score": 0.2595776534742779
          },
          "College Computer Science - Observed inference time (s)": {
            "description": "min=0.294, mean=0.294, max=0.294, sum=0.588 (2)",
            "tab": "Efficiency",
            "score": 0.2938127589225769
          },
          "College Mathematics - Observed inference time (s)": {
            "description": "min=0.269, mean=0.269, max=0.269, sum=0.538 (2)",
            "tab": "Efficiency",
            "score": 0.26912292957305906
          },
          "College Medicine - Observed inference time (s)": {
            "description": "min=0.309, mean=0.309, max=0.309, sum=0.618 (2)",
            "tab": "Efficiency",
            "score": 0.30890216579327007
          },
          "College Physics - Observed inference time (s)": {
            "description": "min=0.374, mean=0.374, max=0.374, sum=0.749 (2)",
            "tab": "Efficiency",
            "score": 0.374276315464693
          },
          "College Chemistry - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Chemistry - # prompt tokens": {
            "description": "min=549.4, mean=549.4, max=549.4, sum=1098.8 (2)",
            "tab": "General information",
            "score": 549.4
          },
          "College Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # eval": {
            "description": "min=144, mean=144, max=144, sum=288 (2)",
            "tab": "General information",
            "score": 144.0
          },
          "College Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # prompt tokens": {
            "description": "min=363.431, mean=363.431, max=363.431, sum=726.861 (2)",
            "tab": "General information",
            "score": 363.43055555555554
          },
          "College Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # prompt tokens": {
            "description": "min=720.67, mean=720.67, max=720.67, sum=1441.34 (2)",
            "tab": "General information",
            "score": 720.67
          },
          "College Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # prompt tokens": {
            "description": "min=535.22, mean=535.22, max=535.22, sum=1070.44 (2)",
            "tab": "General information",
            "score": 535.22
          },
          "College Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # eval": {
            "description": "min=173, mean=173, max=173, sum=346 (2)",
            "tab": "General information",
            "score": 173.0
          },
          "College Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # prompt tokens": {
            "description": "min=397.855, mean=397.855, max=397.855, sum=795.711 (2)",
            "tab": "General information",
            "score": 397.8554913294798
          },
          "College Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # eval": {
            "description": "min=102, mean=102, max=102, sum=204 (2)",
            "tab": "General information",
            "score": 102.0
          },
          "College Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # prompt tokens": {
            "description": "min=392.598, mean=392.598, max=392.598, sum=785.196 (2)",
            "tab": "General information",
            "score": 392.5980392156863
          },
          "College Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Computer Security - EM",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.76,
        "details": {
          "description": "min=0.76, mean=0.76, max=0.76, sum=1.52 (2)",
          "tab": "Accuracy",
          "Computer Security - Observed inference time (s)": {
            "description": "min=0.253, mean=0.253, max=0.253, sum=0.506 (2)",
            "tab": "Efficiency",
            "score": 0.2529018998146057
          },
          "Computer Security - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Computer Security - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Computer Security - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Computer Security - # prompt tokens": {
            "description": "min=378.54, mean=378.54, max=378.54, sum=757.08 (2)",
            "tab": "General information",
            "score": 378.54
          },
          "Computer Security - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - EM",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.439,
        "details": {
          "description": "min=0.439, mean=0.439, max=0.439, sum=0.877 (2)",
          "tab": "Accuracy",
          "Econometrics - Observed inference time (s)": {
            "description": "min=0.254, mean=0.254, max=0.254, sum=0.507 (2)",
            "tab": "Efficiency",
            "score": 0.25371592086658146
          },
          "Econometrics - # eval": {
            "description": "min=114, mean=114, max=114, sum=228 (2)",
            "tab": "General information",
            "score": 114.0
          },
          "Econometrics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Econometrics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Econometrics - # prompt tokens": {
            "description": "min=614.43, mean=614.43, max=614.43, sum=1228.86 (2)",
            "tab": "General information",
            "score": 614.4298245614035
          },
          "Econometrics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - EM",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.4,
        "details": {
          "description": "min=0.4, mean=0.4, max=0.4, sum=0.8 (2)",
          "tab": "Accuracy",
          "Global Facts - Observed inference time (s)": {
            "description": "min=0.257, mean=0.257, max=0.257, sum=0.514 (2)",
            "tab": "Efficiency",
            "score": 0.25686686754226684
          },
          "Global Facts - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Global Facts - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Global Facts - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Global Facts - # prompt tokens": {
            "description": "min=329.71, mean=329.71, max=329.71, sum=659.42 (2)",
            "tab": "General information",
            "score": 329.71
          },
          "Global Facts - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - EM",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.796,
        "details": {
          "description": "min=0.796, mean=0.796, max=0.796, sum=1.593 (2)",
          "tab": "Accuracy",
          "Jurisprudence - Observed inference time (s)": {
            "description": "min=0.26, mean=0.26, max=0.26, sum=0.521 (2)",
            "tab": "Efficiency",
            "score": 0.260397990544637
          },
          "Jurisprudence - # eval": {
            "description": "min=108, mean=108, max=108, sum=216 (2)",
            "tab": "General information",
            "score": 108.0
          },
          "Jurisprudence - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Jurisprudence - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Jurisprudence - # prompt tokens": {
            "description": "min=312.287, mean=312.287, max=312.287, sum=624.574 (2)",
            "tab": "General information",
            "score": 312.287037037037
          },
          "Jurisprudence - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - EM",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.749,
        "details": {
          "description": "min=0.749, mean=0.749, max=0.749, sum=1.498 (2)",
          "tab": "Accuracy",
          "Philosophy - Observed inference time (s)": {
            "description": "min=0.252, mean=0.252, max=0.252, sum=0.504 (2)",
            "tab": "Efficiency",
            "score": 0.25189057270430293
          },
          "Philosophy - # eval": {
            "description": "min=311, mean=311, max=311, sum=622 (2)",
            "tab": "General information",
            "score": 311.0
          },
          "Philosophy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Philosophy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Philosophy - # prompt tokens": {
            "description": "min=267.441, mean=267.441, max=267.441, sum=534.881 (2)",
            "tab": "General information",
            "score": 267.4405144694534
          },
          "Philosophy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Psychology - EM",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.716,
        "details": {
          "description": "min=0.716, mean=0.716, max=0.716, sum=1.431 (2)",
          "tab": "Accuracy",
          "Professional Medicine - Observed inference time (s)": {
            "description": "min=0.308, mean=0.308, max=0.308, sum=0.616 (2)",
            "tab": "Efficiency",
            "score": 0.30818068542901206
          },
          "Professional Accounting - Observed inference time (s)": {
            "description": "min=0.266, mean=0.266, max=0.266, sum=0.532 (2)",
            "tab": "Efficiency",
            "score": 0.26598995881723175
          },
          "Professional Law - Observed inference time (s)": {
            "description": "min=0.365, mean=0.365, max=0.365, sum=0.73 (2)",
            "tab": "Efficiency",
            "score": 0.36489380229716195
          },
          "Professional Psychology - Observed inference time (s)": {
            "description": "min=0.255, mean=0.255, max=0.255, sum=0.511 (2)",
            "tab": "Efficiency",
            "score": 0.25544750768374774
          },
          "Professional Medicine - # eval": {
            "description": "min=272, mean=272, max=272, sum=544 (2)",
            "tab": "General information",
            "score": 272.0
          },
          "Professional Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Medicine - # prompt tokens": {
            "description": "min=813.651, mean=813.651, max=813.651, sum=1627.301 (2)",
            "tab": "General information",
            "score": 813.6507352941177
          },
          "Professional Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # eval": {
            "description": "min=282, mean=282, max=282, sum=564 (2)",
            "tab": "General information",
            "score": 282.0
          },
          "Professional Accounting - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Accounting - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # prompt tokens": {
            "description": "min=555.461, mean=555.461, max=555.461, sum=1110.922 (2)",
            "tab": "General information",
            "score": 555.4609929078014
          },
          "Professional Accounting - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # eval": {
            "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
            "tab": "General information",
            "score": 1534.0
          },
          "Professional Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # prompt tokens": {
            "description": "min=1151.508, mean=1151.508, max=1151.508, sum=2303.016 (2)",
            "tab": "General information",
            "score": 1151.5078226857888
          },
          "Professional Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # eval": {
            "description": "min=612, mean=612, max=612, sum=1224 (2)",
            "tab": "General information",
            "score": 612.0
          },
          "Professional Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # prompt tokens": {
            "description": "min=422.158, mean=422.158, max=422.158, sum=844.317 (2)",
            "tab": "General information",
            "score": 422.15849673202615
          },
          "Professional Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - EM",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.91,
        "details": {
          "description": "min=0.91, mean=0.91, max=0.91, sum=1.82 (2)",
          "tab": "Accuracy",
          "Us Foreign Policy - Observed inference time (s)": {
            "description": "min=0.271, mean=0.271, max=0.271, sum=0.542 (2)",
            "tab": "Efficiency",
            "score": 0.27118161678314207
          },
          "Us Foreign Policy - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Us Foreign Policy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Us Foreign Policy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Us Foreign Policy - # prompt tokens": {
            "description": "min=422.79, mean=422.79, max=422.79, sum=845.58 (2)",
            "tab": "General information",
            "score": 422.79
          },
          "Us Foreign Policy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - EM",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.73,
        "details": {
          "description": "min=0.73, mean=0.73, max=0.73, sum=1.461 (2)",
          "tab": "Accuracy",
          "Astronomy - Observed inference time (s)": {
            "description": "min=0.276, mean=0.276, max=0.276, sum=0.553 (2)",
            "tab": "Efficiency",
            "score": 0.27634719171022115
          },
          "Astronomy - # eval": {
            "description": "min=152, mean=152, max=152, sum=304 (2)",
            "tab": "General information",
            "score": 152.0
          },
          "Astronomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Astronomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Astronomy - # prompt tokens": {
            "description": "min=440.612, mean=440.612, max=440.612, sum=881.224 (2)",
            "tab": "General information",
            "score": 440.6118421052632
          },
          "Astronomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - EM",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.6,
        "details": {
          "description": "min=0.6, mean=0.6, max=0.6, sum=1.2 (2)",
          "tab": "Accuracy",
          "Business Ethics - Observed inference time (s)": {
            "description": "min=0.267, mean=0.267, max=0.267, sum=0.533 (2)",
            "tab": "Efficiency",
            "score": 0.2665403389930725
          },
          "Business Ethics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Business Ethics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Business Ethics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Business Ethics - # prompt tokens": {
            "description": "min=521.13, mean=521.13, max=521.13, sum=1042.26 (2)",
            "tab": "General information",
            "score": 521.13
          },
          "Business Ethics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - EM",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.702,
        "details": {
          "description": "min=0.702, mean=0.702, max=0.702, sum=1.404 (2)",
          "tab": "Accuracy",
          "Clinical Knowledge - Observed inference time (s)": {
            "description": "min=0.259, mean=0.259, max=0.259, sum=0.517 (2)",
            "tab": "Efficiency",
            "score": 0.25872870661177727
          },
          "Clinical Knowledge - # eval": {
            "description": "min=265, mean=265, max=265, sum=530 (2)",
            "tab": "General information",
            "score": 265.0
          },
          "Clinical Knowledge - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Clinical Knowledge - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Clinical Knowledge - # prompt tokens": {
            "description": "min=317.268, mean=317.268, max=317.268, sum=634.536 (2)",
            "tab": "General information",
            "score": 317.2679245283019
          },
          "Clinical Knowledge - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - EM",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.677,
        "details": {
          "description": "min=0.677, mean=0.677, max=0.677, sum=1.353 (2)",
          "tab": "Accuracy",
          "Conceptual Physics - Observed inference time (s)": {
            "description": "min=0.306, mean=0.306, max=0.306, sum=0.613 (2)",
            "tab": "Efficiency",
            "score": 0.30636518965376186
          },
          "Conceptual Physics - # eval": {
            "description": "min=235, mean=235, max=235, sum=470 (2)",
            "tab": "General information",
            "score": 235.0
          },
          "Conceptual Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Conceptual Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Conceptual Physics - # prompt tokens": {
            "description": "min=241.511, mean=241.511, max=241.511, sum=483.021 (2)",
            "tab": "General information",
            "score": 241.51063829787233
          },
          "Conceptual Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - EM",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.621,
        "details": {
          "description": "min=0.621, mean=0.621, max=0.621, sum=1.241 (2)",
          "tab": "Accuracy",
          "Electrical Engineering - Observed inference time (s)": {
            "description": "min=0.412, mean=0.412, max=0.412, sum=0.825 (2)",
            "tab": "Efficiency",
            "score": 0.41247522255470015
          },
          "Electrical Engineering - # eval": {
            "description": "min=145, mean=145, max=145, sum=290 (2)",
            "tab": "General information",
            "score": 145.0
          },
          "Electrical Engineering - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Electrical Engineering - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Electrical Engineering - # prompt tokens": {
            "description": "min=382.393, mean=382.393, max=382.393, sum=764.786 (2)",
            "tab": "General information",
            "score": 382.39310344827584
          },
          "Electrical Engineering - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.497,
        "details": {
          "description": "min=0.497, mean=0.497, max=0.497, sum=0.995 (2)",
          "tab": "Accuracy",
          "Elementary Mathematics - Observed inference time (s)": {
            "description": "min=0.259, mean=0.259, max=0.259, sum=0.517 (2)",
            "tab": "Efficiency",
            "score": 0.2586819948973479
          },
          "Elementary Mathematics - # eval": {
            "description": "min=378, mean=378, max=378, sum=756 (2)",
            "tab": "General information",
            "score": 378.0
          },
          "Elementary Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Elementary Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Elementary Mathematics - # prompt tokens": {
            "description": "min=467.987, mean=467.987, max=467.987, sum=935.974 (2)",
            "tab": "General information",
            "score": 467.9867724867725
          },
          "Elementary Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - EM",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.444,
        "details": {
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.889 (2)",
          "tab": "Accuracy",
          "Formal Logic - Observed inference time (s)": {
            "description": "min=0.263, mean=0.263, max=0.263, sum=0.526 (2)",
            "tab": "Efficiency",
            "score": 0.2629187542294699
          },
          "Formal Logic - # eval": {
            "description": "min=126, mean=126, max=126, sum=252 (2)",
            "tab": "General information",
            "score": 126.0
          },
          "Formal Logic - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Formal Logic - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Formal Logic - # prompt tokens": {
            "description": "min=559.865, mean=559.865, max=559.865, sum=1119.73 (2)",
            "tab": "General information",
            "score": 559.8650793650794
          },
          "Formal Logic - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School World History - EM",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.797,
        "details": {
          "description": "min=0.797, mean=0.797, max=0.797, sum=1.595 (2)",
          "tab": "Accuracy",
          "High School Biology - Observed inference time (s)": {
            "description": "min=0.256, mean=0.256, max=0.256, sum=0.513 (2)",
            "tab": "Efficiency",
            "score": 0.25630061088069794
          },
          "High School Chemistry - Observed inference time (s)": {
            "description": "min=0.259, mean=0.259, max=0.259, sum=0.519 (2)",
            "tab": "Efficiency",
            "score": 0.2594739521665526
          },
          "High School Computer Science - Observed inference time (s)": {
            "description": "min=0.294, mean=0.294, max=0.294, sum=0.588 (2)",
            "tab": "Efficiency",
            "score": 0.29399110078811647
          },
          "High School European History - Observed inference time (s)": {
            "description": "min=0.519, mean=0.519, max=0.519, sum=1.039 (2)",
            "tab": "Efficiency",
            "score": 0.5194540543989702
          },
          "High School Geography - Observed inference time (s)": {
            "description": "min=0.25, mean=0.25, max=0.25, sum=0.5 (2)",
            "tab": "Efficiency",
            "score": 0.24992815051415954
          },
          "High School Government And Politics - Observed inference time (s)": {
            "description": "min=0.242, mean=0.242, max=0.242, sum=0.484 (2)",
            "tab": "Efficiency",
            "score": 0.242088835474123
          },
          "High School Macroeconomics - Observed inference time (s)": {
            "description": "min=0.24, mean=0.24, max=0.24, sum=0.481 (2)",
            "tab": "Efficiency",
            "score": 0.240464658003587
          },
          "High School Mathematics - Observed inference time (s)": {
            "description": "min=0.252, mean=0.252, max=0.252, sum=0.503 (2)",
            "tab": "Efficiency",
            "score": 0.25154934459262424
          },
          "High School Microeconomics - Observed inference time (s)": {
            "description": "min=0.25, mean=0.25, max=0.25, sum=0.501 (2)",
            "tab": "Efficiency",
            "score": 0.25046268931957855
          },
          "High School Physics - Observed inference time (s)": {
            "description": "min=0.256, mean=0.256, max=0.256, sum=0.511 (2)",
            "tab": "Efficiency",
            "score": 0.25560809444907484
          },
          "High School Psychology - Observed inference time (s)": {
            "description": "min=0.251, mean=0.251, max=0.251, sum=0.501 (2)",
            "tab": "Efficiency",
            "score": 0.250657169971991
          },
          "High School Statistics - Observed inference time (s)": {
            "description": "min=0.282, mean=0.282, max=0.282, sum=0.564 (2)",
            "tab": "Efficiency",
            "score": 0.2818450938772272
          },
          "High School US History - Observed inference time (s)": {
            "description": "min=0.45, mean=0.45, max=0.45, sum=0.9 (2)",
            "tab": "Efficiency",
            "score": 0.44991188072690774
          },
          "High School World History - Observed inference time (s)": {
            "description": "min=0.347, mean=0.347, max=0.347, sum=0.693 (2)",
            "tab": "Efficiency",
            "score": 0.3466388042466047
          },
          "High School Biology - # eval": {
            "description": "min=310, mean=310, max=310, sum=620 (2)",
            "tab": "General information",
            "score": 310.0
          },
          "High School Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Biology - # prompt tokens": {
            "description": "min=380.871, mean=380.871, max=380.871, sum=761.742 (2)",
            "tab": "General information",
            "score": 380.8709677419355
          },
          "High School Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # eval": {
            "description": "min=203, mean=203, max=203, sum=406 (2)",
            "tab": "General information",
            "score": 203.0
          },
          "High School Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # prompt tokens": {
            "description": "min=401.734, mean=401.734, max=401.734, sum=803.468 (2)",
            "tab": "General information",
            "score": 401.73399014778323
          },
          "High School Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "High School Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # prompt tokens": {
            "description": "min=793.8, mean=793.8, max=793.8, sum=1587.6 (2)",
            "tab": "General information",
            "score": 793.8
          },
          "High School Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # eval": {
            "description": "min=165, mean=165, max=165, sum=330 (2)",
            "tab": "General information",
            "score": 165.0
          },
          "High School European History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School European History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # prompt tokens": {
            "description": "min=2081.679, mean=2081.679, max=2081.679, sum=4163.358 (2)",
            "tab": "General information",
            "score": 2081.6787878787877
          },
          "High School European History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # eval": {
            "description": "min=198, mean=198, max=198, sum=396 (2)",
            "tab": "General information",
            "score": 198.0
          },
          "High School Geography - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Geography - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # prompt tokens": {
            "description": "min=299.717, mean=299.717, max=299.717, sum=599.434 (2)",
            "tab": "General information",
            "score": 299.7171717171717
          },
          "High School Geography - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # eval": {
            "description": "min=193, mean=193, max=193, sum=386 (2)",
            "tab": "General information",
            "score": 193.0
          },
          "High School Government And Politics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Government And Politics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # prompt tokens": {
            "description": "min=333.601, mean=333.601, max=333.601, sum=667.202 (2)",
            "tab": "General information",
            "score": 333.60103626943004
          },
          "High School Government And Politics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # eval": {
            "description": "min=390, mean=390, max=390, sum=780 (2)",
            "tab": "General information",
            "score": 390.0
          },
          "High School Macroeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Macroeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # prompt tokens": {
            "description": "min=286.562, mean=286.562, max=286.562, sum=573.123 (2)",
            "tab": "General information",
            "score": 286.5615384615385
          },
          "High School Macroeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # eval": {
            "description": "min=270, mean=270, max=270, sum=540 (2)",
            "tab": "General information",
            "score": 270.0
          },
          "High School Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # prompt tokens": {
            "description": "min=421.889, mean=421.889, max=421.889, sum=843.778 (2)",
            "tab": "General information",
            "score": 421.8888888888889
          },
          "High School Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # eval": {
            "description": "min=238, mean=238, max=238, sum=476 (2)",
            "tab": "General information",
            "score": 238.0
          },
          "High School Microeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Microeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # prompt tokens": {
            "description": "min=301.231, mean=301.231, max=301.231, sum=602.462 (2)",
            "tab": "General information",
            "score": 301.2310924369748
          },
          "High School Microeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # eval": {
            "description": "min=151, mean=151, max=151, sum=302 (2)",
            "tab": "General information",
            "score": 151.0
          },
          "High School Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # prompt tokens": {
            "description": "min=453.51, mean=453.51, max=453.51, sum=907.02 (2)",
            "tab": "General information",
            "score": 453.50993377483445
          },
          "High School Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # eval": {
            "description": "min=545, mean=545, max=545, sum=1090 (2)",
            "tab": "General information",
            "score": 545.0
          },
          "High School Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # prompt tokens": {
            "description": "min=355.059, mean=355.059, max=355.059, sum=710.117 (2)",
            "tab": "General information",
            "score": 355.0587155963303
          },
          "High School Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # eval": {
            "description": "min=216, mean=216, max=216, sum=432 (2)",
            "tab": "General information",
            "score": 216.0
          },
          "High School Statistics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Statistics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # prompt tokens": {
            "description": "min=648.037, mean=648.037, max=648.037, sum=1296.074 (2)",
            "tab": "General information",
            "score": 648.0370370370371
          },
          "High School Statistics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # eval": {
            "description": "min=204, mean=204, max=204, sum=408 (2)",
            "tab": "General information",
            "score": 204.0
          },
          "High School US History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School US History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # prompt tokens": {
            "description": "min=1628.495, mean=1628.495, max=1628.495, sum=3256.99 (2)",
            "tab": "General information",
            "score": 1628.4950980392157
          },
          "High School US History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # eval": {
            "description": "min=237, mean=237, max=237, sum=474 (2)",
            "tab": "General information",
            "score": 237.0
          },
          "High School World History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School World History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # prompt tokens": {
            "description": "min=1025.097, mean=1025.097, max=1025.097, sum=2050.194 (2)",
            "tab": "General information",
            "score": 1025.097046413502
          },
          "High School World History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Sexuality - EM",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.794,
        "details": {
          "description": "min=0.794, mean=0.794, max=0.794, sum=1.588 (2)",
          "tab": "Accuracy",
          "Human Aging - Observed inference time (s)": {
            "description": "min=0.233, mean=0.233, max=0.233, sum=0.466 (2)",
            "tab": "Efficiency",
            "score": 0.2328128023532474
          },
          "Human Sexuality - Observed inference time (s)": {
            "description": "min=0.251, mean=0.251, max=0.251, sum=0.501 (2)",
            "tab": "Efficiency",
            "score": 0.2506928462108583
          },
          "Human Aging - # eval": {
            "description": "min=223, mean=223, max=223, sum=446 (2)",
            "tab": "General information",
            "score": 223.0
          },
          "Human Aging - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Aging - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Aging - # prompt tokens": {
            "description": "min=250.915, mean=250.915, max=250.915, sum=501.83 (2)",
            "tab": "General information",
            "score": 250.91479820627802
          },
          "Human Aging - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # eval": {
            "description": "min=131, mean=131, max=131, sum=262 (2)",
            "tab": "General information",
            "score": 131.0
          },
          "Human Sexuality - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Sexuality - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # prompt tokens": {
            "description": "min=263.183, mean=263.183, max=263.183, sum=526.366 (2)",
            "tab": "General information",
            "score": 263.1832061068702
          },
          "Human Sexuality - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - EM",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.835,
        "details": {
          "description": "min=0.835, mean=0.835, max=0.835, sum=1.669 (2)",
          "tab": "Accuracy",
          "International Law - Observed inference time (s)": {
            "description": "min=0.271, mean=0.271, max=0.271, sum=0.542 (2)",
            "tab": "Efficiency",
            "score": 0.27110107082965945
          },
          "International Law - # eval": {
            "description": "min=121, mean=121, max=121, sum=242 (2)",
            "tab": "General information",
            "score": 121.0
          },
          "International Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "International Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "International Law - # prompt tokens": {
            "description": "min=477.843, mean=477.843, max=477.843, sum=955.686 (2)",
            "tab": "General information",
            "score": 477.8429752066116
          },
          "International Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - EM",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.706,
        "details": {
          "description": "min=0.706, mean=0.706, max=0.706, sum=1.411 (2)",
          "tab": "Accuracy",
          "Logical Fallacies - Observed inference time (s)": {
            "description": "min=0.25, mean=0.25, max=0.25, sum=0.499 (2)",
            "tab": "Efficiency",
            "score": 0.24970631804202964
          },
          "Logical Fallacies - # eval": {
            "description": "min=163, mean=163, max=163, sum=326 (2)",
            "tab": "General information",
            "score": 163.0
          },
          "Logical Fallacies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Logical Fallacies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Logical Fallacies - # prompt tokens": {
            "description": "min=337.718, mean=337.718, max=337.718, sum=675.436 (2)",
            "tab": "General information",
            "score": 337.7177914110429
          },
          "Logical Fallacies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - EM",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.536,
        "details": {
          "description": "min=0.536, mean=0.536, max=0.536, sum=1.071 (2)",
          "tab": "Accuracy",
          "Machine Learning - Observed inference time (s)": {
            "description": "min=0.267, mean=0.267, max=0.267, sum=0.533 (2)",
            "tab": "Efficiency",
            "score": 0.2665597881589617
          },
          "Machine Learning - # eval": {
            "description": "min=112, mean=112, max=112, sum=224 (2)",
            "tab": "General information",
            "score": 112.0
          },
          "Machine Learning - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Machine Learning - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Machine Learning - # prompt tokens": {
            "description": "min=559.277, mean=559.277, max=559.277, sum=1118.554 (2)",
            "tab": "General information",
            "score": 559.2767857142857
          },
          "Machine Learning - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - EM",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.786,
        "details": {
          "description": "min=0.786, mean=0.786, max=0.786, sum=1.573 (2)",
          "tab": "Accuracy",
          "Management - Observed inference time (s)": {
            "description": "min=0.241, mean=0.241, max=0.241, sum=0.481 (2)",
            "tab": "Efficiency",
            "score": 0.24073980386974742
          },
          "Management - # eval": {
            "description": "min=103, mean=103, max=103, sum=206 (2)",
            "tab": "General information",
            "score": 103.0
          },
          "Management - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Management - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Management - # prompt tokens": {
            "description": "min=225.262, mean=225.262, max=225.262, sum=450.524 (2)",
            "tab": "General information",
            "score": 225.2621359223301
          },
          "Management - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - EM",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.885,
        "details": {
          "description": "min=0.885, mean=0.885, max=0.885, sum=1.769 (2)",
          "tab": "Accuracy",
          "Marketing - Observed inference time (s)": {
            "description": "min=0.258, mean=0.258, max=0.258, sum=0.517 (2)",
            "tab": "Efficiency",
            "score": 0.25835410753885907
          },
          "Marketing - # eval": {
            "description": "min=234, mean=234, max=234, sum=468 (2)",
            "tab": "General information",
            "score": 234.0
          },
          "Marketing - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Marketing - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Marketing - # prompt tokens": {
            "description": "min=351.573, mean=351.573, max=351.573, sum=703.145 (2)",
            "tab": "General information",
            "score": 351.5726495726496
          },
          "Marketing - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - EM",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.67,
        "details": {
          "description": "min=0.67, mean=0.67, max=0.67, sum=1.34 (2)",
          "tab": "Accuracy",
          "Medical Genetics - Observed inference time (s)": {
            "description": "min=0.251, mean=0.251, max=0.251, sum=0.502 (2)",
            "tab": "Efficiency",
            "score": 0.2510761094093323
          },
          "Medical Genetics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Medical Genetics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Medical Genetics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Medical Genetics - # prompt tokens": {
            "description": "min=274.75, mean=274.75, max=274.75, sum=549.5 (2)",
            "tab": "General information",
            "score": 274.75
          },
          "Medical Genetics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - EM",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.865,
        "details": {
          "description": "min=0.865, mean=0.865, max=0.865, sum=1.729 (2)",
          "tab": "Accuracy",
          "Miscellaneous - Observed inference time (s)": {
            "description": "min=0.233, mean=0.233, max=0.233, sum=0.466 (2)",
            "tab": "Efficiency",
            "score": 0.23304342005596915
          },
          "Miscellaneous - # eval": {
            "description": "min=783, mean=783, max=783, sum=1566 (2)",
            "tab": "General information",
            "score": 783.0
          },
          "Miscellaneous - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Miscellaneous - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Miscellaneous - # prompt tokens": {
            "description": "min=254.525, mean=254.525, max=254.525, sum=509.05 (2)",
            "tab": "General information",
            "score": 254.5249042145594
          },
          "Miscellaneous - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - EM",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.465,
        "details": {
          "description": "min=0.465, mean=0.465, max=0.465, sum=0.93 (2)",
          "tab": "Accuracy",
          "Moral Disputes - Observed inference time (s)": {
            "description": "min=0.256, mean=0.256, max=0.256, sum=0.512 (2)",
            "tab": "Efficiency",
            "score": 0.2561916905331474
          },
          "Moral Scenarios - Observed inference time (s)": {
            "description": "min=0.262, mean=0.262, max=0.262, sum=0.525 (2)",
            "tab": "Efficiency",
            "score": 0.2624055065922231
          },
          "Moral Disputes - # eval": {
            "description": "min=346, mean=346, max=346, sum=692 (2)",
            "tab": "General information",
            "score": 346.0
          },
          "Moral Disputes - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Disputes - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Disputes - # prompt tokens": {
            "description": "min=357.165, mean=357.165, max=357.165, sum=714.329 (2)",
            "tab": "General information",
            "score": 357.16473988439304
          },
          "Moral Disputes - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # eval": {
            "description": "min=895, mean=895, max=895, sum=1790 (2)",
            "tab": "General information",
            "score": 895.0
          },
          "Moral Scenarios - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Scenarios - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # prompt tokens": {
            "description": "min=546.793, mean=546.793, max=546.793, sum=1093.587 (2)",
            "tab": "General information",
            "score": 546.7932960893854
          },
          "Moral Scenarios - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - EM",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.745,
        "details": {
          "description": "min=0.745, mean=0.745, max=0.745, sum=1.49 (2)",
          "tab": "Accuracy",
          "Nutrition - Observed inference time (s)": {
            "description": "min=0.248, mean=0.248, max=0.248, sum=0.496 (2)",
            "tab": "Efficiency",
            "score": 0.2479639964945176
          },
          "Nutrition - # eval": {
            "description": "min=306, mean=306, max=306, sum=612 (2)",
            "tab": "General information",
            "score": 306.0
          },
          "Nutrition - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Nutrition - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Nutrition - # prompt tokens": {
            "description": "min=454.758, mean=454.758, max=454.758, sum=909.516 (2)",
            "tab": "General information",
            "score": 454.75816993464053
          },
          "Nutrition - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - EM",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.796,
        "details": {
          "description": "min=0.796, mean=0.796, max=0.796, sum=1.593 (2)",
          "tab": "Accuracy",
          "Prehistory - Observed inference time (s)": {
            "description": "min=0.254, mean=0.254, max=0.254, sum=0.508 (2)",
            "tab": "Efficiency",
            "score": 0.2538878917694092
          },
          "Prehistory - # eval": {
            "description": "min=324, mean=324, max=324, sum=648 (2)",
            "tab": "General information",
            "score": 324.0
          },
          "Prehistory - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Prehistory - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Prehistory - # prompt tokens": {
            "description": "min=410.315, mean=410.315, max=410.315, sum=820.63 (2)",
            "tab": "General information",
            "score": 410.31481481481484
          },
          "Prehistory - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Public Relations - EM",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.682,
        "details": {
          "description": "min=0.682, mean=0.682, max=0.682, sum=1.364 (2)",
          "tab": "Accuracy",
          "Public Relations - Observed inference time (s)": {
            "description": "min=0.252, mean=0.252, max=0.252, sum=0.505 (2)",
            "tab": "Efficiency",
            "score": 0.25225248553536156
          },
          "Public Relations - # eval": {
            "description": "min=110, mean=110, max=110, sum=220 (2)",
            "tab": "General information",
            "score": 110.0
          },
          "Public Relations - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Public Relations - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Public Relations - # prompt tokens": {
            "description": "min=316.591, mean=316.591, max=316.591, sum=633.182 (2)",
            "tab": "General information",
            "score": 316.59090909090907
          },
          "Public Relations - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - EM",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.743,
        "details": {
          "description": "min=0.743, mean=0.743, max=0.743, sum=1.486 (2)",
          "tab": "Accuracy",
          "Security Studies - Observed inference time (s)": {
            "description": "min=0.31, mean=0.31, max=0.31, sum=0.62 (2)",
            "tab": "Efficiency",
            "score": 0.30983400539476047
          },
          "Security Studies - # eval": {
            "description": "min=245, mean=245, max=245, sum=490 (2)",
            "tab": "General information",
            "score": 245.0
          },
          "Security Studies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Security Studies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Security Studies - # prompt tokens": {
            "description": "min=856.637, mean=856.637, max=856.637, sum=1713.273 (2)",
            "tab": "General information",
            "score": 856.6367346938775
          },
          "Security Studies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - EM",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.891,
        "details": {
          "description": "min=0.891, mean=0.891, max=0.891, sum=1.781 (2)",
          "tab": "Accuracy",
          "Sociology - Observed inference time (s)": {
            "description": "min=0.258, mean=0.258, max=0.258, sum=0.515 (2)",
            "tab": "Efficiency",
            "score": 0.25752189384764107
          },
          "Sociology - # eval": {
            "description": "min=201, mean=201, max=201, sum=402 (2)",
            "tab": "General information",
            "score": 201.0
          },
          "Sociology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Sociology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Sociology - # prompt tokens": {
            "description": "min=327.801, mean=327.801, max=327.801, sum=655.602 (2)",
            "tab": "General information",
            "score": 327.80099502487565
          },
          "Sociology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - EM",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.53,
        "details": {
          "description": "min=0.53, mean=0.53, max=0.53, sum=1.06 (2)",
          "tab": "Accuracy",
          "Virology - Observed inference time (s)": {
            "description": "min=0.238, mean=0.238, max=0.238, sum=0.477 (2)",
            "tab": "Efficiency",
            "score": 0.23830672200903835
          },
          "Virology - # eval": {
            "description": "min=166, mean=166, max=166, sum=332 (2)",
            "tab": "General information",
            "score": 166.0
          },
          "Virology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Virology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Virology - # prompt tokens": {
            "description": "min=267.458, mean=267.458, max=267.458, sum=534.916 (2)",
            "tab": "General information",
            "score": 267.4578313253012
          },
          "Virology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - EM",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.813,
        "details": {
          "description": "min=0.813, mean=0.813, max=0.813, sum=1.626 (2)",
          "tab": "Accuracy",
          "World Religions - Observed inference time (s)": {
            "description": "min=0.236, mean=0.236, max=0.236, sum=0.473 (2)",
            "tab": "Efficiency",
            "score": 0.23630904593662908
          },
          "World Religions - # eval": {
            "description": "min=171, mean=171, max=171, sum=342 (2)",
            "tab": "General information",
            "score": 171.0
          },
          "World Religions - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "World Religions - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "World Religions - # prompt tokens": {
            "description": "min=223.731, mean=223.731, max=223.731, sum=447.462 (2)",
            "tab": "General information",
            "score": 223.73099415204678
          },
          "World Religions - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.887,
        "details": {
          "description": null,
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    }
  ]
}