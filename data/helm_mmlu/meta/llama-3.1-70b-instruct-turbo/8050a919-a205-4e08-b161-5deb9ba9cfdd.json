{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_mmlu/meta_llama-3.1-70b-instruct-turbo/1765639045.712842",
  "retrieved_timestamp": "1765639045.712842",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
  ],
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Llama 3.1 Instruct Turbo (70B)",
    "id": "meta/llama-3.1-70b-instruct-turbo",
    "developer": "meta",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.801,
        "details": {
          "description": "min=0.404, mean=0.801, max=0.984, sum=91.318 (114)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - EM",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.55,
        "details": {
          "description": "min=0.55, mean=0.55, max=0.55, sum=1.1 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - EM",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.8,
        "details": {
          "description": "min=0.8, mean=0.8, max=0.8, sum=1.6 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Chemistry - EM",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.59,
        "details": {
          "description": "min=0.59, mean=0.59, max=0.59, sum=1.18 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "Computer Security - EM",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.8,
        "details": {
          "description": "min=0.8, mean=0.8, max=0.8, sum=1.6 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - EM",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.675,
        "details": {
          "description": "min=0.675, mean=0.675, max=0.675, sum=1.351 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - EM",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.61,
        "details": {
          "description": "min=0.61, mean=0.61, max=0.61, sum=1.22 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - EM",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.889,
        "details": {
          "description": "min=0.889, mean=0.889, max=0.889, sum=1.778 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - EM",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.833,
        "details": {
          "description": "min=0.833, mean=0.833, max=0.833, sum=1.666 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Medicine - EM",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.93,
        "details": {
          "description": "min=0.93, mean=0.93, max=0.93, sum=1.86 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - EM",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.93,
        "details": {
          "description": "min=0.93, mean=0.93, max=0.93, sum=1.86 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - EM",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.908,
        "details": {
          "description": "min=0.908, mean=0.908, max=0.908, sum=1.816 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - EM",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.72,
        "details": {
          "description": "min=0.72, mean=0.72, max=0.72, sum=1.44 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - EM",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.845,
        "details": {
          "description": "min=0.845, mean=0.845, max=0.845, sum=1.691 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "College Biology - EM",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.938,
        "details": {
          "description": "min=0.938, mean=0.938, max=0.938, sum=1.875 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Computer Science - EM",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.71,
        "details": {
          "description": "min=0.71, mean=0.71, max=0.71, sum=1.42 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.49,
        "details": {
          "description": "min=0.49, mean=0.49, max=0.49, sum=0.98 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Medicine - EM",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.763,
        "details": {
          "description": "min=0.763, mean=0.763, max=0.763, sum=1.526 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Physics - EM",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.559,
        "details": {
          "description": "min=0.559, mean=0.559, max=0.559, sum=1.118 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - EM",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.834,
        "details": {
          "description": "min=0.834, mean=0.834, max=0.834, sum=1.668 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - EM",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.745,
        "details": {
          "description": "min=0.745, mean=0.745, max=0.745, sum=1.49 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.701,
        "details": {
          "description": "min=0.701, mean=0.701, max=0.701, sum=1.402 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - EM",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.675,
        "details": {
          "description": "min=0.675, mean=0.675, max=0.675, sum=1.349 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School Biology - EM",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.923,
        "details": {
          "description": "min=0.923, mean=0.923, max=0.923, sum=1.845 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Chemistry - EM",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.754,
        "details": {
          "description": "min=0.754, mean=0.754, max=0.754, sum=1.507 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Computer Science - EM",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.92,
        "details": {
          "description": "min=0.92, mean=0.92, max=0.92, sum=1.84 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School European History - EM",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.873,
        "details": {
          "description": "min=0.873, mean=0.873, max=0.873, sum=1.745 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School Geography - EM",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.939,
        "details": {
          "description": "min=0.939, mean=0.939, max=0.939, sum=1.879 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - EM",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.984,
        "details": {
          "description": "min=0.984, mean=0.984, max=0.984, sum=1.969 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - EM",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.841,
        "details": {
          "description": "min=0.841, mean=0.841, max=0.841, sum=1.682 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.404,
        "details": {
          "description": "min=0.404, mean=0.404, max=0.404, sum=0.807 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - EM",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.916,
        "details": {
          "description": "min=0.916, mean=0.916, max=0.916, sum=1.832 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Physics - EM",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.629,
        "details": {
          "description": "min=0.629, mean=0.629, max=0.629, sum=1.258 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Psychology - EM",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.949,
        "details": {
          "description": "min=0.949, mean=0.949, max=0.949, sum=1.897 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Statistics - EM",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.755,
        "details": {
          "description": "min=0.755, mean=0.755, max=0.755, sum=1.509 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School US History - EM",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.941,
        "details": {
          "description": "min=0.941, mean=0.941, max=0.941, sum=1.882 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School World History - EM",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.937,
        "details": {
          "description": "min=0.937, mean=0.937, max=0.937, sum=1.873 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Aging - EM",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.821,
        "details": {
          "description": "min=0.821, mean=0.821, max=0.821, sum=1.641 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Sexuality - EM",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.855,
        "details": {
          "description": "min=0.855, mean=0.855, max=0.855, sum=1.71 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - EM",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.926,
        "details": {
          "description": "min=0.926, mean=0.926, max=0.926, sum=1.851 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - EM",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.84,
        "details": {
          "description": "min=0.84, mean=0.84, max=0.84, sum=1.681 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - EM",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.696,
        "details": {
          "description": "min=0.696, mean=0.696, max=0.696, sum=1.393 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - EM",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.913,
        "details": {
          "description": "min=0.913, mean=0.913, max=0.913, sum=1.825 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - EM",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.936,
        "details": {
          "description": "min=0.936, mean=0.936, max=0.936, sum=1.872 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - EM",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.93,
        "details": {
          "description": "min=0.93, mean=0.93, max=0.93, sum=1.86 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - EM",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.913,
        "details": {
          "description": "min=0.913, mean=0.913, max=0.913, sum=1.826 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Disputes - EM",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.841,
        "details": {
          "description": "min=0.841, mean=0.841, max=0.841, sum=1.682 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - EM",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.834,
        "details": {
          "description": "min=0.834, mean=0.834, max=0.834, sum=1.667 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - EM",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.889,
        "details": {
          "description": "min=0.889, mean=0.889, max=0.889, sum=1.778 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - EM",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.88,
        "details": {
          "description": "min=0.88, mean=0.88, max=0.88, sum=1.759 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Professional Accounting - EM",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.656,
        "details": {
          "description": "min=0.656, mean=0.656, max=0.656, sum=1.312 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Law - EM",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.674,
        "details": {
          "description": "min=0.674, mean=0.674, max=0.674, sum=1.348 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Psychology - EM",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.846,
        "details": {
          "description": "min=0.846, mean=0.846, max=0.846, sum=1.693 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Public Relations - EM",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.709,
        "details": {
          "description": "min=0.709, mean=0.709, max=0.709, sum=1.418 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - EM",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.849,
        "details": {
          "description": "min=0.849, mean=0.849, max=0.849, sum=1.698 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - EM",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.92,
        "details": {
          "description": "min=0.92, mean=0.92, max=0.92, sum=1.841 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - EM",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.578,
        "details": {
          "description": "min=0.578, mean=0.578, max=0.578, sum=1.157 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - EM",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.895,
        "details": {
          "description": "min=0.895, mean=0.895, max=0.895, sum=1.789 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.021,
        "details": {
          "description": null,
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU All Subjects - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 5.993,
        "details": {
          "description": "min=2.517, mean=5.993, max=45.251, sum=683.146 (114)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 46.0
      },
      "score_details": {
        "score": 45.251,
        "details": {
          "description": "min=45.251, mean=45.251, max=45.251, sum=90.501 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 37.0
      },
      "score_details": {
        "score": 36.973,
        "details": {
          "description": "min=36.973, mean=36.973, max=36.973, sum=73.946 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Chemistry - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.774,
        "details": {
          "description": "min=4.774, mean=4.774, max=4.774, sum=9.548 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "Computer Security - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 2.976,
        "details": {
          "description": "min=2.976, mean=2.976, max=2.976, sum=5.951 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.295,
        "details": {
          "description": "min=4.295, mean=4.295, max=4.295, sum=8.59 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.637,
        "details": {
          "description": "min=3.637, mean=3.637, max=3.637, sum=7.275 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.163,
        "details": {
          "description": "min=3.163, mean=3.163, max=3.163, sum=6.326 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.264,
        "details": {
          "description": "min=3.264, mean=3.264, max=3.264, sum=6.527 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Medicine - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 10.0
      },
      "score_details": {
        "score": 3.871,
        "details": {
          "description": "min=3.871, mean=3.871, max=3.871, sum=7.742 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 2.836,
        "details": {
          "description": "min=2.836, mean=2.836, max=2.836, sum=5.672 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 34.0
      },
      "score_details": {
        "score": 33.307,
        "details": {
          "description": "min=33.307, mean=33.307, max=33.307, sum=66.613 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 35.0
      },
      "score_details": {
        "score": 34.272,
        "details": {
          "description": "min=34.272, mean=34.272, max=34.272, sum=68.544 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 7.0
      },
      "score_details": {
        "score": 6.181,
        "details": {
          "description": "min=6.181, mean=6.181, max=6.181, sum=12.362 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "College Biology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.993,
        "details": {
          "description": "min=4.993, mean=4.993, max=4.993, sum=9.986 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Computer Science - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 24.0
      },
      "score_details": {
        "score": 4.499,
        "details": {
          "description": "min=4.499, mean=4.499, max=4.499, sum=8.999 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Mathematics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.479,
        "details": {
          "description": "min=4.479, mean=4.479, max=4.479, sum=8.957 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Medicine - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.886,
        "details": {
          "description": "min=3.886, mean=3.886, max=3.886, sum=7.773 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Physics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 7.0
      },
      "score_details": {
        "score": 3.274,
        "details": {
          "description": "min=3.274, mean=3.274, max=3.274, sum=6.548 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.413,
        "details": {
          "description": "min=3.413, mean=3.413, max=3.413, sum=6.825 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 11.0
      },
      "score_details": {
        "score": 4.146,
        "details": {
          "description": "min=4.146, mean=4.146, max=4.146, sum=8.292 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.13,
        "details": {
          "description": "min=4.13, mean=4.13, max=4.13, sum=8.261 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.65,
        "details": {
          "description": "min=3.65, mean=3.65, max=3.65, sum=7.301 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School Biology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.179,
        "details": {
          "description": "min=4.179, mean=4.179, max=4.179, sum=8.357 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Chemistry - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.78,
        "details": {
          "description": "min=3.78, mean=3.78, max=3.78, sum=7.56 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Computer Science - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 7.0
      },
      "score_details": {
        "score": 4.276,
        "details": {
          "description": "min=4.276, mean=4.276, max=4.276, sum=8.553 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School European History - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 23.0
      },
      "score_details": {
        "score": 4.728,
        "details": {
          "description": "min=4.728, mean=4.728, max=4.728, sum=9.457 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School Geography - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.994,
        "details": {
          "description": "min=3.994, mean=3.994, max=3.994, sum=7.987 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.056,
        "details": {
          "description": "min=4.056, mean=4.056, max=4.056, sum=8.111 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.06,
        "details": {
          "description": "min=4.06, mean=4.06, max=4.06, sum=8.12 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.211,
        "details": {
          "description": "min=4.211, mean=4.211, max=4.211, sum=8.422 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.869,
        "details": {
          "description": "min=3.869, mean=3.869, max=3.869, sum=7.738 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Physics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.802,
        "details": {
          "description": "min=3.802, mean=3.802, max=3.802, sum=7.604 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Psychology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.897,
        "details": {
          "description": "min=3.897, mean=3.897, max=3.897, sum=7.793 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Statistics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 10.0
      },
      "score_details": {
        "score": 3.5,
        "details": {
          "description": "min=3.5, mean=3.5, max=3.5, sum=6.999 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School US History - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 18.0
      },
      "score_details": {
        "score": 3.948,
        "details": {
          "description": "min=3.948, mean=3.948, max=3.948, sum=7.897 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School World History - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 12.0
      },
      "score_details": {
        "score": 3.316,
        "details": {
          "description": "min=3.316, mean=3.316, max=3.316, sum=6.632 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Aging - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.222,
        "details": {
          "description": "min=3.222, mean=3.222, max=3.222, sum=6.444 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Sexuality - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.132,
        "details": {
          "description": "min=3.132, mean=3.132, max=3.132, sum=6.264 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 3.686,
        "details": {
          "description": "min=3.686, mean=3.686, max=3.686, sum=7.372 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 2.835,
        "details": {
          "description": "min=2.835, mean=2.835, max=2.835, sum=5.67 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 2.82,
        "details": {
          "description": "min=2.82, mean=2.82, max=2.82, sum=5.639 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 2.909,
        "details": {
          "description": "min=2.909, mean=2.909, max=2.909, sum=5.818 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 2.727,
        "details": {
          "description": "min=2.727, mean=2.727, max=2.727, sum=5.455 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 2.657,
        "details": {
          "description": "min=2.657, mean=2.657, max=2.657, sum=5.314 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.308,
        "details": {
          "description": "min=3.308, mean=3.308, max=3.308, sum=6.616 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Disputes - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 2.926,
        "details": {
          "description": "min=2.926, mean=2.926, max=2.926, sum=5.852 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 3.608,
        "details": {
          "description": "min=3.608, mean=3.608, max=3.608, sum=7.216 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.56,
        "details": {
          "description": "min=3.56, mean=3.56, max=3.56, sum=7.12 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.546,
        "details": {
          "description": "min=3.546, mean=3.546, max=3.546, sum=7.091 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Professional Accounting - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 2.943,
        "details": {
          "description": "min=2.943, mean=2.943, max=2.943, sum=5.886 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Law - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 14.0
      },
      "score_details": {
        "score": 3.318,
        "details": {
          "description": "min=3.318, mean=3.318, max=3.318, sum=6.637 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Psychology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 3.102,
        "details": {
          "description": "min=3.102, mean=3.102, max=3.102, sum=6.203 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Public Relations - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.03,
        "details": {
          "description": "min=3.03, mean=3.03, max=3.03, sum=6.06 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 10.0
      },
      "score_details": {
        "score": 2.949,
        "details": {
          "description": "min=2.949, mean=2.949, max=2.949, sum=5.898 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 8.0
      },
      "score_details": {
        "score": 2.843,
        "details": {
          "description": "min=2.843, mean=2.843, max=2.843, sum=5.686 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 3.05,
        "details": {
          "description": "min=3.05, mean=3.05, max=3.05, sum=6.101 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 2.517,
        "details": {
          "description": "min=2.517, mean=2.517, max=2.517, sum=5.033 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # eval",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 247.0
      },
      "score_details": {
        "score": 246.351,
        "details": {
          "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # train",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=570 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - truncated",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 716.0
      },
      "score_details": {
        "score": 614.619,
        "details": {
          "description": "min=274.52, mean=614.619, max=2797.885, sum=70066.61 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # output tokens",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=114 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # eval",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # train",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - truncated",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 436.0
      },
      "score_details": {
        "score": 373.43,
        "details": {
          "description": "min=373.43, mean=373.43, max=373.43, sum=746.86 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # output tokens",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - # eval",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 135.0
      },
      "score_details": {
        "score": 135.0,
        "details": {
          "description": "min=135, mean=135, max=135, sum=270 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - # train",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - truncated",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 436.0
      },
      "score_details": {
        "score": 353.874,
        "details": {
          "description": "min=353.874, mean=353.874, max=353.874, sum=707.748 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - # output tokens",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Chemistry - # eval",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - # train",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - truncated",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 638.0
      },
      "score_details": {
        "score": 549.28,
        "details": {
          "description": "min=549.28, mean=549.28, max=549.28, sum=1098.56 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - # output tokens",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "Computer Security - # eval",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - # train",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - truncated",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 464.0
      },
      "score_details": {
        "score": 378.51,
        "details": {
          "description": "min=378.51, mean=378.51, max=378.51, sum=757.02 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - # output tokens",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - # eval",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 114.0
      },
      "score_details": {
        "score": 114.0,
        "details": {
          "description": "min=114, mean=114, max=114, sum=228 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - # train",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - truncated",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 698.0
      },
      "score_details": {
        "score": 614.421,
        "details": {
          "description": "min=614.421, mean=614.421, max=614.421, sum=1228.842 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - # output tokens",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - # eval",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - # train",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - truncated",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 494.0
      },
      "score_details": {
        "score": 399.71,
        "details": {
          "description": "min=399.71, mean=399.71, max=399.71, sum=799.42 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - # output tokens",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # eval",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 108.0
      },
      "score_details": {
        "score": 108.0,
        "details": {
          "description": "min=108, mean=108, max=108, sum=216 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # train",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - truncated",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 497.0
      },
      "score_details": {
        "score": 394.63,
        "details": {
          "description": "min=394.63, mean=394.63, max=394.63, sum=789.259 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # output tokens",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - # eval",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 311.0
      },
      "score_details": {
        "score": 311.0,
        "details": {
          "description": "min=311, mean=311, max=311, sum=622 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - # train",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - truncated",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 425.0
      },
      "score_details": {
        "score": 329.084,
        "details": {
          "description": "min=329.084, mean=329.084, max=329.084, sum=658.167 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - # output tokens",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # eval",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 272.0
      },
      "score_details": {
        "score": 272.0,
        "details": {
          "description": "min=272, mean=272, max=272, sum=544 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # train",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - truncated",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1340.0
      },
      "score_details": {
        "score": 1094.489,
        "details": {
          "description": "min=1094.489, mean=1094.489, max=1094.489, sum=2188.978 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # eval",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # train",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - truncated",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 521.0
      },
      "score_details": {
        "score": 422.79,
        "details": {
          "description": "min=422.79, mean=422.79, max=422.79, sum=845.58 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # output tokens",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - # eval",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 152.0
      },
      "score_details": {
        "score": 152.0,
        "details": {
          "description": "min=152, mean=152, max=152, sum=304 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - # train",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - truncated",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 691.0
      },
      "score_details": {
        "score": 579.684,
        "details": {
          "description": "min=579.684, mean=579.684, max=579.684, sum=1159.368 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - # output tokens",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - # eval",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - # train",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - truncated",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 684.0
      },
      "score_details": {
        "score": 569.52,
        "details": {
          "description": "min=569.52, mean=569.52, max=569.52, sum=1139.04 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - # output tokens",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # eval",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 265.0
      },
      "score_details": {
        "score": 265.0,
        "details": {
          "description": "min=265, mean=265, max=265, sum=530 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # train",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - truncated",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 498.0
      },
      "score_details": {
        "score": 397.928,
        "details": {
          "description": "min=397.928, mean=397.928, max=397.928, sum=795.857 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # output tokens",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "College Biology - # eval",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 144.0
      },
      "score_details": {
        "score": 144.0,
        "details": {
          "description": "min=144, mean=144, max=144, sum=288 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - # train",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - truncated",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 563.0
      },
      "score_details": {
        "score": 473.875,
        "details": {
          "description": "min=473.875, mean=473.875, max=473.875, sum=947.75 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - # output tokens",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Computer Science - # eval",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - # train",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - truncated",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 913.0
      },
      "score_details": {
        "score": 828.29,
        "details": {
          "description": "min=828.29, mean=828.29, max=828.29, sum=1656.58 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - # output tokens",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Mathematics - # eval",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - # train",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - truncated",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 670.0
      },
      "score_details": {
        "score": 594.51,
        "details": {
          "description": "min=594.51, mean=594.51, max=594.51, sum=1189.02 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - # output tokens",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Medicine - # eval",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 173.0
      },
      "score_details": {
        "score": 173.0,
        "details": {
          "description": "min=173, mean=173, max=173, sum=346 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - # train",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - truncated",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 618.0
      },
      "score_details": {
        "score": 502.705,
        "details": {
          "description": "min=502.705, mean=502.705, max=502.705, sum=1005.41 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - # output tokens",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Physics - # eval",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 102.0
      },
      "score_details": {
        "score": 102.0,
        "details": {
          "description": "min=102, mean=102, max=102, sum=204 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - # train",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - truncated",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 562.0
      },
      "score_details": {
        "score": 503.569,
        "details": {
          "description": "min=503.569, mean=503.569, max=503.569, sum=1007.137 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - # output tokens",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # eval",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 235.0
      },
      "score_details": {
        "score": 235.0,
        "details": {
          "description": "min=235, mean=235, max=235, sum=470 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # train",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - truncated",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 389.0
      },
      "score_details": {
        "score": 304.834,
        "details": {
          "description": "min=304.834, mean=304.834, max=304.834, sum=609.668 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # output tokens",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # eval",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 145.0
      },
      "score_details": {
        "score": 145.0,
        "details": {
          "description": "min=145, mean=145, max=145, sum=290 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # train",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - truncated",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 512.0
      },
      "score_details": {
        "score": 435.607,
        "details": {
          "description": "min=435.607, mean=435.607, max=435.607, sum=871.214 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # output tokens",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # eval",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 378.0
      },
      "score_details": {
        "score": 378.0,
        "details": {
          "description": "min=378, mean=378, max=378, sum=756 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # train",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - truncated",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 624.0
      },
      "score_details": {
        "score": 531.854,
        "details": {
          "description": "min=531.854, mean=531.854, max=531.854, sum=1063.709 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # output tokens",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - # eval",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 126.0
      },
      "score_details": {
        "score": 126.0,
        "details": {
          "description": "min=126, mean=126, max=126, sum=252 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - # train",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - truncated",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 729.0
      },
      "score_details": {
        "score": 601.778,
        "details": {
          "description": "min=601.778, mean=601.778, max=601.778, sum=1203.556 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - # output tokens",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School Biology - # eval",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 310.0
      },
      "score_details": {
        "score": 310.0,
        "details": {
          "description": "min=310, mean=310, max=310, sum=620 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - # train",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - truncated",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 611.0
      },
      "score_details": {
        "score": 513.671,
        "details": {
          "description": "min=513.671, mean=513.671, max=513.671, sum=1027.342 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # eval",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 203.0
      },
      "score_details": {
        "score": 203.0,
        "details": {
          "description": "min=203, mean=203, max=203, sum=406 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # train",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - truncated",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 583.0
      },
      "score_details": {
        "score": 496.704,
        "details": {
          "description": "min=496.704, mean=496.704, max=496.704, sum=993.409 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # eval",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # train",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - truncated",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 999.0
      },
      "score_details": {
        "score": 867.78,
        "details": {
          "description": "min=867.78, mean=867.78, max=867.78, sum=1735.56 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School European History - # eval",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 165.0
      },
      "score_details": {
        "score": 165.0,
        "details": {
          "description": "min=165, mean=165, max=165, sum=330 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - # train",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - truncated",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 3169.0
      },
      "score_details": {
        "score": 2797.885,
        "details": {
          "description": "min=2797.885, mean=2797.885, max=2797.885, sum=5595.77 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School Geography - # eval",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 198.0
      },
      "score_details": {
        "score": 198.0,
        "details": {
          "description": "min=198, mean=198, max=198, sum=396 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - # train",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - truncated",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 478.0
      },
      "score_details": {
        "score": 372.035,
        "details": {
          "description": "min=372.035, mean=372.035, max=372.035, sum=744.071 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # eval",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 193.0
      },
      "score_details": {
        "score": 193.0,
        "details": {
          "description": "min=193, mean=193, max=193, sum=386 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # train",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - truncated",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 557.0
      },
      "score_details": {
        "score": 465.824,
        "details": {
          "description": "min=465.824, mean=465.824, max=465.824, sum=931.648 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # eval",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 390.0
      },
      "score_details": {
        "score": 390.0,
        "details": {
          "description": "min=390, mean=390, max=390, sum=780 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # train",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - truncated",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 472.0
      },
      "score_details": {
        "score": 370.908,
        "details": {
          "description": "min=370.908, mean=370.908, max=370.908, sum=741.815 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # eval",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 270.0
      },
      "score_details": {
        "score": 270.0,
        "details": {
          "description": "min=270, mean=270, max=270, sum=540 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # train",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - truncated",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 595.0
      },
      "score_details": {
        "score": 532.356,
        "details": {
          "description": "min=532.356, mean=532.356, max=532.356, sum=1064.711 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # eval",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 238.0
      },
      "score_details": {
        "score": 238.0,
        "details": {
          "description": "min=238, mean=238, max=238, sum=476 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # train",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - truncated",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 486.0
      },
      "score_details": {
        "score": 399.013,
        "details": {
          "description": "min=399.013, mean=399.013, max=399.013, sum=798.025 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Physics - # eval",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 151.0
      },
      "score_details": {
        "score": 151.0,
        "details": {
          "description": "min=151, mean=151, max=151, sum=302 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - # train",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - truncated",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 633.0
      },
      "score_details": {
        "score": 560.457,
        "details": {
          "description": "min=560.457, mean=560.457, max=560.457, sum=1120.914 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Psychology - # eval",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 545.0
      },
      "score_details": {
        "score": 545.0,
        "details": {
          "description": "min=545, mean=545, max=545, sum=1090 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - # train",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - truncated",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 595.0
      },
      "score_details": {
        "score": 495.242,
        "details": {
          "description": "min=495.242, mean=495.242, max=495.242, sum=990.484 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Statistics - # eval",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 216.0
      },
      "score_details": {
        "score": 216.0,
        "details": {
          "description": "min=216, mean=216, max=216, sum=432 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - # train",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - truncated",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 924.0
      },
      "score_details": {
        "score": 795.639,
        "details": {
          "description": "min=795.639, mean=795.639, max=795.639, sum=1591.278 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School US History - # eval",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 204.0
      },
      "score_details": {
        "score": 204.0,
        "details": {
          "description": "min=204, mean=204, max=204, sum=408 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - # train",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - truncated",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 2545.0
      },
      "score_details": {
        "score": 2217.809,
        "details": {
          "description": "min=2217.809, mean=2217.809, max=2217.809, sum=4435.618 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School World History - # eval",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 237.0
      },
      "score_details": {
        "score": 237.0,
        "details": {
          "description": "min=237, mean=237, max=237, sum=474 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - # train",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - truncated",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1648.0
      },
      "score_details": {
        "score": 1428.173,
        "details": {
          "description": "min=1428.173, mean=1428.173, max=1428.173, sum=2856.346 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Aging - # eval",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 223.0
      },
      "score_details": {
        "score": 223.0,
        "details": {
          "description": "min=223, mean=223, max=223, sum=446 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - # train",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - truncated",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 401.0
      },
      "score_details": {
        "score": 319.888,
        "details": {
          "description": "min=319.888, mean=319.888, max=319.888, sum=639.776 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - # output tokens",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # eval",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 131.0
      },
      "score_details": {
        "score": 131.0,
        "details": {
          "description": "min=131, mean=131, max=131, sum=262 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # train",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - truncated",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 437.0
      },
      "score_details": {
        "score": 341.168,
        "details": {
          "description": "min=341.168, mean=341.168, max=341.168, sum=682.336 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # output tokens",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - # eval",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 121.0
      },
      "score_details": {
        "score": 121.0,
        "details": {
          "description": "min=121, mean=121, max=121, sum=242 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - # train",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - truncated",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 739.0
      },
      "score_details": {
        "score": 639.818,
        "details": {
          "description": "min=639.818, mean=639.818, max=639.818, sum=1279.636 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - # output tokens",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # eval",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 163.0
      },
      "score_details": {
        "score": 163.0,
        "details": {
          "description": "min=163, mean=163, max=163, sum=326 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # train",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - truncated",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 536.0
      },
      "score_details": {
        "score": 449.564,
        "details": {
          "description": "min=449.564, mean=449.564, max=449.564, sum=899.129 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # output tokens",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - # eval",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 112.0
      },
      "score_details": {
        "score": 112.0,
        "details": {
          "description": "min=112, mean=112, max=112, sum=224 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - # train",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - truncated",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 745.0
      },
      "score_details": {
        "score": 668.054,
        "details": {
          "description": "min=668.054, mean=668.054, max=668.054, sum=1336.107 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - # output tokens",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - # eval",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 103.0
      },
      "score_details": {
        "score": 103.0,
        "details": {
          "description": "min=103, mean=103, max=103, sum=206 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - # train",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - truncated",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 367.0
      },
      "score_details": {
        "score": 283.786,
        "details": {
          "description": "min=283.786, mean=283.786, max=283.786, sum=567.573 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - # output tokens",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - # eval",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 234.0
      },
      "score_details": {
        "score": 234.0,
        "details": {
          "description": "min=234, mean=234, max=234, sum=468 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - # train",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - truncated",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 514.0
      },
      "score_details": {
        "score": 404.218,
        "details": {
          "description": "min=404.218, mean=404.218, max=404.218, sum=808.436 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - # output tokens",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # eval",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # train",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - truncated",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 420.0
      },
      "score_details": {
        "score": 340.99,
        "details": {
          "description": "min=340.99, mean=340.99, max=340.99, sum=681.98 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # output tokens",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # eval",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 783.0
      },
      "score_details": {
        "score": 783.0,
        "details": {
          "description": "min=783, mean=783, max=783, sum=1566 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # train",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - truncated",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 394.0
      },
      "score_details": {
        "score": 299.911,
        "details": {
          "description": "min=299.911, mean=299.911, max=299.911, sum=599.821 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # output tokens",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # eval",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 346.0
      },
      "score_details": {
        "score": 346.0,
        "details": {
          "description": "min=346, mean=346, max=346, sum=692 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # train",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - truncated",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 577.0
      },
      "score_details": {
        "score": 476.113,
        "details": {
          "description": "min=476.113, mean=476.113, max=476.113, sum=952.225 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # output tokens",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # eval",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 895.0
      },
      "score_details": {
        "score": 895.0,
        "details": {
          "description": "min=895, mean=895, max=895, sum=1790 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # train",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - truncated",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 766.0
      },
      "score_details": {
        "score": 656.455,
        "details": {
          "description": "min=656.455, mean=656.455, max=656.455, sum=1312.909 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # output tokens",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - # eval",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 306.0
      },
      "score_details": {
        "score": 306.0,
        "details": {
          "description": "min=306, mean=306, max=306, sum=612 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - # train",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - truncated",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 705.0
      },
      "score_details": {
        "score": 586.814,
        "details": {
          "description": "min=586.814, mean=586.814, max=586.814, sum=1173.627 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - # output tokens",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - # eval",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 324.0
      },
      "score_details": {
        "score": 324.0,
        "details": {
          "description": "min=324, mean=324, max=324, sum=648 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - # train",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - truncated",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 629.0
      },
      "score_details": {
        "score": 514.528,
        "details": {
          "description": "min=514.528, mean=514.528, max=514.528, sum=1029.056 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - # output tokens",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # eval",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 282.0
      },
      "score_details": {
        "score": 282.0,
        "details": {
          "description": "min=282, mean=282, max=282, sum=564 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # train",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - truncated",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 833.0
      },
      "score_details": {
        "score": 658.585,
        "details": {
          "description": "min=658.585, mean=658.585, max=658.585, sum=1317.17 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Law - # eval",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1534.0
      },
      "score_details": {
        "score": 1534.0,
        "details": {
          "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - # train",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - truncated",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1925.0
      },
      "score_details": {
        "score": 1637.601,
        "details": {
          "description": "min=1637.601, mean=1637.601, max=1637.601, sum=3275.202 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # eval",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 612.0
      },
      "score_details": {
        "score": 612.0,
        "details": {
          "description": "min=612, mean=612, max=612, sum=1224 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # train",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - truncated",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 669.0
      },
      "score_details": {
        "score": 575.098,
        "details": {
          "description": "min=575.098, mean=575.098, max=575.098, sum=1150.196 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Public Relations - # eval",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 110.0
      },
      "score_details": {
        "score": 110.0,
        "details": {
          "description": "min=110, mean=110, max=110, sum=220 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - # train",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - truncated",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 498.0
      },
      "score_details": {
        "score": 405.318,
        "details": {
          "description": "min=405.318, mean=405.318, max=405.318, sum=810.636 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - # output tokens",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - # eval",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 245.0
      },
      "score_details": {
        "score": 245.0,
        "details": {
          "description": "min=245, mean=245, max=245, sum=490 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - # train",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - truncated",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1387.0
      },
      "score_details": {
        "score": 1164.473,
        "details": {
          "description": "min=1164.473, mean=1164.473, max=1164.473, sum=2328.947 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - # output tokens",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - # eval",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 201.0
      },
      "score_details": {
        "score": 201.0,
        "details": {
          "description": "min=201, mean=201, max=201, sum=402 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - # train",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - truncated",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 533.0
      },
      "score_details": {
        "score": 445.517,
        "details": {
          "description": "min=445.517, mean=445.517, max=445.517, sum=891.035 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - # output tokens",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - # eval",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 166.0
      },
      "score_details": {
        "score": 166.0,
        "details": {
          "description": "min=166, mean=166, max=166, sum=332 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - # train",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - truncated",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 428.0
      },
      "score_details": {
        "score": 343.018,
        "details": {
          "description": "min=343.018, mean=343.018, max=343.018, sum=686.036 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - # output tokens",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - # eval",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 171.0
      },
      "score_details": {
        "score": 171.0,
        "details": {
          "description": "min=171, mean=171, max=171, sum=342 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - # train",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - truncated",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 359.0
      },
      "score_details": {
        "score": 274.52,
        "details": {
          "description": "min=274.52, mean=274.52, max=274.52, sum=549.041 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - # output tokens",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    }
  ]
}
