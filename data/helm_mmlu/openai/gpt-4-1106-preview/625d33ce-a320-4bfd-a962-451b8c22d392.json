{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_mmlu/openai_gpt-4-1106-preview/1770835937.459157",
  "retrieved_timestamp": "1770835937.459157",
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "GPT-4 Turbo 1106 preview",
    "id": "openai/gpt-4-1106-preview",
    "developer": "openai",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU All Subjects",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.796,
        "details": {
          "description": "min=0.093, mean=0.796, max=0.979, sum=90.688 (114)",
          "tab": "Accuracy",
          "MMLU All Subjects - Observed inference time (s)": {
            "description": "min=0.397, mean=0.537, max=0.852, sum=61.247 (114)",
            "tab": "Efficiency",
            "score": 0.5372507053364665
          },
          "MMLU All Subjects - # eval": {
            "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
            "tab": "General information",
            "score": 246.35087719298247
          },
          "MMLU All Subjects - # train": {
            "description": "min=5, mean=5, max=5, sum=570 (114)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU All Subjects - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU All Subjects - # prompt tokens": {
            "description": "min=268.561, mean=607.852, max=2791.073, sum=69295.086 (114)",
            "tab": "General information",
            "score": 607.851634217556
          },
          "MMLU All Subjects - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=114 (114)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "anatomy",
            "astronomy",
            "business_ethics",
            "clinical_knowledge",
            "college_biology",
            "college_chemistry",
            "college_computer_science",
            "college_mathematics",
            "college_medicine",
            "college_physics",
            "computer_security",
            "conceptual_physics",
            "econometrics",
            "electrical_engineering",
            "elementary_mathematics",
            "formal_logic",
            "global_facts",
            "high_school_biology",
            "high_school_chemistry",
            "high_school_computer_science",
            "high_school_european_history",
            "high_school_geography",
            "high_school_government_and_politics",
            "high_school_macroeconomics",
            "high_school_mathematics",
            "high_school_microeconomics",
            "high_school_physics",
            "high_school_psychology",
            "high_school_statistics",
            "high_school_us_history",
            "high_school_world_history",
            "human_aging",
            "human_sexuality",
            "international_law",
            "jurisprudence",
            "logical_fallacies",
            "machine_learning",
            "management",
            "marketing",
            "medical_genetics",
            "miscellaneous",
            "moral_disputes",
            "moral_scenarios",
            "nutrition",
            "philosophy",
            "prehistory",
            "professional_accounting",
            "professional_law",
            "professional_medicine",
            "professional_psychology",
            "public_relations",
            "security_studies",
            "sociology",
            "us_foreign_policy",
            "virology",
            "world_religions"
          ],
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": [
            "mmlu_abstract_algebra",
            "mmlu_anatomy",
            "mmlu_astronomy",
            "mmlu_business_ethics",
            "mmlu_clinical_knowledge",
            "mmlu_college_biology",
            "mmlu_college_chemistry",
            "mmlu_college_computer_science",
            "mmlu_college_mathematics",
            "mmlu_college_medicine",
            "mmlu_college_physics",
            "mmlu_computer_security",
            "mmlu_conceptual_physics",
            "mmlu_econometrics",
            "mmlu_electrical_engineering",
            "mmlu_elementary_mathematics",
            "mmlu_formal_logic",
            "mmlu_global_facts",
            "mmlu_high_school_biology",
            "mmlu_high_school_chemistry",
            "mmlu_high_school_computer_science",
            "mmlu_high_school_european_history",
            "mmlu_high_school_geography",
            "mmlu_high_school_government_and_politics",
            "mmlu_high_school_macroeconomics",
            "mmlu_high_school_mathematics",
            "mmlu_high_school_microeconomics",
            "mmlu_high_school_physics",
            "mmlu_high_school_psychology",
            "mmlu_high_school_statistics",
            "mmlu_high_school_us_history",
            "mmlu_high_school_world_history",
            "mmlu_human_aging",
            "mmlu_human_sexuality",
            "mmlu_international_law",
            "mmlu_jurisprudence",
            "mmlu_logical_fallacies",
            "mmlu_machine_learning",
            "mmlu_management",
            "mmlu_marketing",
            "mmlu_medical_genetics",
            "mmlu_miscellaneous",
            "mmlu_moral_disputes",
            "mmlu_moral_scenarios",
            "mmlu_nutrition",
            "mmlu_philosophy",
            "mmlu_prehistory",
            "mmlu_professional_accounting",
            "mmlu_professional_law",
            "mmlu_professional_medicine",
            "mmlu_professional_psychology",
            "mmlu_public_relations",
            "mmlu_security_studies",
            "mmlu_sociology",
            "mmlu_us_foreign_policy",
            "mmlu_virology",
            "mmlu_world_religions"
          ]
        }
      }
    },
    {
      "evaluation_name": "Abstract Algebra",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Abstract Algebra",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.53,
        "details": {
          "description": "min=0.53, mean=0.53, max=0.53, sum=1.06 (2)",
          "tab": "Accuracy",
          "Abstract Algebra - Observed inference time (s)": {
            "description": "min=0.425, mean=0.425, max=0.425, sum=0.85 (2)",
            "tab": "Efficiency",
            "score": 0.42504594564437864
          },
          "Abstract Algebra - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Abstract Algebra - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Abstract Algebra - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Abstract Algebra - # prompt tokens": {
            "description": "min=366.44, mean=366.44, max=366.44, sum=732.88 (2)",
            "tab": "General information",
            "score": 366.44
          },
          "Abstract Algebra - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "abstract_algebra",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_abstract_algebra"
        }
      }
    },
    {
      "evaluation_name": "Anatomy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Anatomy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.807,
        "details": {
          "description": "min=0.807, mean=0.807, max=0.807, sum=1.615 (2)",
          "tab": "Accuracy",
          "Anatomy - Observed inference time (s)": {
            "description": "min=0.569, mean=0.569, max=0.569, sum=1.138 (2)",
            "tab": "Efficiency",
            "score": 0.5691532982720269
          },
          "Anatomy - # eval": {
            "description": "min=135, mean=135, max=135, sum=270 (2)",
            "tab": "General information",
            "score": 135.0
          },
          "Anatomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Anatomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Anatomy - # prompt tokens": {
            "description": "min=346.978, mean=346.978, max=346.978, sum=693.956 (2)",
            "tab": "General information",
            "score": 346.97777777777776
          },
          "Anatomy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "anatomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_anatomy"
        }
      }
    },
    {
      "evaluation_name": "College Physics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on College Physics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.402,
        "details": {
          "description": "min=0.402, mean=0.402, max=0.402, sum=0.804 (2)",
          "tab": "Accuracy",
          "College Chemistry - Observed inference time (s)": {
            "description": "min=0.457, mean=0.457, max=0.457, sum=0.913 (2)",
            "tab": "Efficiency",
            "score": 0.456736900806427
          },
          "College Biology - Observed inference time (s)": {
            "description": "min=0.444, mean=0.444, max=0.444, sum=0.888 (2)",
            "tab": "Efficiency",
            "score": 0.44404302537441254
          },
          "College Computer Science - Observed inference time (s)": {
            "description": "min=0.516, mean=0.516, max=0.516, sum=1.033 (2)",
            "tab": "Efficiency",
            "score": 0.516348373889923
          },
          "College Mathematics - Observed inference time (s)": {
            "description": "min=0.534, mean=0.534, max=0.534, sum=1.067 (2)",
            "tab": "Efficiency",
            "score": 0.5335026264190674
          },
          "College Medicine - Observed inference time (s)": {
            "description": "min=0.491, mean=0.491, max=0.491, sum=0.982 (2)",
            "tab": "Efficiency",
            "score": 0.4908691348368033
          },
          "College Physics - Observed inference time (s)": {
            "description": "min=0.75, mean=0.75, max=0.75, sum=1.499 (2)",
            "tab": "Efficiency",
            "score": 0.7497045245825076
          },
          "College Chemistry - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Chemistry - # prompt tokens": {
            "description": "min=542.4, mean=542.4, max=542.4, sum=1084.8 (2)",
            "tab": "General information",
            "score": 542.4
          },
          "College Chemistry - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Biology - # eval": {
            "description": "min=144, mean=144, max=144, sum=288 (2)",
            "tab": "General information",
            "score": 144.0
          },
          "College Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # prompt tokens": {
            "description": "min=466.917, mean=466.917, max=466.917, sum=933.833 (2)",
            "tab": "General information",
            "score": 466.9166666666667
          },
          "College Biology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # prompt tokens": {
            "description": "min=821.39, mean=821.39, max=821.39, sum=1642.78 (2)",
            "tab": "General information",
            "score": 821.39
          },
          "College Computer Science - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Mathematics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # prompt tokens": {
            "description": "min=587.52, mean=587.52, max=587.52, sum=1175.04 (2)",
            "tab": "General information",
            "score": 587.52
          },
          "College Mathematics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Medicine - # eval": {
            "description": "min=173, mean=173, max=173, sum=346 (2)",
            "tab": "General information",
            "score": 173.0
          },
          "College Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # prompt tokens": {
            "description": "min=495.728, mean=495.728, max=495.728, sum=991.457 (2)",
            "tab": "General information",
            "score": 495.728323699422
          },
          "College Medicine - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Physics - # eval": {
            "description": "min=102, mean=102, max=102, sum=204 (2)",
            "tab": "General information",
            "score": 102.0
          },
          "College Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # prompt tokens": {
            "description": "min=496.608, mean=496.608, max=496.608, sum=993.216 (2)",
            "tab": "General information",
            "score": 496.6078431372549
          },
          "College Physics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "college_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_college_physics"
        }
      }
    },
    {
      "evaluation_name": "Computer Security",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Computer Security",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.86,
        "details": {
          "description": "min=0.86, mean=0.86, max=0.86, sum=1.72 (2)",
          "tab": "Accuracy",
          "Computer Security - Observed inference time (s)": {
            "description": "min=0.442, mean=0.442, max=0.442, sum=0.884 (2)",
            "tab": "Efficiency",
            "score": 0.4418716287612915
          },
          "Computer Security - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Computer Security - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Computer Security - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Computer Security - # prompt tokens": {
            "description": "min=371.54, mean=371.54, max=371.54, sum=743.08 (2)",
            "tab": "General information",
            "score": 371.54
          },
          "Computer Security - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "computer_security",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_computer_security"
        }
      }
    },
    {
      "evaluation_name": "Econometrics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Econometrics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.675,
        "details": {
          "description": "min=0.675, mean=0.675, max=0.675, sum=1.351 (2)",
          "tab": "Accuracy",
          "Econometrics - Observed inference time (s)": {
            "description": "min=0.515, mean=0.515, max=0.515, sum=1.03 (2)",
            "tab": "Efficiency",
            "score": 0.5149402095560442
          },
          "Econometrics - # eval": {
            "description": "min=114, mean=114, max=114, sum=228 (2)",
            "tab": "General information",
            "score": 114.0
          },
          "Econometrics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Econometrics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Econometrics - # prompt tokens": {
            "description": "min=607.43, mean=607.43, max=607.43, sum=1214.86 (2)",
            "tab": "General information",
            "score": 607.4298245614035
          },
          "Econometrics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "econometrics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_econometrics"
        }
      }
    },
    {
      "evaluation_name": "Global Facts",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Global Facts",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.58,
        "details": {
          "description": "min=0.58, mean=0.58, max=0.58, sum=1.16 (2)",
          "tab": "Accuracy",
          "Global Facts - Observed inference time (s)": {
            "description": "min=0.486, mean=0.486, max=0.486, sum=0.973 (2)",
            "tab": "Efficiency",
            "score": 0.4863955807685852
          },
          "Global Facts - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Global Facts - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Global Facts - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Global Facts - # prompt tokens": {
            "description": "min=392.71, mean=392.71, max=392.71, sum=785.42 (2)",
            "tab": "General information",
            "score": 392.71
          },
          "Global Facts - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "global_facts",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_global_facts"
        }
      }
    },
    {
      "evaluation_name": "Jurisprudence",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Jurisprudence",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.889,
        "details": {
          "description": "min=0.889, mean=0.889, max=0.889, sum=1.778 (2)",
          "tab": "Accuracy",
          "Jurisprudence - Observed inference time (s)": {
            "description": "min=0.731, mean=0.731, max=0.731, sum=1.462 (2)",
            "tab": "Efficiency",
            "score": 0.7311423023541769
          },
          "Jurisprudence - # eval": {
            "description": "min=108, mean=108, max=108, sum=216 (2)",
            "tab": "General information",
            "score": 108.0
          },
          "Jurisprudence - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Jurisprudence - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Jurisprudence - # prompt tokens": {
            "description": "min=387.639, mean=387.639, max=387.639, sum=775.278 (2)",
            "tab": "General information",
            "score": 387.6388888888889
          },
          "Jurisprudence - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "jurisprudence",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_jurisprudence"
        }
      }
    },
    {
      "evaluation_name": "Philosophy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Philosophy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.852,
        "details": {
          "description": "min=0.852, mean=0.852, max=0.852, sum=1.704 (2)",
          "tab": "Accuracy",
          "Philosophy - Observed inference time (s)": {
            "description": "min=0.486, mean=0.486, max=0.486, sum=0.973 (2)",
            "tab": "Efficiency",
            "score": 0.4863421380328212
          },
          "Philosophy - # eval": {
            "description": "min=311, mean=311, max=311, sum=622 (2)",
            "tab": "General information",
            "score": 311.0
          },
          "Philosophy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Philosophy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Philosophy - # prompt tokens": {
            "description": "min=322.084, mean=322.084, max=322.084, sum=644.167 (2)",
            "tab": "General information",
            "score": 322.08360128617363
          },
          "Philosophy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "philosophy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_philosophy"
        }
      }
    },
    {
      "evaluation_name": "Professional Psychology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Professional Psychology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.887,
        "details": {
          "description": "min=0.887, mean=0.887, max=0.887, sum=1.775 (2)",
          "tab": "Accuracy",
          "Professional Medicine - Observed inference time (s)": {
            "description": "min=0.551, mean=0.551, max=0.551, sum=1.103 (2)",
            "tab": "Efficiency",
            "score": 0.5514215528964996
          },
          "Professional Accounting - Observed inference time (s)": {
            "description": "min=0.54, mean=0.54, max=0.54, sum=1.079 (2)",
            "tab": "Efficiency",
            "score": 0.5395518828791084
          },
          "Professional Law - Observed inference time (s)": {
            "description": "min=0.616, mean=0.616, max=0.616, sum=1.232 (2)",
            "tab": "Efficiency",
            "score": 0.6162493903447317
          },
          "Professional Psychology - Observed inference time (s)": {
            "description": "min=0.563, mean=0.563, max=0.563, sum=1.126 (2)",
            "tab": "Efficiency",
            "score": 0.5629562961509804
          },
          "Professional Medicine - # eval": {
            "description": "min=272, mean=272, max=272, sum=544 (2)",
            "tab": "General information",
            "score": 272.0
          },
          "Professional Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Medicine - # prompt tokens": {
            "description": "min=1087.585, mean=1087.585, max=1087.585, sum=2175.169 (2)",
            "tab": "General information",
            "score": 1087.5845588235295
          },
          "Professional Medicine - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Professional Accounting - # eval": {
            "description": "min=282, mean=282, max=282, sum=564 (2)",
            "tab": "General information",
            "score": 282.0
          },
          "Professional Accounting - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Accounting - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # prompt tokens": {
            "description": "min=651.592, mean=651.592, max=651.592, sum=1303.184 (2)",
            "tab": "General information",
            "score": 651.5921985815603
          },
          "Professional Accounting - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Professional Law - # eval": {
            "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
            "tab": "General information",
            "score": 1534.0
          },
          "Professional Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # prompt tokens": {
            "description": "min=1630.787, mean=1630.787, max=1630.787, sum=3261.574 (2)",
            "tab": "General information",
            "score": 1630.7868318122555
          },
          "Professional Law - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Professional Psychology - # eval": {
            "description": "min=612, mean=612, max=612, sum=1224 (2)",
            "tab": "General information",
            "score": 612.0
          },
          "Professional Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # prompt tokens": {
            "description": "min=568.114, mean=568.114, max=568.114, sum=1136.229 (2)",
            "tab": "General information",
            "score": 568.1143790849674
          },
          "Professional Psychology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "professional_psychology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_professional_psychology"
        }
      }
    },
    {
      "evaluation_name": "Us Foreign Policy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Us Foreign Policy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.96,
        "details": {
          "description": "min=0.96, mean=0.96, max=0.96, sum=1.92 (2)",
          "tab": "Accuracy",
          "Us Foreign Policy - Observed inference time (s)": {
            "description": "min=0.397, mean=0.397, max=0.397, sum=0.794 (2)",
            "tab": "Efficiency",
            "score": 0.39724321842193605
          },
          "Us Foreign Policy - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Us Foreign Policy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Us Foreign Policy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Us Foreign Policy - # prompt tokens": {
            "description": "min=415.79, mean=415.79, max=415.79, sum=831.58 (2)",
            "tab": "General information",
            "score": 415.79
          },
          "Us Foreign Policy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "us_foreign_policy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_us_foreign_policy"
        }
      }
    },
    {
      "evaluation_name": "Astronomy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Astronomy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.941,
        "details": {
          "description": "min=0.941, mean=0.941, max=0.941, sum=1.882 (2)",
          "tab": "Accuracy",
          "Astronomy - Observed inference time (s)": {
            "description": "min=0.519, mean=0.519, max=0.519, sum=1.038 (2)",
            "tab": "Efficiency",
            "score": 0.5192367622726842
          },
          "Astronomy - # eval": {
            "description": "min=152, mean=152, max=152, sum=304 (2)",
            "tab": "General information",
            "score": 152.0
          },
          "Astronomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Astronomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Astronomy - # prompt tokens": {
            "description": "min=572.691, mean=572.691, max=572.691, sum=1145.382 (2)",
            "tab": "General information",
            "score": 572.6907894736842
          },
          "Astronomy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "astronomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_astronomy"
        }
      }
    },
    {
      "evaluation_name": "Business Ethics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Business Ethics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.78,
        "details": {
          "description": "min=0.78, mean=0.78, max=0.78, sum=1.56 (2)",
          "tab": "Accuracy",
          "Business Ethics - Observed inference time (s)": {
            "description": "min=0.495, mean=0.495, max=0.495, sum=0.99 (2)",
            "tab": "Efficiency",
            "score": 0.49495640993118284
          },
          "Business Ethics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Business Ethics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Business Ethics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Business Ethics - # prompt tokens": {
            "description": "min=562.52, mean=562.52, max=562.52, sum=1125.04 (2)",
            "tab": "General information",
            "score": 562.52
          },
          "Business Ethics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "business_ethics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_business_ethics"
        }
      }
    },
    {
      "evaluation_name": "Clinical Knowledge",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Clinical Knowledge",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.864,
        "details": {
          "description": "min=0.864, mean=0.864, max=0.864, sum=1.728 (2)",
          "tab": "Accuracy",
          "Clinical Knowledge - Observed inference time (s)": {
            "description": "min=0.642, mean=0.642, max=0.642, sum=1.284 (2)",
            "tab": "Efficiency",
            "score": 0.6421918509141454
          },
          "Clinical Knowledge - # eval": {
            "description": "min=265, mean=265, max=265, sum=530 (2)",
            "tab": "General information",
            "score": 265.0
          },
          "Clinical Knowledge - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Clinical Knowledge - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Clinical Knowledge - # prompt tokens": {
            "description": "min=390.947, mean=390.947, max=390.947, sum=781.894 (2)",
            "tab": "General information",
            "score": 390.94716981132075
          },
          "Clinical Knowledge - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "clinical_knowledge",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_clinical_knowledge"
        }
      }
    },
    {
      "evaluation_name": "Conceptual Physics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Conceptual Physics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.894,
        "details": {
          "description": "min=0.894, mean=0.894, max=0.894, sum=1.787 (2)",
          "tab": "Accuracy",
          "Conceptual Physics - Observed inference time (s)": {
            "description": "min=0.586, mean=0.586, max=0.586, sum=1.172 (2)",
            "tab": "Efficiency",
            "score": 0.5859095319788507
          },
          "Conceptual Physics - # eval": {
            "description": "min=235, mean=235, max=235, sum=470 (2)",
            "tab": "General information",
            "score": 235.0
          },
          "Conceptual Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Conceptual Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Conceptual Physics - # prompt tokens": {
            "description": "min=297.838, mean=297.838, max=297.838, sum=595.677 (2)",
            "tab": "General information",
            "score": 297.83829787234043
          },
          "Conceptual Physics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "conceptual_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_conceptual_physics"
        }
      }
    },
    {
      "evaluation_name": "Electrical Engineering",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Electrical Engineering",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.772,
        "details": {
          "description": "min=0.772, mean=0.772, max=0.772, sum=1.545 (2)",
          "tab": "Accuracy",
          "Electrical Engineering - Observed inference time (s)": {
            "description": "min=0.507, mean=0.507, max=0.507, sum=1.014 (2)",
            "tab": "Efficiency",
            "score": 0.5071375830420133
          },
          "Electrical Engineering - # eval": {
            "description": "min=145, mean=145, max=145, sum=290 (2)",
            "tab": "General information",
            "score": 145.0
          },
          "Electrical Engineering - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Electrical Engineering - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Electrical Engineering - # prompt tokens": {
            "description": "min=433.641, mean=433.641, max=433.641, sum=867.283 (2)",
            "tab": "General information",
            "score": 433.6413793103448
          },
          "Electrical Engineering - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "electrical_engineering",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_electrical_engineering"
        }
      }
    },
    {
      "evaluation_name": "Elementary Mathematics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Elementary Mathematics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.638,
        "details": {
          "description": "min=0.638, mean=0.638, max=0.638, sum=1.275 (2)",
          "tab": "Accuracy",
          "Elementary Mathematics - Observed inference time (s)": {
            "description": "min=0.486, mean=0.486, max=0.486, sum=0.972 (2)",
            "tab": "Efficiency",
            "score": 0.48600239034682985
          },
          "Elementary Mathematics - # eval": {
            "description": "min=378, mean=378, max=378, sum=756 (2)",
            "tab": "General information",
            "score": 378.0
          },
          "Elementary Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Elementary Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Elementary Mathematics - # prompt tokens": {
            "description": "min=524.862, mean=524.862, max=524.862, sum=1049.725 (2)",
            "tab": "General information",
            "score": 524.8624338624338
          },
          "Elementary Mathematics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "elementary_mathematics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_elementary_mathematics"
        }
      }
    },
    {
      "evaluation_name": "Formal Logic",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Formal Logic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.651,
        "details": {
          "description": "min=0.651, mean=0.651, max=0.651, sum=1.302 (2)",
          "tab": "Accuracy",
          "Formal Logic - Observed inference time (s)": {
            "description": "min=0.491, mean=0.491, max=0.491, sum=0.983 (2)",
            "tab": "Efficiency",
            "score": 0.4912937557886517
          },
          "Formal Logic - # eval": {
            "description": "min=126, mean=126, max=126, sum=252 (2)",
            "tab": "General information",
            "score": 126.0
          },
          "Formal Logic - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Formal Logic - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Formal Logic - # prompt tokens": {
            "description": "min=599.762, mean=599.762, max=599.762, sum=1199.524 (2)",
            "tab": "General information",
            "score": 599.7619047619048
          },
          "Formal Logic - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "formal_logic",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_formal_logic"
        }
      }
    },
    {
      "evaluation_name": "High School World History",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on High School World History",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.958,
        "details": {
          "description": "min=0.958, mean=0.958, max=0.958, sum=1.916 (2)",
          "tab": "Accuracy",
          "High School Biology - Observed inference time (s)": {
            "description": "min=0.572, mean=0.572, max=0.572, sum=1.144 (2)",
            "tab": "Efficiency",
            "score": 0.5719813362244637
          },
          "High School Chemistry - Observed inference time (s)": {
            "description": "min=0.656, mean=0.656, max=0.656, sum=1.312 (2)",
            "tab": "Efficiency",
            "score": 0.6560086276143643
          },
          "High School Computer Science - Observed inference time (s)": {
            "description": "min=0.568, mean=0.568, max=0.568, sum=1.137 (2)",
            "tab": "Efficiency",
            "score": 0.5683712005615235
          },
          "High School European History - Observed inference time (s)": {
            "description": "min=0.64, mean=0.64, max=0.64, sum=1.28 (2)",
            "tab": "Efficiency",
            "score": 0.6399081995992949
          },
          "High School Geography - Observed inference time (s)": {
            "description": "min=0.471, mean=0.471, max=0.471, sum=0.943 (2)",
            "tab": "Efficiency",
            "score": 0.47148694173254146
          },
          "High School Government And Politics - Observed inference time (s)": {
            "description": "min=0.42, mean=0.42, max=0.42, sum=0.84 (2)",
            "tab": "Efficiency",
            "score": 0.420210268831006
          },
          "High School Macroeconomics - Observed inference time (s)": {
            "description": "min=0.445, mean=0.445, max=0.445, sum=0.89 (2)",
            "tab": "Efficiency",
            "score": 0.4451567802673731
          },
          "High School Mathematics - Observed inference time (s)": {
            "description": "min=0.434, mean=0.434, max=0.434, sum=0.868 (2)",
            "tab": "Efficiency",
            "score": 0.43410645679191306
          },
          "High School Microeconomics - Observed inference time (s)": {
            "description": "min=0.656, mean=0.656, max=0.656, sum=1.312 (2)",
            "tab": "Efficiency",
            "score": 0.6560712812327537
          },
          "High School Physics - Observed inference time (s)": {
            "description": "min=0.574, mean=0.574, max=0.574, sum=1.148 (2)",
            "tab": "Efficiency",
            "score": 0.5739512143545593
          },
          "High School Psychology - Observed inference time (s)": {
            "description": "min=0.446, mean=0.446, max=0.446, sum=0.892 (2)",
            "tab": "Efficiency",
            "score": 0.4460442779261038
          },
          "High School Statistics - Observed inference time (s)": {
            "description": "min=0.586, mean=0.586, max=0.586, sum=1.171 (2)",
            "tab": "Efficiency",
            "score": 0.5855172486216934
          },
          "High School US History - Observed inference time (s)": {
            "description": "min=0.579, mean=0.579, max=0.579, sum=1.158 (2)",
            "tab": "Efficiency",
            "score": 0.5790434245969734
          },
          "High School World History - Observed inference time (s)": {
            "description": "min=0.643, mean=0.643, max=0.643, sum=1.285 (2)",
            "tab": "Efficiency",
            "score": 0.6425194448559596
          },
          "High School Biology - # eval": {
            "description": "min=310, mean=310, max=310, sum=620 (2)",
            "tab": "General information",
            "score": 310.0
          },
          "High School Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Biology - # prompt tokens": {
            "description": "min=506.677, mean=506.677, max=506.677, sum=1013.355 (2)",
            "tab": "General information",
            "score": 506.6774193548387
          },
          "High School Biology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Chemistry - # eval": {
            "description": "min=203, mean=203, max=203, sum=406 (2)",
            "tab": "General information",
            "score": 203.0
          },
          "High School Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # prompt tokens": {
            "description": "min=489.714, mean=489.714, max=489.714, sum=979.429 (2)",
            "tab": "General information",
            "score": 489.7142857142857
          },
          "High School Chemistry - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "High School Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # prompt tokens": {
            "description": "min=860.78, mean=860.78, max=860.78, sum=1721.56 (2)",
            "tab": "General information",
            "score": 860.78
          },
          "High School Computer Science - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School European History - # eval": {
            "description": "min=165, mean=165, max=165, sum=330 (2)",
            "tab": "General information",
            "score": 165.0
          },
          "High School European History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School European History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # prompt tokens": {
            "description": "min=2791.073, mean=2791.073, max=2791.073, sum=5582.145 (2)",
            "tab": "General information",
            "score": 2791.072727272727
          },
          "High School European History - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Geography - # eval": {
            "description": "min=198, mean=198, max=198, sum=396 (2)",
            "tab": "General information",
            "score": 198.0
          },
          "High School Geography - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Geography - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # prompt tokens": {
            "description": "min=365.045, mean=365.045, max=365.045, sum=730.091 (2)",
            "tab": "General information",
            "score": 365.04545454545456
          },
          "High School Geography - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Government And Politics - # eval": {
            "description": "min=193, mean=193, max=193, sum=386 (2)",
            "tab": "General information",
            "score": 193.0
          },
          "High School Government And Politics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Government And Politics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # prompt tokens": {
            "description": "min=458.824, mean=458.824, max=458.824, sum=917.648 (2)",
            "tab": "General information",
            "score": 458.8238341968912
          },
          "High School Government And Politics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Macroeconomics - # eval": {
            "description": "min=390, mean=390, max=390, sum=780 (2)",
            "tab": "General information",
            "score": 390.0
          },
          "High School Macroeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Macroeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # prompt tokens": {
            "description": "min=364.562, mean=364.562, max=364.562, sum=729.123 (2)",
            "tab": "General information",
            "score": 364.5615384615385
          },
          "High School Macroeconomics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Mathematics - # eval": {
            "description": "min=270, mean=270, max=270, sum=540 (2)",
            "tab": "General information",
            "score": 270.0
          },
          "High School Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # prompt tokens": {
            "description": "min=525.374, mean=525.374, max=525.374, sum=1050.748 (2)",
            "tab": "General information",
            "score": 525.3740740740741
          },
          "High School Mathematics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Microeconomics - # eval": {
            "description": "min=238, mean=238, max=238, sum=476 (2)",
            "tab": "General information",
            "score": 238.0
          },
          "High School Microeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Microeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # prompt tokens": {
            "description": "min=392.025, mean=392.025, max=392.025, sum=784.05 (2)",
            "tab": "General information",
            "score": 392.02521008403363
          },
          "High School Microeconomics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Physics - # eval": {
            "description": "min=151, mean=151, max=151, sum=302 (2)",
            "tab": "General information",
            "score": 151.0
          },
          "High School Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # prompt tokens": {
            "description": "min=553.464, mean=553.464, max=553.464, sum=1106.927 (2)",
            "tab": "General information",
            "score": 553.4635761589404
          },
          "High School Physics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Psychology - # eval": {
            "description": "min=545, mean=545, max=545, sum=1090 (2)",
            "tab": "General information",
            "score": 545.0
          },
          "High School Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # prompt tokens": {
            "description": "min=488.246, mean=488.246, max=488.246, sum=976.492 (2)",
            "tab": "General information",
            "score": 488.24587155963303
          },
          "High School Psychology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Statistics - # eval": {
            "description": "min=216, mean=216, max=216, sum=432 (2)",
            "tab": "General information",
            "score": 216.0
          },
          "High School Statistics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Statistics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # prompt tokens": {
            "description": "min=788.699, mean=788.699, max=788.699, sum=1577.398 (2)",
            "tab": "General information",
            "score": 788.699074074074
          },
          "High School Statistics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School US History - # eval": {
            "description": "min=204, mean=204, max=204, sum=408 (2)",
            "tab": "General information",
            "score": 204.0
          },
          "High School US History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School US History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # prompt tokens": {
            "description": "min=2210.809, mean=2210.809, max=2210.809, sum=4421.618 (2)",
            "tab": "General information",
            "score": 2210.8088235294117
          },
          "High School US History - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School World History - # eval": {
            "description": "min=237, mean=237, max=237, sum=474 (2)",
            "tab": "General information",
            "score": 237.0
          },
          "High School World History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School World History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # prompt tokens": {
            "description": "min=1421.27, mean=1421.27, max=1421.27, sum=2842.54 (2)",
            "tab": "General information",
            "score": 1421.2700421940929
          },
          "High School World History - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "high_school_world_history",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_high_school_world_history"
        }
      }
    },
    {
      "evaluation_name": "Human Sexuality",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Human Sexuality",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.908,
        "details": {
          "description": "min=0.908, mean=0.908, max=0.908, sum=1.817 (2)",
          "tab": "Accuracy",
          "Human Aging - Observed inference time (s)": {
            "description": "min=0.472, mean=0.472, max=0.472, sum=0.944 (2)",
            "tab": "Efficiency",
            "score": 0.47213134316585526
          },
          "Human Sexuality - Observed inference time (s)": {
            "description": "min=0.515, mean=0.515, max=0.515, sum=1.03 (2)",
            "tab": "Efficiency",
            "score": 0.5152236923916649
          },
          "Human Aging - # eval": {
            "description": "min=223, mean=223, max=223, sum=446 (2)",
            "tab": "General information",
            "score": 223.0
          },
          "Human Aging - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Aging - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Aging - # prompt tokens": {
            "description": "min=312.906, mean=312.906, max=312.906, sum=625.812 (2)",
            "tab": "General information",
            "score": 312.90582959641256
          },
          "Human Aging - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Human Sexuality - # eval": {
            "description": "min=131, mean=131, max=131, sum=262 (2)",
            "tab": "General information",
            "score": 131.0
          },
          "Human Sexuality - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Sexuality - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # prompt tokens": {
            "description": "min=334.183, mean=334.183, max=334.183, sum=668.366 (2)",
            "tab": "General information",
            "score": 334.1832061068702
          },
          "Human Sexuality - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "human_sexuality",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_human_sexuality"
        }
      }
    },
    {
      "evaluation_name": "International Law",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on International Law",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.926,
        "details": {
          "description": "min=0.926, mean=0.926, max=0.926, sum=1.851 (2)",
          "tab": "Accuracy",
          "International Law - Observed inference time (s)": {
            "description": "min=0.523, mean=0.523, max=0.523, sum=1.046 (2)",
            "tab": "Efficiency",
            "score": 0.5229926621618349
          },
          "International Law - # eval": {
            "description": "min=121, mean=121, max=121, sum=242 (2)",
            "tab": "General information",
            "score": 121.0
          },
          "International Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "International Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "International Law - # prompt tokens": {
            "description": "min=632.851, mean=632.851, max=632.851, sum=1265.702 (2)",
            "tab": "General information",
            "score": 632.8512396694215
          },
          "International Law - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "international_law",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_international_law"
        }
      }
    },
    {
      "evaluation_name": "Logical Fallacies",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Logical Fallacies",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.865,
        "details": {
          "description": "min=0.865, mean=0.865, max=0.865, sum=1.73 (2)",
          "tab": "Accuracy",
          "Logical Fallacies - Observed inference time (s)": {
            "description": "min=0.699, mean=0.699, max=0.699, sum=1.398 (2)",
            "tab": "Efficiency",
            "score": 0.6990647155083031
          },
          "Logical Fallacies - # eval": {
            "description": "min=163, mean=163, max=163, sum=326 (2)",
            "tab": "General information",
            "score": 163.0
          },
          "Logical Fallacies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Logical Fallacies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Logical Fallacies - # prompt tokens": {
            "description": "min=442.595, mean=442.595, max=442.595, sum=885.19 (2)",
            "tab": "General information",
            "score": 442.5950920245399
          },
          "Logical Fallacies - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "logical_fallacies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_logical_fallacies"
        }
      }
    },
    {
      "evaluation_name": "Machine Learning",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Machine Learning",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.723,
        "details": {
          "description": "min=0.723, mean=0.723, max=0.723, sum=1.446 (2)",
          "tab": "Accuracy",
          "Machine Learning - Observed inference time (s)": {
            "description": "min=0.606, mean=0.606, max=0.606, sum=1.211 (2)",
            "tab": "Efficiency",
            "score": 0.6055374975715365
          },
          "Machine Learning - # eval": {
            "description": "min=112, mean=112, max=112, sum=224 (2)",
            "tab": "General information",
            "score": 112.0
          },
          "Machine Learning - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Machine Learning - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Machine Learning - # prompt tokens": {
            "description": "min=661.054, mean=661.054, max=661.054, sum=1322.107 (2)",
            "tab": "General information",
            "score": 661.0535714285714
          },
          "Machine Learning - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "machine_learning",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_machine_learning"
        }
      }
    },
    {
      "evaluation_name": "Management",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Management",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.913,
        "details": {
          "description": "min=0.913, mean=0.913, max=0.913, sum=1.825 (2)",
          "tab": "Accuracy",
          "Management - Observed inference time (s)": {
            "description": "min=0.576, mean=0.576, max=0.576, sum=1.152 (2)",
            "tab": "Efficiency",
            "score": 0.5760108475546235
          },
          "Management - # eval": {
            "description": "min=103, mean=103, max=103, sum=206 (2)",
            "tab": "General information",
            "score": 103.0
          },
          "Management - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Management - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Management - # prompt tokens": {
            "description": "min=276.796, mean=276.796, max=276.796, sum=553.592 (2)",
            "tab": "General information",
            "score": 276.79611650485435
          },
          "Management - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "management",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_management"
        }
      }
    },
    {
      "evaluation_name": "Marketing",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Marketing",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.932,
        "details": {
          "description": "min=0.932, mean=0.932, max=0.932, sum=1.863 (2)",
          "tab": "Accuracy",
          "Marketing - Observed inference time (s)": {
            "description": "min=0.495, mean=0.495, max=0.495, sum=0.991 (2)",
            "tab": "Efficiency",
            "score": 0.49540983204148775
          },
          "Marketing - # eval": {
            "description": "min=234, mean=234, max=234, sum=468 (2)",
            "tab": "General information",
            "score": 234.0
          },
          "Marketing - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Marketing - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Marketing - # prompt tokens": {
            "description": "min=397.218, mean=397.218, max=397.218, sum=794.436 (2)",
            "tab": "General information",
            "score": 397.21794871794873
          },
          "Marketing - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "marketing",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_marketing"
        }
      }
    },
    {
      "evaluation_name": "Medical Genetics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Medical Genetics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.93,
        "details": {
          "description": "min=0.93, mean=0.93, max=0.93, sum=1.86 (2)",
          "tab": "Accuracy",
          "Medical Genetics - Observed inference time (s)": {
            "description": "min=0.541, mean=0.541, max=0.541, sum=1.082 (2)",
            "tab": "Efficiency",
            "score": 0.5407642388343811
          },
          "Medical Genetics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Medical Genetics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Medical Genetics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Medical Genetics - # prompt tokens": {
            "description": "min=334, mean=334, max=334, sum=668 (2)",
            "tab": "General information",
            "score": 334.0
          },
          "Medical Genetics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "medical_genetics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_medical_genetics"
        }
      }
    },
    {
      "evaluation_name": "Miscellaneous",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Miscellaneous",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.946,
        "details": {
          "description": "min=0.946, mean=0.946, max=0.946, sum=1.893 (2)",
          "tab": "Accuracy",
          "Miscellaneous - Observed inference time (s)": {
            "description": "min=0.474, mean=0.474, max=0.474, sum=0.947 (2)",
            "tab": "Efficiency",
            "score": 0.4736132238103055
          },
          "Miscellaneous - # eval": {
            "description": "min=783, mean=783, max=783, sum=1566 (2)",
            "tab": "General information",
            "score": 783.0
          },
          "Miscellaneous - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Miscellaneous - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Miscellaneous - # prompt tokens": {
            "description": "min=292.925, mean=292.925, max=292.925, sum=585.849 (2)",
            "tab": "General information",
            "score": 292.92464878671774
          },
          "Miscellaneous - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "miscellaneous",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_miscellaneous"
        }
      }
    },
    {
      "evaluation_name": "Moral Scenarios",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Moral Scenarios",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.816,
        "details": {
          "description": "min=0.816, mean=0.816, max=0.816, sum=1.631 (2)",
          "tab": "Accuracy",
          "Moral Disputes - Observed inference time (s)": {
            "description": "min=0.451, mean=0.451, max=0.451, sum=0.901 (2)",
            "tab": "Efficiency",
            "score": 0.45068276686475456
          },
          "Moral Scenarios - Observed inference time (s)": {
            "description": "min=0.545, mean=0.545, max=0.545, sum=1.09 (2)",
            "tab": "Efficiency",
            "score": 0.5448215519249773
          },
          "Moral Disputes - # eval": {
            "description": "min=346, mean=346, max=346, sum=692 (2)",
            "tab": "General information",
            "score": 346.0
          },
          "Moral Disputes - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Disputes - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Disputes - # prompt tokens": {
            "description": "min=469.145, mean=469.145, max=469.145, sum=938.289 (2)",
            "tab": "General information",
            "score": 469.1445086705202
          },
          "Moral Disputes - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Moral Scenarios - # eval": {
            "description": "min=895, mean=895, max=895, sum=1790 (2)",
            "tab": "General information",
            "score": 895.0
          },
          "Moral Scenarios - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Scenarios - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # prompt tokens": {
            "description": "min=649.455, mean=649.455, max=649.455, sum=1298.909 (2)",
            "tab": "General information",
            "score": 649.454748603352
          },
          "Moral Scenarios - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "moral_scenarios",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_moral_scenarios"
        }
      }
    },
    {
      "evaluation_name": "Nutrition",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Nutrition",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.879,
        "details": {
          "description": "min=0.879, mean=0.879, max=0.879, sum=1.758 (2)",
          "tab": "Accuracy",
          "Nutrition - Observed inference time (s)": {
            "description": "min=0.441, mean=0.441, max=0.441, sum=0.882 (2)",
            "tab": "Efficiency",
            "score": 0.4411514296251185
          },
          "Nutrition - # eval": {
            "description": "min=306, mean=306, max=306, sum=612 (2)",
            "tab": "General information",
            "score": 306.0
          },
          "Nutrition - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Nutrition - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Nutrition - # prompt tokens": {
            "description": "min=579.817, mean=579.817, max=579.817, sum=1159.634 (2)",
            "tab": "General information",
            "score": 579.8169934640523
          },
          "Nutrition - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "nutrition",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_nutrition"
        }
      }
    },
    {
      "evaluation_name": "Prehistory",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Prehistory",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.917,
        "details": {
          "description": "min=0.917, mean=0.917, max=0.917, sum=1.833 (2)",
          "tab": "Accuracy",
          "Prehistory - Observed inference time (s)": {
            "description": "min=0.489, mean=0.489, max=0.489, sum=0.978 (2)",
            "tab": "Efficiency",
            "score": 0.4891524300163175
          },
          "Prehistory - # eval": {
            "description": "min=324, mean=324, max=324, sum=648 (2)",
            "tab": "General information",
            "score": 324.0
          },
          "Prehistory - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Prehistory - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Prehistory - # prompt tokens": {
            "description": "min=507.559, mean=507.559, max=507.559, sum=1015.117 (2)",
            "tab": "General information",
            "score": 507.55864197530866
          },
          "Prehistory - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "prehistory",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_prehistory"
        }
      }
    },
    {
      "evaluation_name": "Public Relations",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Public Relations",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.782,
        "details": {
          "description": "min=0.782, mean=0.782, max=0.782, sum=1.564 (2)",
          "tab": "Accuracy",
          "Public Relations - Observed inference time (s)": {
            "description": "min=0.46, mean=0.46, max=0.46, sum=0.92 (2)",
            "tab": "Efficiency",
            "score": 0.46012504534287885
          },
          "Public Relations - # eval": {
            "description": "min=110, mean=110, max=110, sum=220 (2)",
            "tab": "General information",
            "score": 110.0
          },
          "Public Relations - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Public Relations - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Public Relations - # prompt tokens": {
            "description": "min=398.318, mean=398.318, max=398.318, sum=796.636 (2)",
            "tab": "General information",
            "score": 398.3181818181818
          },
          "Public Relations - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "public_relations",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_public_relations"
        }
      }
    },
    {
      "evaluation_name": "Security Studies",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Security Studies",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.841,
        "details": {
          "description": "min=0.841, mean=0.841, max=0.841, sum=1.682 (2)",
          "tab": "Accuracy",
          "Security Studies - Observed inference time (s)": {
            "description": "min=0.546, mean=0.546, max=0.546, sum=1.093 (2)",
            "tab": "Efficiency",
            "score": 0.546490309189777
          },
          "Security Studies - # eval": {
            "description": "min=245, mean=245, max=245, sum=490 (2)",
            "tab": "General information",
            "score": 245.0
          },
          "Security Studies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Security Studies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Security Studies - # prompt tokens": {
            "description": "min=1157.473, mean=1157.473, max=1157.473, sum=2314.947 (2)",
            "tab": "General information",
            "score": 1157.4734693877551
          },
          "Security Studies - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "security_studies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_security_studies"
        }
      }
    },
    {
      "evaluation_name": "Sociology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Sociology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.925,
        "details": {
          "description": "min=0.925, mean=0.925, max=0.925, sum=1.851 (2)",
          "tab": "Accuracy",
          "Sociology - Observed inference time (s)": {
            "description": "min=0.441, mean=0.441, max=0.441, sum=0.882 (2)",
            "tab": "Efficiency",
            "score": 0.4410626805243801
          },
          "Sociology - # eval": {
            "description": "min=201, mean=201, max=201, sum=402 (2)",
            "tab": "General information",
            "score": 201.0
          },
          "Sociology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Sociology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Sociology - # prompt tokens": {
            "description": "min=438.522, mean=438.522, max=438.522, sum=877.045 (2)",
            "tab": "General information",
            "score": 438.5223880597015
          },
          "Sociology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "sociology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_sociology"
        }
      }
    },
    {
      "evaluation_name": "Virology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Virology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.59,
        "details": {
          "description": "min=0.59, mean=0.59, max=0.59, sum=1.181 (2)",
          "tab": "Accuracy",
          "Virology - Observed inference time (s)": {
            "description": "min=0.852, mean=0.852, max=0.852, sum=1.704 (2)",
            "tab": "Efficiency",
            "score": 0.851962562066963
          },
          "Virology - # eval": {
            "description": "min=166, mean=166, max=166, sum=332 (2)",
            "tab": "General information",
            "score": 166.0
          },
          "Virology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Virology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Virology - # prompt tokens": {
            "description": "min=336.09, mean=336.09, max=336.09, sum=672.181 (2)",
            "tab": "General information",
            "score": 336.0903614457831
          },
          "Virology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "virology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_virology"
        }
      }
    },
    {
      "evaluation_name": "World Religions",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on World Religions",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.854,
        "details": {
          "description": "min=0.854, mean=0.854, max=0.854, sum=1.708 (2)",
          "tab": "Accuracy",
          "World Religions - Observed inference time (s)": {
            "description": "min=0.566, mean=0.566, max=0.566, sum=1.133 (2)",
            "tab": "Efficiency",
            "score": 0.5664703581068251
          },
          "World Religions - # eval": {
            "description": "min=171, mean=171, max=171, sum=342 (2)",
            "tab": "General information",
            "score": 171.0
          },
          "World Religions - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "World Religions - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "World Religions - # prompt tokens": {
            "description": "min=268.561, mean=268.561, max=268.561, sum=537.123 (2)",
            "tab": "General information",
            "score": 268.56140350877195
          },
          "World Religions - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "world_religions",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_world_religions"
        }
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.416,
        "details": {
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}