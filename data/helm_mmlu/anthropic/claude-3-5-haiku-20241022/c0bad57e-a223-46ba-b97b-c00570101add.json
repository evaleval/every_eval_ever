{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_mmlu/anthropic_claude-3-5-haiku-20241022/1767656407.235096",
  "retrieved_timestamp": "1767656407.235096",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
  ],
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Claude 3.5 Haiku (20241022)",
    "id": "anthropic/claude-3-5-haiku-20241022",
    "developer": "anthropic",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.743,
        "details": {
          "description": "min=0.359, mean=0.743, max=0.94, sum=84.719 (114)",
          "tab": "Accuracy",
          "MMLU All Subjects - Observed inference time (s)": {
            "description": "min=0.909, mean=1.108, max=1.572, sum=126.32 (114)",
            "tab": "Efficiency",
            "score": 1.1080717974066416
          },
          "MMLU All Subjects - # eval": {
            "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
            "tab": "General information",
            "score": 246.35087719298247
          },
          "MMLU All Subjects - # train": {
            "description": "min=5, mean=5, max=5, sum=570 (114)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU All Subjects - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU All Subjects - # prompt tokens": {
            "description": "min=293.018, mean=638.288, max=2887.576, sum=72764.875 (114)",
            "tab": "General information",
            "score": 638.2883793758953
          },
          "MMLU All Subjects - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=114 (114)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - EM",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.47,
        "details": {
          "description": "min=0.47, mean=0.47, max=0.47, sum=0.94 (2)",
          "tab": "Accuracy",
          "Abstract Algebra - Observed inference time (s)": {
            "description": "min=0.909, mean=0.909, max=0.909, sum=1.819 (2)",
            "tab": "Efficiency",
            "score": 0.9094081521034241
          },
          "Abstract Algebra - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Abstract Algebra - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Abstract Algebra - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Abstract Algebra - # prompt tokens": {
            "description": "min=370.26, mean=370.26, max=370.26, sum=740.52 (2)",
            "tab": "General information",
            "score": 370.26
          },
          "Abstract Algebra - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - EM",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.793,
        "details": {
          "description": "min=0.793, mean=0.793, max=0.793, sum=1.585 (2)",
          "tab": "Accuracy",
          "Anatomy - Observed inference time (s)": {
            "description": "min=1.124, mean=1.124, max=1.124, sum=2.247 (2)",
            "tab": "Efficiency",
            "score": 1.1236292309231228
          },
          "Anatomy - # eval": {
            "description": "min=135, mean=135, max=135, sum=270 (2)",
            "tab": "General information",
            "score": 135.0
          },
          "Anatomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Anatomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Anatomy - # prompt tokens": {
            "description": "min=370.8, mean=370.8, max=370.8, sum=741.6 (2)",
            "tab": "General information",
            "score": 370.8
          },
          "Anatomy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Physics - EM",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.52,
        "details": {
          "description": "min=0.52, mean=0.52, max=0.52, sum=1.039 (2)",
          "tab": "Accuracy",
          "College Chemistry - Observed inference time (s)": {
            "description": "min=1.196, mean=1.196, max=1.196, sum=2.392 (2)",
            "tab": "Efficiency",
            "score": 1.1962119388580321
          },
          "College Biology - Observed inference time (s)": {
            "description": "min=1.247, mean=1.247, max=1.247, sum=2.494 (2)",
            "tab": "Efficiency",
            "score": 1.2467927502261267
          },
          "College Computer Science - Observed inference time (s)": {
            "description": "min=1.572, mean=1.572, max=1.572, sum=3.144 (2)",
            "tab": "Efficiency",
            "score": 1.5719245457649231
          },
          "College Mathematics - Observed inference time (s)": {
            "description": "min=1.13, mean=1.13, max=1.13, sum=2.26 (2)",
            "tab": "Efficiency",
            "score": 1.1302329087257386
          },
          "College Medicine - Observed inference time (s)": {
            "description": "min=1.259, mean=1.259, max=1.259, sum=2.517 (2)",
            "tab": "Efficiency",
            "score": 1.2587321479885565
          },
          "College Physics - Observed inference time (s)": {
            "description": "min=1.261, mean=1.261, max=1.261, sum=2.521 (2)",
            "tab": "Efficiency",
            "score": 1.2606473857281255
          },
          "College Chemistry - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Chemistry - # prompt tokens": {
            "description": "min=550.01, mean=550.01, max=550.01, sum=1100.02 (2)",
            "tab": "General information",
            "score": 550.01
          },
          "College Chemistry - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Biology - # eval": {
            "description": "min=144, mean=144, max=144, sum=288 (2)",
            "tab": "General information",
            "score": 144.0
          },
          "College Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # prompt tokens": {
            "description": "min=490.347, mean=490.347, max=490.347, sum=980.694 (2)",
            "tab": "General information",
            "score": 490.34722222222223
          },
          "College Biology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # prompt tokens": {
            "description": "min=838.24, mean=838.24, max=838.24, sum=1676.48 (2)",
            "tab": "General information",
            "score": 838.24
          },
          "College Computer Science - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Mathematics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # prompt tokens": {
            "description": "min=604.19, mean=604.19, max=604.19, sum=1208.38 (2)",
            "tab": "General information",
            "score": 604.19
          },
          "College Mathematics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Medicine - # eval": {
            "description": "min=173, mean=173, max=173, sum=346 (2)",
            "tab": "General information",
            "score": 173.0
          },
          "College Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # prompt tokens": {
            "description": "min=540.63, mean=540.63, max=540.63, sum=1081.26 (2)",
            "tab": "General information",
            "score": 540.6300578034682
          },
          "College Medicine - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Physics - # eval": {
            "description": "min=102, mean=102, max=102, sum=204 (2)",
            "tab": "General information",
            "score": 102.0
          },
          "College Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # prompt tokens": {
            "description": "min=489.48, mean=489.48, max=489.48, sum=978.961 (2)",
            "tab": "General information",
            "score": 489.48039215686276
          },
          "College Physics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Computer Security - EM",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.84,
        "details": {
          "description": "min=0.84, mean=0.84, max=0.84, sum=1.68 (2)",
          "tab": "Accuracy",
          "Computer Security - Observed inference time (s)": {
            "description": "min=1.013, mean=1.013, max=1.013, sum=2.027 (2)",
            "tab": "Efficiency",
            "score": 1.0133756017684936
          },
          "Computer Security - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Computer Security - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Computer Security - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Computer Security - # prompt tokens": {
            "description": "min=398.62, mean=398.62, max=398.62, sum=797.24 (2)",
            "tab": "General information",
            "score": 398.62
          },
          "Computer Security - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - EM",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.596,
        "details": {
          "description": "min=0.596, mean=0.596, max=0.596, sum=1.193 (2)",
          "tab": "Accuracy",
          "Econometrics - Observed inference time (s)": {
            "description": "min=0.922, mean=0.922, max=0.922, sum=1.845 (2)",
            "tab": "Efficiency",
            "score": 0.9224813549142135
          },
          "Econometrics - # eval": {
            "description": "min=114, mean=114, max=114, sum=228 (2)",
            "tab": "General information",
            "score": 114.0
          },
          "Econometrics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Econometrics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Econometrics - # prompt tokens": {
            "description": "min=619.596, mean=619.596, max=619.596, sum=1239.193 (2)",
            "tab": "General information",
            "score": 619.5964912280701
          },
          "Econometrics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - EM",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.5,
        "details": {
          "description": "min=0.5, mean=0.5, max=0.5, sum=1 (2)",
          "tab": "Accuracy",
          "Global Facts - Observed inference time (s)": {
            "description": "min=1.101, mean=1.101, max=1.101, sum=2.201 (2)",
            "tab": "Efficiency",
            "score": 1.1007365608215331
          },
          "Global Facts - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Global Facts - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Global Facts - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Global Facts - # prompt tokens": {
            "description": "min=411.61, mean=411.61, max=411.61, sum=823.22 (2)",
            "tab": "General information",
            "score": 411.61
          },
          "Global Facts - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - EM",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.861,
        "details": {
          "description": "min=0.861, mean=0.861, max=0.861, sum=1.722 (2)",
          "tab": "Accuracy",
          "Jurisprudence - Observed inference time (s)": {
            "description": "min=1.104, mean=1.104, max=1.104, sum=2.209 (2)",
            "tab": "Efficiency",
            "score": 1.1042848251484059
          },
          "Jurisprudence - # eval": {
            "description": "min=108, mean=108, max=108, sum=216 (2)",
            "tab": "General information",
            "score": 108.0
          },
          "Jurisprudence - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Jurisprudence - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Jurisprudence - # prompt tokens": {
            "description": "min=431.426, mean=431.426, max=431.426, sum=862.852 (2)",
            "tab": "General information",
            "score": 431.4259259259259
          },
          "Jurisprudence - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - EM",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.823,
        "details": {
          "description": "min=0.823, mean=0.823, max=0.823, sum=1.646 (2)",
          "tab": "Accuracy",
          "Philosophy - Observed inference time (s)": {
            "description": "min=1.117, mean=1.117, max=1.117, sum=2.233 (2)",
            "tab": "Efficiency",
            "score": 1.1165370488856767
          },
          "Philosophy - # eval": {
            "description": "min=311, mean=311, max=311, sum=622 (2)",
            "tab": "General information",
            "score": 311.0
          },
          "Philosophy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Philosophy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Philosophy - # prompt tokens": {
            "description": "min=359.965, mean=359.965, max=359.965, sum=719.929 (2)",
            "tab": "General information",
            "score": 359.9646302250804
          },
          "Philosophy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Psychology - EM",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.825,
        "details": {
          "description": "min=0.825, mean=0.825, max=0.825, sum=1.65 (2)",
          "tab": "Accuracy",
          "Professional Medicine - Observed inference time (s)": {
            "description": "min=1.412, mean=1.412, max=1.412, sum=2.824 (2)",
            "tab": "Efficiency",
            "score": 1.4119182877680834
          },
          "Professional Accounting - Observed inference time (s)": {
            "description": "min=0.984, mean=0.984, max=0.984, sum=1.967 (2)",
            "tab": "Efficiency",
            "score": 0.9836687187776498
          },
          "Professional Law - Observed inference time (s)": {
            "description": "min=1.016, mean=1.016, max=1.016, sum=2.032 (2)",
            "tab": "Efficiency",
            "score": 1.0160297585901412
          },
          "Professional Psychology - Observed inference time (s)": {
            "description": "min=0.979, mean=0.979, max=0.979, sum=1.958 (2)",
            "tab": "Efficiency",
            "score": 0.9789344672284095
          },
          "Professional Medicine - # eval": {
            "description": "min=272, mean=272, max=272, sum=544 (2)",
            "tab": "General information",
            "score": 272.0
          },
          "Professional Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Medicine - # prompt tokens": {
            "description": "min=1123.537, mean=1123.537, max=1123.537, sum=2247.074 (2)",
            "tab": "General information",
            "score": 1123.5367647058824
          },
          "Professional Medicine - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Professional Accounting - # eval": {
            "description": "min=282, mean=282, max=282, sum=564 (2)",
            "tab": "General information",
            "score": 282.0
          },
          "Professional Accounting - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Accounting - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # prompt tokens": {
            "description": "min=665.422, mean=665.422, max=665.422, sum=1330.844 (2)",
            "tab": "General information",
            "score": 665.4219858156029
          },
          "Professional Accounting - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Professional Law - # eval": {
            "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
            "tab": "General information",
            "score": 1534.0
          },
          "Professional Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # prompt tokens": {
            "description": "min=1701.16, mean=1701.16, max=1701.16, sum=3402.321 (2)",
            "tab": "General information",
            "score": 1701.16036505867
          },
          "Professional Law - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Professional Psychology - # eval": {
            "description": "min=612, mean=612, max=612, sum=1224 (2)",
            "tab": "General information",
            "score": 612.0
          },
          "Professional Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # prompt tokens": {
            "description": "min=603.168, mean=603.168, max=603.168, sum=1206.337 (2)",
            "tab": "General information",
            "score": 603.1683006535948
          },
          "Professional Psychology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - EM",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.94,
        "details": {
          "description": "min=0.94, mean=0.94, max=0.94, sum=1.88 (2)",
          "tab": "Accuracy",
          "Us Foreign Policy - Observed inference time (s)": {
            "description": "min=0.97, mean=0.97, max=0.97, sum=1.941 (2)",
            "tab": "Efficiency",
            "score": 0.9703591632843017
          },
          "Us Foreign Policy - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Us Foreign Policy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Us Foreign Policy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Us Foreign Policy - # prompt tokens": {
            "description": "min=455.25, mean=455.25, max=455.25, sum=910.5 (2)",
            "tab": "General information",
            "score": 455.25
          },
          "Us Foreign Policy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - EM",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.829,
        "details": {
          "description": "min=0.829, mean=0.829, max=0.829, sum=1.658 (2)",
          "tab": "Accuracy",
          "Astronomy - Observed inference time (s)": {
            "description": "min=1.18, mean=1.18, max=1.18, sum=2.36 (2)",
            "tab": "Efficiency",
            "score": 1.1798271034893237
          },
          "Astronomy - # eval": {
            "description": "min=152, mean=152, max=152, sum=304 (2)",
            "tab": "General information",
            "score": 152.0
          },
          "Astronomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Astronomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Astronomy - # prompt tokens": {
            "description": "min=604.493, mean=604.493, max=604.493, sum=1208.987 (2)",
            "tab": "General information",
            "score": 604.4934210526316
          },
          "Astronomy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - EM",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.8,
        "details": {
          "description": "min=0.8, mean=0.8, max=0.8, sum=1.6 (2)",
          "tab": "Accuracy",
          "Business Ethics - Observed inference time (s)": {
            "description": "min=1.147, mean=1.147, max=1.147, sum=2.295 (2)",
            "tab": "Efficiency",
            "score": 1.1473834657669066
          },
          "Business Ethics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Business Ethics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Business Ethics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Business Ethics - # prompt tokens": {
            "description": "min=600.02, mean=600.02, max=600.02, sum=1200.04 (2)",
            "tab": "General information",
            "score": 600.02
          },
          "Business Ethics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - EM",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.823,
        "details": {
          "description": "min=0.823, mean=0.823, max=0.823, sum=1.645 (2)",
          "tab": "Accuracy",
          "Clinical Knowledge - Observed inference time (s)": {
            "description": "min=1.099, mean=1.099, max=1.099, sum=2.198 (2)",
            "tab": "Efficiency",
            "score": 1.0991604094235403
          },
          "Clinical Knowledge - # eval": {
            "description": "min=265, mean=265, max=265, sum=530 (2)",
            "tab": "General information",
            "score": 265.0
          },
          "Clinical Knowledge - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Clinical Knowledge - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Clinical Knowledge - # prompt tokens": {
            "description": "min=429.457, mean=429.457, max=429.457, sum=858.913 (2)",
            "tab": "General information",
            "score": 429.4566037735849
          },
          "Clinical Knowledge - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - EM",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.723,
        "details": {
          "description": "min=0.723, mean=0.723, max=0.723, sum=1.447 (2)",
          "tab": "Accuracy",
          "Conceptual Physics - Observed inference time (s)": {
            "description": "min=1.537, mean=1.537, max=1.537, sum=3.074 (2)",
            "tab": "Efficiency",
            "score": 1.536949543242759
          },
          "Conceptual Physics - # eval": {
            "description": "min=235, mean=235, max=235, sum=470 (2)",
            "tab": "General information",
            "score": 235.0
          },
          "Conceptual Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Conceptual Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Conceptual Physics - # prompt tokens": {
            "description": "min=323.536, mean=323.536, max=323.536, sum=647.072 (2)",
            "tab": "General information",
            "score": 323.53617021276597
          },
          "Conceptual Physics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - EM",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.717,
        "details": {
          "description": "min=0.717, mean=0.717, max=0.717, sum=1.434 (2)",
          "tab": "Accuracy",
          "Electrical Engineering - Observed inference time (s)": {
            "description": "min=1.249, mean=1.249, max=1.249, sum=2.497 (2)",
            "tab": "Efficiency",
            "score": 1.2485630594450852
          },
          "Electrical Engineering - # eval": {
            "description": "min=145, mean=145, max=145, sum=290 (2)",
            "tab": "General information",
            "score": 145.0
          },
          "Electrical Engineering - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Electrical Engineering - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Electrical Engineering - # prompt tokens": {
            "description": "min=437.041, mean=437.041, max=437.041, sum=874.083 (2)",
            "tab": "General information",
            "score": 437.04137931034484
          },
          "Electrical Engineering - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.561,
        "details": {
          "description": "min=0.561, mean=0.561, max=0.561, sum=1.122 (2)",
          "tab": "Accuracy",
          "Elementary Mathematics - Observed inference time (s)": {
            "description": "min=1.558, mean=1.558, max=1.558, sum=3.116 (2)",
            "tab": "Efficiency",
            "score": 1.5580224965615248
          },
          "Elementary Mathematics - # eval": {
            "description": "min=378, mean=378, max=378, sum=756 (2)",
            "tab": "General information",
            "score": 378.0
          },
          "Elementary Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Elementary Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Elementary Mathematics - # prompt tokens": {
            "description": "min=511.066, mean=511.066, max=511.066, sum=1022.132 (2)",
            "tab": "General information",
            "score": 511.06613756613757
          },
          "Elementary Mathematics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - EM",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.619,
        "details": {
          "description": "min=0.619, mean=0.619, max=0.619, sum=1.238 (2)",
          "tab": "Accuracy",
          "Formal Logic - Observed inference time (s)": {
            "description": "min=1.526, mean=1.526, max=1.526, sum=3.052 (2)",
            "tab": "Efficiency",
            "score": 1.5258309424869598
          },
          "Formal Logic - # eval": {
            "description": "min=126, mean=126, max=126, sum=252 (2)",
            "tab": "General information",
            "score": 126.0
          },
          "Formal Logic - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Formal Logic - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Formal Logic - # prompt tokens": {
            "description": "min=646.746, mean=646.746, max=646.746, sum=1293.492 (2)",
            "tab": "General information",
            "score": 646.7460317460317
          },
          "Formal Logic - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School World History - EM",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.882,
        "details": {
          "description": "min=0.882, mean=0.882, max=0.882, sum=1.764 (2)",
          "tab": "Accuracy",
          "High School Biology - Observed inference time (s)": {
            "description": "min=1.15, mean=1.15, max=1.15, sum=2.299 (2)",
            "tab": "Efficiency",
            "score": 1.1497065974820044
          },
          "High School Chemistry - Observed inference time (s)": {
            "description": "min=1.227, mean=1.227, max=1.227, sum=2.454 (2)",
            "tab": "Efficiency",
            "score": 1.2272211636228514
          },
          "High School Computer Science - Observed inference time (s)": {
            "description": "min=1.014, mean=1.014, max=1.014, sum=2.027 (2)",
            "tab": "Efficiency",
            "score": 1.0136730527877809
          },
          "High School European History - Observed inference time (s)": {
            "description": "min=1.024, mean=1.024, max=1.024, sum=2.047 (2)",
            "tab": "Efficiency",
            "score": 1.0236461119218305
          },
          "High School Geography - Observed inference time (s)": {
            "description": "min=1.059, mean=1.059, max=1.059, sum=2.119 (2)",
            "tab": "Efficiency",
            "score": 1.0594979368074975
          },
          "High School Government And Politics - Observed inference time (s)": {
            "description": "min=1.138, mean=1.138, max=1.138, sum=2.275 (2)",
            "tab": "Efficiency",
            "score": 1.1376265478875354
          },
          "High School Macroeconomics - Observed inference time (s)": {
            "description": "min=1.107, mean=1.107, max=1.107, sum=2.214 (2)",
            "tab": "Efficiency",
            "score": 1.1069551357856164
          },
          "High School Mathematics - Observed inference time (s)": {
            "description": "min=1.094, mean=1.094, max=1.094, sum=2.188 (2)",
            "tab": "Efficiency",
            "score": 1.0940863344404432
          },
          "High School Microeconomics - Observed inference time (s)": {
            "description": "min=1.034, mean=1.034, max=1.034, sum=2.068 (2)",
            "tab": "Efficiency",
            "score": 1.03420967815303
          },
          "High School Physics - Observed inference time (s)": {
            "description": "min=1.059, mean=1.059, max=1.059, sum=2.119 (2)",
            "tab": "Efficiency",
            "score": 1.0594944227610203
          },
          "High School Psychology - Observed inference time (s)": {
            "description": "min=1.074, mean=1.074, max=1.074, sum=2.149 (2)",
            "tab": "Efficiency",
            "score": 1.07433808177983
          },
          "High School Statistics - Observed inference time (s)": {
            "description": "min=1.053, mean=1.053, max=1.053, sum=2.107 (2)",
            "tab": "Efficiency",
            "score": 1.0534564554691315
          },
          "High School US History - Observed inference time (s)": {
            "description": "min=1.101, mean=1.101, max=1.101, sum=2.201 (2)",
            "tab": "Efficiency",
            "score": 1.1006785748051662
          },
          "High School World History - Observed inference time (s)": {
            "description": "min=1.093, mean=1.093, max=1.093, sum=2.186 (2)",
            "tab": "Efficiency",
            "score": 1.0931011674776359
          },
          "High School Biology - # eval": {
            "description": "min=310, mean=310, max=310, sum=620 (2)",
            "tab": "General information",
            "score": 310.0
          },
          "High School Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Biology - # prompt tokens": {
            "description": "min=534.577, mean=534.577, max=534.577, sum=1069.155 (2)",
            "tab": "General information",
            "score": 534.5774193548388
          },
          "High School Biology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Chemistry - # eval": {
            "description": "min=203, mean=203, max=203, sum=406 (2)",
            "tab": "General information",
            "score": 203.0
          },
          "High School Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # prompt tokens": {
            "description": "min=497.921, mean=497.921, max=497.921, sum=995.842 (2)",
            "tab": "General information",
            "score": 497.92118226600985
          },
          "High School Chemistry - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "High School Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # prompt tokens": {
            "description": "min=882.4, mean=882.4, max=882.4, sum=1764.8 (2)",
            "tab": "General information",
            "score": 882.4
          },
          "High School Computer Science - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School European History - # eval": {
            "description": "min=165, mean=165, max=165, sum=330 (2)",
            "tab": "General information",
            "score": 165.0
          },
          "High School European History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School European History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # prompt tokens": {
            "description": "min=2887.576, mean=2887.576, max=2887.576, sum=5775.152 (2)",
            "tab": "General information",
            "score": 2887.5757575757575
          },
          "High School European History - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Geography - # eval": {
            "description": "min=198, mean=198, max=198, sum=396 (2)",
            "tab": "General information",
            "score": 198.0
          },
          "High School Geography - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Geography - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # prompt tokens": {
            "description": "min=412.268, mean=412.268, max=412.268, sum=824.535 (2)",
            "tab": "General information",
            "score": 412.2676767676768
          },
          "High School Geography - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Government And Politics - # eval": {
            "description": "min=193, mean=193, max=193, sum=386 (2)",
            "tab": "General information",
            "score": 193.0
          },
          "High School Government And Politics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Government And Politics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # prompt tokens": {
            "description": "min=491.104, mean=491.104, max=491.104, sum=982.207 (2)",
            "tab": "General information",
            "score": 491.10362694300517
          },
          "High School Government And Politics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Macroeconomics - # eval": {
            "description": "min=390, mean=390, max=390, sum=780 (2)",
            "tab": "General information",
            "score": 390.0
          },
          "High School Macroeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Macroeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # prompt tokens": {
            "description": "min=406.036, mean=406.036, max=406.036, sum=812.072 (2)",
            "tab": "General information",
            "score": 406.0358974358974
          },
          "High School Macroeconomics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Mathematics - # eval": {
            "description": "min=270, mean=270, max=270, sum=540 (2)",
            "tab": "General information",
            "score": 270.0
          },
          "High School Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # prompt tokens": {
            "description": "min=519.881, mean=519.881, max=519.881, sum=1039.763 (2)",
            "tab": "General information",
            "score": 519.8814814814815
          },
          "High School Mathematics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Microeconomics - # eval": {
            "description": "min=238, mean=238, max=238, sum=476 (2)",
            "tab": "General information",
            "score": 238.0
          },
          "High School Microeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Microeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # prompt tokens": {
            "description": "min=420.513, mean=420.513, max=420.513, sum=841.025 (2)",
            "tab": "General information",
            "score": 420.5126050420168
          },
          "High School Microeconomics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Physics - # eval": {
            "description": "min=151, mean=151, max=151, sum=302 (2)",
            "tab": "General information",
            "score": 151.0
          },
          "High School Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # prompt tokens": {
            "description": "min=558.841, mean=558.841, max=558.841, sum=1117.682 (2)",
            "tab": "General information",
            "score": 558.841059602649
          },
          "High School Physics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Psychology - # eval": {
            "description": "min=545, mean=545, max=545, sum=1090 (2)",
            "tab": "General information",
            "score": 545.0
          },
          "High School Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # prompt tokens": {
            "description": "min=521.42, mean=521.42, max=521.42, sum=1042.84 (2)",
            "tab": "General information",
            "score": 521.4201834862386
          },
          "High School Psychology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Statistics - # eval": {
            "description": "min=216, mean=216, max=216, sum=432 (2)",
            "tab": "General information",
            "score": 216.0
          },
          "High School Statistics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Statistics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # prompt tokens": {
            "description": "min=806.963, mean=806.963, max=806.963, sum=1613.926 (2)",
            "tab": "General information",
            "score": 806.9629629629629
          },
          "High School Statistics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School US History - # eval": {
            "description": "min=204, mean=204, max=204, sum=408 (2)",
            "tab": "General information",
            "score": 204.0
          },
          "High School US History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School US History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # prompt tokens": {
            "description": "min=2288.49, mean=2288.49, max=2288.49, sum=4576.98 (2)",
            "tab": "General information",
            "score": 2288.4901960784314
          },
          "High School US History - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School World History - # eval": {
            "description": "min=237, mean=237, max=237, sum=474 (2)",
            "tab": "General information",
            "score": 237.0
          },
          "High School World History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School World History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # prompt tokens": {
            "description": "min=1475.932, mean=1475.932, max=1475.932, sum=2951.865 (2)",
            "tab": "General information",
            "score": 1475.9324894514768
          },
          "High School World History - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Sexuality - EM",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.885,
        "details": {
          "description": "min=0.885, mean=0.885, max=0.885, sum=1.771 (2)",
          "tab": "Accuracy",
          "Human Aging - Observed inference time (s)": {
            "description": "min=1.084, mean=1.084, max=1.084, sum=2.169 (2)",
            "tab": "Efficiency",
            "score": 1.0844623775225584
          },
          "Human Sexuality - Observed inference time (s)": {
            "description": "min=1.056, mean=1.056, max=1.056, sum=2.112 (2)",
            "tab": "Efficiency",
            "score": 1.0560545211529915
          },
          "Human Aging - # eval": {
            "description": "min=223, mean=223, max=223, sum=446 (2)",
            "tab": "General information",
            "score": 223.0
          },
          "Human Aging - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Aging - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Aging - # prompt tokens": {
            "description": "min=335.955, mean=335.955, max=335.955, sum=671.91 (2)",
            "tab": "General information",
            "score": 335.95515695067263
          },
          "Human Aging - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Human Sexuality - # eval": {
            "description": "min=131, mean=131, max=131, sum=262 (2)",
            "tab": "General information",
            "score": 131.0
          },
          "Human Sexuality - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Sexuality - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # prompt tokens": {
            "description": "min=371.496, mean=371.496, max=371.496, sum=742.992 (2)",
            "tab": "General information",
            "score": 371.4961832061069
          },
          "Human Sexuality - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - EM",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.884,
        "details": {
          "description": "min=0.884, mean=0.884, max=0.884, sum=1.769 (2)",
          "tab": "Accuracy",
          "International Law - Observed inference time (s)": {
            "description": "min=1.112, mean=1.112, max=1.112, sum=2.225 (2)",
            "tab": "Efficiency",
            "score": 1.1124236544301687
          },
          "International Law - # eval": {
            "description": "min=121, mean=121, max=121, sum=242 (2)",
            "tab": "General information",
            "score": 121.0
          },
          "International Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "International Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "International Law - # prompt tokens": {
            "description": "min=664.165, mean=664.165, max=664.165, sum=1328.331 (2)",
            "tab": "General information",
            "score": 664.1652892561983
          },
          "International Law - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - EM",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.822,
        "details": {
          "description": "min=0.822, mean=0.822, max=0.822, sum=1.644 (2)",
          "tab": "Accuracy",
          "Logical Fallacies - Observed inference time (s)": {
            "description": "min=1.015, mean=1.015, max=1.015, sum=2.03 (2)",
            "tab": "Efficiency",
            "score": 1.0148307984591993
          },
          "Logical Fallacies - # eval": {
            "description": "min=163, mean=163, max=163, sum=326 (2)",
            "tab": "General information",
            "score": 163.0
          },
          "Logical Fallacies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Logical Fallacies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Logical Fallacies - # prompt tokens": {
            "description": "min=470.276, mean=470.276, max=470.276, sum=940.552 (2)",
            "tab": "General information",
            "score": 470.2760736196319
          },
          "Logical Fallacies - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - EM",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.518,
        "details": {
          "description": "min=0.518, mean=0.518, max=0.518, sum=1.036 (2)",
          "tab": "Accuracy",
          "Machine Learning - Observed inference time (s)": {
            "description": "min=1.067, mean=1.067, max=1.067, sum=2.135 (2)",
            "tab": "Efficiency",
            "score": 1.0673569909163885
          },
          "Machine Learning - # eval": {
            "description": "min=112, mean=112, max=112, sum=224 (2)",
            "tab": "General information",
            "score": 112.0
          },
          "Machine Learning - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Machine Learning - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Machine Learning - # prompt tokens": {
            "description": "min=676.518, mean=676.518, max=676.518, sum=1353.036 (2)",
            "tab": "General information",
            "score": 676.5178571428571
          },
          "Machine Learning - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - EM",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.845,
        "details": {
          "description": "min=0.845, mean=0.845, max=0.845, sum=1.689 (2)",
          "tab": "Accuracy",
          "Management - Observed inference time (s)": {
            "description": "min=1.038, mean=1.038, max=1.038, sum=2.076 (2)",
            "tab": "Efficiency",
            "score": 1.0377622229381673
          },
          "Management - # eval": {
            "description": "min=103, mean=103, max=103, sum=206 (2)",
            "tab": "General information",
            "score": 103.0
          },
          "Management - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Management - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Management - # prompt tokens": {
            "description": "min=301.282, mean=301.282, max=301.282, sum=602.563 (2)",
            "tab": "General information",
            "score": 301.28155339805824
          },
          "Management - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - EM",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.897,
        "details": {
          "description": "min=0.897, mean=0.897, max=0.897, sum=1.795 (2)",
          "tab": "Accuracy",
          "Marketing - Observed inference time (s)": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.986 (2)",
            "tab": "Efficiency",
            "score": 0.9929133276654105
          },
          "Marketing - # eval": {
            "description": "min=234, mean=234, max=234, sum=468 (2)",
            "tab": "General information",
            "score": 234.0
          },
          "Marketing - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Marketing - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Marketing - # prompt tokens": {
            "description": "min=448.064, mean=448.064, max=448.064, sum=896.128 (2)",
            "tab": "General information",
            "score": 448.06410256410254
          },
          "Marketing - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - EM",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.83,
        "details": {
          "description": "min=0.83, mean=0.83, max=0.83, sum=1.66 (2)",
          "tab": "Accuracy",
          "Medical Genetics - Observed inference time (s)": {
            "description": "min=1.041, mean=1.041, max=1.041, sum=2.082 (2)",
            "tab": "Efficiency",
            "score": 1.041243133544922
          },
          "Medical Genetics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Medical Genetics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Medical Genetics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Medical Genetics - # prompt tokens": {
            "description": "min=354.88, mean=354.88, max=354.88, sum=709.76 (2)",
            "tab": "General information",
            "score": 354.88
          },
          "Medical Genetics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - EM",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.905,
        "details": {
          "description": "min=0.905, mean=0.905, max=0.905, sum=1.811 (2)",
          "tab": "Accuracy",
          "Miscellaneous - Observed inference time (s)": {
            "description": "min=1.043, mean=1.043, max=1.043, sum=2.086 (2)",
            "tab": "Efficiency",
            "score": 1.0429492231225297
          },
          "Miscellaneous - # eval": {
            "description": "min=783, mean=783, max=783, sum=1566 (2)",
            "tab": "General information",
            "score": 783.0
          },
          "Miscellaneous - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Miscellaneous - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Miscellaneous - # prompt tokens": {
            "description": "min=328.628, mean=328.628, max=328.628, sum=657.257 (2)",
            "tab": "General information",
            "score": 328.62835249042143
          },
          "Miscellaneous - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - EM",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.476,
        "details": {
          "description": "min=0.476, mean=0.476, max=0.476, sum=0.952 (2)",
          "tab": "Accuracy",
          "Moral Disputes - Observed inference time (s)": {
            "description": "min=1.044, mean=1.044, max=1.044, sum=2.088 (2)",
            "tab": "Efficiency",
            "score": 1.0438106094481627
          },
          "Moral Scenarios - Observed inference time (s)": {
            "description": "min=0.96, mean=0.96, max=0.96, sum=1.919 (2)",
            "tab": "Efficiency",
            "score": 0.95963474492121
          },
          "Moral Disputes - # eval": {
            "description": "min=346, mean=346, max=346, sum=692 (2)",
            "tab": "General information",
            "score": 346.0
          },
          "Moral Disputes - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Disputes - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Disputes - # prompt tokens": {
            "description": "min=511.789, mean=511.789, max=511.789, sum=1023.578 (2)",
            "tab": "General information",
            "score": 511.78901734104045
          },
          "Moral Disputes - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "Moral Scenarios - # eval": {
            "description": "min=895, mean=895, max=895, sum=1790 (2)",
            "tab": "General information",
            "score": 895.0
          },
          "Moral Scenarios - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Scenarios - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # prompt tokens": {
            "description": "min=676.949, mean=676.949, max=676.949, sum=1353.897 (2)",
            "tab": "General information",
            "score": 676.9486033519553
          },
          "Moral Scenarios - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - EM",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.846,
        "details": {
          "description": "min=0.846, mean=0.846, max=0.846, sum=1.693 (2)",
          "tab": "Accuracy",
          "Nutrition - Observed inference time (s)": {
            "description": "min=0.981, mean=0.981, max=0.981, sum=1.962 (2)",
            "tab": "Efficiency",
            "score": 0.9811088399949417
          },
          "Nutrition - # eval": {
            "description": "min=306, mean=306, max=306, sum=612 (2)",
            "tab": "General information",
            "score": 306.0
          },
          "Nutrition - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Nutrition - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Nutrition - # prompt tokens": {
            "description": "min=617.065, mean=617.065, max=617.065, sum=1234.131 (2)",
            "tab": "General information",
            "score": 617.0653594771242
          },
          "Nutrition - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - EM",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.877,
        "details": {
          "description": "min=0.877, mean=0.877, max=0.877, sum=1.753 (2)",
          "tab": "Accuracy",
          "Prehistory - Observed inference time (s)": {
            "description": "min=1.003, mean=1.003, max=1.003, sum=2.006 (2)",
            "tab": "Efficiency",
            "score": 1.0031694571177165
          },
          "Prehistory - # eval": {
            "description": "min=324, mean=324, max=324, sum=648 (2)",
            "tab": "General information",
            "score": 324.0
          },
          "Prehistory - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Prehistory - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Prehistory - # prompt tokens": {
            "description": "min=545.639, mean=545.639, max=545.639, sum=1091.278 (2)",
            "tab": "General information",
            "score": 545.6388888888889
          },
          "Prehistory - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Public Relations - EM",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.727,
        "details": {
          "description": "min=0.727, mean=0.727, max=0.727, sum=1.455 (2)",
          "tab": "Accuracy",
          "Public Relations - Observed inference time (s)": {
            "description": "min=0.941, mean=0.941, max=0.941, sum=1.882 (2)",
            "tab": "Efficiency",
            "score": 0.9410657709295099
          },
          "Public Relations - # eval": {
            "description": "min=110, mean=110, max=110, sum=220 (2)",
            "tab": "General information",
            "score": 110.0
          },
          "Public Relations - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Public Relations - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Public Relations - # prompt tokens": {
            "description": "min=432.991, mean=432.991, max=432.991, sum=865.982 (2)",
            "tab": "General information",
            "score": 432.9909090909091
          },
          "Public Relations - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - EM",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.792,
        "details": {
          "description": "min=0.792, mean=0.792, max=0.792, sum=1.584 (2)",
          "tab": "Accuracy",
          "Security Studies - Observed inference time (s)": {
            "description": "min=1.016, mean=1.016, max=1.016, sum=2.033 (2)",
            "tab": "Efficiency",
            "score": 1.0164005843960509
          },
          "Security Studies - # eval": {
            "description": "min=245, mean=245, max=245, sum=490 (2)",
            "tab": "General information",
            "score": 245.0
          },
          "Security Studies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Security Studies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Security Studies - # prompt tokens": {
            "description": "min=1243.804, mean=1243.804, max=1243.804, sum=2487.608 (2)",
            "tab": "General information",
            "score": 1243.8040816326532
          },
          "Security Studies - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - EM",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.905,
        "details": {
          "description": "min=0.905, mean=0.905, max=0.905, sum=1.811 (2)",
          "tab": "Accuracy",
          "Sociology - Observed inference time (s)": {
            "description": "min=0.976, mean=0.976, max=0.976, sum=1.952 (2)",
            "tab": "Efficiency",
            "score": 0.9757713939420026
          },
          "Sociology - # eval": {
            "description": "min=201, mean=201, max=201, sum=402 (2)",
            "tab": "General information",
            "score": 201.0
          },
          "Sociology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Sociology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Sociology - # prompt tokens": {
            "description": "min=467.274, mean=467.274, max=467.274, sum=934.547 (2)",
            "tab": "General information",
            "score": 467.27363184079604
          },
          "Sociology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - EM",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.566,
        "details": {
          "description": "min=0.566, mean=0.566, max=0.566, sum=1.133 (2)",
          "tab": "Accuracy",
          "Virology - Observed inference time (s)": {
            "description": "min=0.929, mean=0.929, max=0.929, sum=1.858 (2)",
            "tab": "Efficiency",
            "score": 0.9289331062730536
          },
          "Virology - # eval": {
            "description": "min=166, mean=166, max=166, sum=332 (2)",
            "tab": "General information",
            "score": 166.0
          },
          "Virology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Virology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Virology - # prompt tokens": {
            "description": "min=362.651, mean=362.651, max=362.651, sum=725.301 (2)",
            "tab": "General information",
            "score": 362.65060240963857
          },
          "Virology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - EM",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.865,
        "details": {
          "description": "min=0.865, mean=0.865, max=0.865, sum=1.731 (2)",
          "tab": "Accuracy",
          "World Religions - Observed inference time (s)": {
            "description": "min=1.021, mean=1.021, max=1.021, sum=2.042 (2)",
            "tab": "Efficiency",
            "score": 1.0208685663011339
          },
          "World Religions - # eval": {
            "description": "min=171, mean=171, max=171, sum=342 (2)",
            "tab": "General information",
            "score": 171.0
          },
          "World Religions - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "World Religions - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "World Religions - # prompt tokens": {
            "description": "min=293.018, mean=293.018, max=293.018, sum=586.035 (2)",
            "tab": "General information",
            "score": 293.0175438596491
          },
          "World Religions - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.128,
        "details": {
          "description": null,
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    }
  ]
}