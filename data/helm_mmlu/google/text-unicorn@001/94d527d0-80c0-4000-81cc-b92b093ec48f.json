{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_mmlu/google_text-unicorn@001/1767656407.235096",
  "retrieved_timestamp": "1767656407.235096",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
  ],
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "PaLM-2 (Unicorn)",
    "id": "google/text-unicorn@001",
    "developer": "google",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.786,
        "details": {
          "description": "min=0.493, mean=0.786, max=0.979, sum=89.606 (114)",
          "tab": "Accuracy",
          "MMLU All Subjects - Observed inference time (s)": {
            "description": "min=0.743, mean=1.052, max=2.108, sum=119.953 (114)",
            "tab": "Efficiency",
            "score": 1.0522220782452074
          },
          "MMLU All Subjects - # eval": {
            "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
            "tab": "General information",
            "score": 246.35087719298247
          },
          "MMLU All Subjects - # train": {
            "description": "min=5, mean=5, max=5, sum=570 (114)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU All Subjects - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU All Subjects - # prompt tokens": {
            "description": "min=270.187, mean=635.61, max=2823.23, sum=72459.527 (114)",
            "tab": "General information",
            "score": 635.6098850770794
          },
          "MMLU All Subjects - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - EM",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.51,
        "details": {
          "description": "min=0.51, mean=0.51, max=0.51, sum=1.02 (2)",
          "tab": "Accuracy",
          "Abstract Algebra - Observed inference time (s)": {
            "description": "min=1.277, mean=1.277, max=1.277, sum=2.555 (2)",
            "tab": "Efficiency",
            "score": 1.2773328518867493
          },
          "Abstract Algebra - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Abstract Algebra - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Abstract Algebra - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Abstract Algebra - # prompt tokens": {
            "description": "min=387.12, mean=387.12, max=387.12, sum=774.24 (2)",
            "tab": "General information",
            "score": 387.12
          },
          "Abstract Algebra - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - EM",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.733,
        "details": {
          "description": "min=0.733, mean=0.733, max=0.733, sum=1.467 (2)",
          "tab": "Accuracy",
          "Anatomy - Observed inference time (s)": {
            "description": "min=0.957, mean=0.957, max=0.957, sum=1.914 (2)",
            "tab": "Efficiency",
            "score": 0.9569159172199391
          },
          "Anatomy - # eval": {
            "description": "min=135, mean=135, max=135, sum=270 (2)",
            "tab": "General information",
            "score": 135.0
          },
          "Anatomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Anatomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Anatomy - # prompt tokens": {
            "description": "min=344.089, mean=344.089, max=344.089, sum=688.178 (2)",
            "tab": "General information",
            "score": 344.0888888888889
          },
          "Anatomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Physics - EM",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.549,
        "details": {
          "description": "min=0.549, mean=0.549, max=0.549, sum=1.098 (2)",
          "tab": "Accuracy",
          "College Chemistry - Observed inference time (s)": {
            "description": "min=0.934, mean=0.934, max=0.934, sum=1.869 (2)",
            "tab": "Efficiency",
            "score": 0.9343120718002319
          },
          "College Biology - Observed inference time (s)": {
            "description": "min=0.873, mean=0.873, max=0.873, sum=1.746 (2)",
            "tab": "Efficiency",
            "score": 0.8729922622442245
          },
          "College Computer Science - Observed inference time (s)": {
            "description": "min=1.165, mean=1.165, max=1.165, sum=2.33 (2)",
            "tab": "Efficiency",
            "score": 1.165095055103302
          },
          "College Mathematics - Observed inference time (s)": {
            "description": "min=1.062, mean=1.062, max=1.062, sum=2.124 (2)",
            "tab": "Efficiency",
            "score": 1.0619186329841614
          },
          "College Medicine - Observed inference time (s)": {
            "description": "min=0.978, mean=0.978, max=0.978, sum=1.957 (2)",
            "tab": "Efficiency",
            "score": 0.978282785140021
          },
          "College Physics - Observed inference time (s)": {
            "description": "min=0.852, mean=0.852, max=0.852, sum=1.704 (2)",
            "tab": "Efficiency",
            "score": 0.8518095483966902
          },
          "College Chemistry - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Chemistry - # prompt tokens": {
            "description": "min=573.7, mean=573.7, max=573.7, sum=1147.4 (2)",
            "tab": "General information",
            "score": 573.7
          },
          "College Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # eval": {
            "description": "min=144, mean=144, max=144, sum=288 (2)",
            "tab": "General information",
            "score": 144.0
          },
          "College Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # prompt tokens": {
            "description": "min=480.875, mean=480.875, max=480.875, sum=961.75 (2)",
            "tab": "General information",
            "score": 480.875
          },
          "College Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # prompt tokens": {
            "description": "min=861.96, mean=861.96, max=861.96, sum=1723.92 (2)",
            "tab": "General information",
            "score": 861.96
          },
          "College Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # prompt tokens": {
            "description": "min=636.94, mean=636.94, max=636.94, sum=1273.88 (2)",
            "tab": "General information",
            "score": 636.94
          },
          "College Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # eval": {
            "description": "min=173, mean=173, max=173, sum=346 (2)",
            "tab": "General information",
            "score": 173.0
          },
          "College Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # prompt tokens": {
            "description": "min=512.584, mean=512.584, max=512.584, sum=1025.168 (2)",
            "tab": "General information",
            "score": 512.5838150289018
          },
          "College Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # eval": {
            "description": "min=102, mean=102, max=102, sum=204 (2)",
            "tab": "General information",
            "score": 102.0
          },
          "College Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # prompt tokens": {
            "description": "min=513.647, mean=513.647, max=513.647, sum=1027.294 (2)",
            "tab": "General information",
            "score": 513.6470588235294
          },
          "College Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Computer Security - EM",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.77,
        "details": {
          "description": "min=0.77, mean=0.77, max=0.77, sum=1.54 (2)",
          "tab": "Accuracy",
          "Computer Security - Observed inference time (s)": {
            "description": "min=0.845, mean=0.845, max=0.845, sum=1.69 (2)",
            "tab": "Efficiency",
            "score": 0.8448482728004456
          },
          "Computer Security - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Computer Security - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Computer Security - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Computer Security - # prompt tokens": {
            "description": "min=384.24, mean=384.24, max=384.24, sum=768.48 (2)",
            "tab": "General information",
            "score": 384.24
          },
          "Computer Security - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - EM",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.649,
        "details": {
          "description": "min=0.649, mean=0.649, max=0.649, sum=1.298 (2)",
          "tab": "Accuracy",
          "Econometrics - Observed inference time (s)": {
            "description": "min=0.852, mean=0.852, max=0.852, sum=1.704 (2)",
            "tab": "Efficiency",
            "score": 0.8522159112127203
          },
          "Econometrics - # eval": {
            "description": "min=114, mean=114, max=114, sum=228 (2)",
            "tab": "General information",
            "score": 114.0
          },
          "Econometrics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Econometrics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Econometrics - # prompt tokens": {
            "description": "min=644.395, mean=644.395, max=644.395, sum=1288.789 (2)",
            "tab": "General information",
            "score": 644.3947368421053
          },
          "Econometrics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - EM",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.53,
        "details": {
          "description": "min=0.53, mean=0.53, max=0.53, sum=1.06 (2)",
          "tab": "Accuracy",
          "Global Facts - Observed inference time (s)": {
            "description": "min=0.888, mean=0.888, max=0.888, sum=1.775 (2)",
            "tab": "Efficiency",
            "score": 0.8876941871643066
          },
          "Global Facts - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Global Facts - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Global Facts - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Global Facts - # prompt tokens": {
            "description": "min=455.63, mean=455.63, max=455.63, sum=911.26 (2)",
            "tab": "General information",
            "score": 455.63
          },
          "Global Facts - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - EM",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.88,
        "details": {
          "description": "min=0.88, mean=0.88, max=0.88, sum=1.759 (2)",
          "tab": "Accuracy",
          "Jurisprudence - Observed inference time (s)": {
            "description": "min=1.017, mean=1.017, max=1.017, sum=2.034 (2)",
            "tab": "Efficiency",
            "score": 1.0168068651799802
          },
          "Jurisprudence - # eval": {
            "description": "min=108, mean=108, max=108, sum=216 (2)",
            "tab": "General information",
            "score": 108.0
          },
          "Jurisprudence - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Jurisprudence - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Jurisprudence - # prompt tokens": {
            "description": "min=414.444, mean=414.444, max=414.444, sum=828.889 (2)",
            "tab": "General information",
            "score": 414.44444444444446
          },
          "Jurisprudence - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - EM",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.836,
        "details": {
          "description": "min=0.836, mean=0.836, max=0.836, sum=1.672 (2)",
          "tab": "Accuracy",
          "Philosophy - Observed inference time (s)": {
            "description": "min=0.895, mean=0.895, max=0.895, sum=1.79 (2)",
            "tab": "Efficiency",
            "score": 0.8949410808048064
          },
          "Philosophy - # eval": {
            "description": "min=311, mean=311, max=311, sum=622 (2)",
            "tab": "General information",
            "score": 311.0
          },
          "Philosophy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Philosophy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Philosophy - # prompt tokens": {
            "description": "min=339.093, mean=339.093, max=339.093, sum=678.186 (2)",
            "tab": "General information",
            "score": 339.09324758842445
          },
          "Philosophy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Psychology - EM",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.858,
        "details": {
          "description": "min=0.858, mean=0.858, max=0.858, sum=1.716 (2)",
          "tab": "Accuracy",
          "Professional Medicine - Observed inference time (s)": {
            "description": "min=1.088, mean=1.088, max=1.088, sum=2.175 (2)",
            "tab": "Efficiency",
            "score": 1.0875138991019304
          },
          "Professional Accounting - Observed inference time (s)": {
            "description": "min=0.978, mean=0.978, max=0.978, sum=1.956 (2)",
            "tab": "Efficiency",
            "score": 0.9778145923682139
          },
          "Professional Law - Observed inference time (s)": {
            "description": "min=1.205, mean=1.205, max=1.205, sum=2.41 (2)",
            "tab": "Efficiency",
            "score": 1.204983455416743
          },
          "Professional Psychology - Observed inference time (s)": {
            "description": "min=0.789, mean=0.789, max=0.789, sum=1.578 (2)",
            "tab": "Efficiency",
            "score": 0.7891469753645604
          },
          "Professional Medicine - # eval": {
            "description": "min=272, mean=272, max=272, sum=544 (2)",
            "tab": "General information",
            "score": 272.0
          },
          "Professional Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Medicine - # prompt tokens": {
            "description": "min=1104.614, mean=1104.614, max=1104.614, sum=2209.228 (2)",
            "tab": "General information",
            "score": 1104.6139705882354
          },
          "Professional Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # eval": {
            "description": "min=282, mean=282, max=282, sum=564 (2)",
            "tab": "General information",
            "score": 282.0
          },
          "Professional Accounting - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Accounting - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # prompt tokens": {
            "description": "min=752.83, mean=752.83, max=752.83, sum=1505.66 (2)",
            "tab": "General information",
            "score": 752.8297872340426
          },
          "Professional Accounting - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # eval": {
            "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
            "tab": "General information",
            "score": 1534.0
          },
          "Professional Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # prompt tokens": {
            "description": "min=1701.909, mean=1701.909, max=1701.909, sum=3403.819 (2)",
            "tab": "General information",
            "score": 1701.9093872229466
          },
          "Professional Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # eval": {
            "description": "min=612, mean=612, max=612, sum=1224 (2)",
            "tab": "General information",
            "score": 612.0
          },
          "Professional Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # prompt tokens": {
            "description": "min=594.446, mean=594.446, max=594.446, sum=1188.892 (2)",
            "tab": "General information",
            "score": 594.4460784313726
          },
          "Professional Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - EM",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.96,
        "details": {
          "description": "min=0.96, mean=0.96, max=0.96, sum=1.92 (2)",
          "tab": "Accuracy",
          "Us Foreign Policy - Observed inference time (s)": {
            "description": "min=0.743, mean=0.743, max=0.743, sum=1.485 (2)",
            "tab": "Efficiency",
            "score": 0.7426803350448609
          },
          "Us Foreign Policy - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Us Foreign Policy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Us Foreign Policy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Us Foreign Policy - # prompt tokens": {
            "description": "min=440.48, mean=440.48, max=440.48, sum=880.96 (2)",
            "tab": "General information",
            "score": 440.48
          },
          "Us Foreign Policy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - EM",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.862,
        "details": {
          "description": "min=0.862, mean=0.862, max=0.862, sum=1.724 (2)",
          "tab": "Accuracy",
          "Astronomy - Observed inference time (s)": {
            "description": "min=0.843, mean=0.843, max=0.843, sum=1.686 (2)",
            "tab": "Efficiency",
            "score": 0.8429784712038542
          },
          "Astronomy - # eval": {
            "description": "min=152, mean=152, max=152, sum=304 (2)",
            "tab": "General information",
            "score": 152.0
          },
          "Astronomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Astronomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Astronomy - # prompt tokens": {
            "description": "min=613.033, mean=613.033, max=613.033, sum=1226.066 (2)",
            "tab": "General information",
            "score": 613.0328947368421
          },
          "Astronomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - EM",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.83,
        "details": {
          "description": "min=0.83, mean=0.83, max=0.83, sum=1.66 (2)",
          "tab": "Accuracy",
          "Business Ethics - Observed inference time (s)": {
            "description": "min=1.018, mean=1.018, max=1.018, sum=2.035 (2)",
            "tab": "Efficiency",
            "score": 1.0176324987411498
          },
          "Business Ethics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Business Ethics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Business Ethics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Business Ethics - # prompt tokens": {
            "description": "min=559.31, mean=559.31, max=559.31, sum=1118.62 (2)",
            "tab": "General information",
            "score": 559.31
          },
          "Business Ethics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - EM",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.804,
        "details": {
          "description": "min=0.804, mean=0.804, max=0.804, sum=1.608 (2)",
          "tab": "Accuracy",
          "Clinical Knowledge - Observed inference time (s)": {
            "description": "min=0.954, mean=0.954, max=0.954, sum=1.909 (2)",
            "tab": "Efficiency",
            "score": 0.9543584787620688
          },
          "Clinical Knowledge - # eval": {
            "description": "min=265, mean=265, max=265, sum=530 (2)",
            "tab": "General information",
            "score": 265.0
          },
          "Clinical Knowledge - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Clinical Knowledge - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Clinical Knowledge - # prompt tokens": {
            "description": "min=394.77, mean=394.77, max=394.77, sum=789.54 (2)",
            "tab": "General information",
            "score": 394.76981132075474
          },
          "Clinical Knowledge - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - EM",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.809,
        "details": {
          "description": "min=0.809, mean=0.809, max=0.809, sum=1.617 (2)",
          "tab": "Accuracy",
          "Conceptual Physics - Observed inference time (s)": {
            "description": "min=0.834, mean=0.834, max=0.834, sum=1.667 (2)",
            "tab": "Efficiency",
            "score": 0.8336589884250722
          },
          "Conceptual Physics - # eval": {
            "description": "min=235, mean=235, max=235, sum=470 (2)",
            "tab": "General information",
            "score": 235.0
          },
          "Conceptual Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Conceptual Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Conceptual Physics - # prompt tokens": {
            "description": "min=309.477, mean=309.477, max=309.477, sum=618.953 (2)",
            "tab": "General information",
            "score": 309.4765957446809
          },
          "Conceptual Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - EM",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.772,
        "details": {
          "description": "min=0.772, mean=0.772, max=0.772, sum=1.545 (2)",
          "tab": "Accuracy",
          "Electrical Engineering - Observed inference time (s)": {
            "description": "min=1.064, mean=1.064, max=1.064, sum=2.128 (2)",
            "tab": "Efficiency",
            "score": 1.0639554155283961
          },
          "Electrical Engineering - # eval": {
            "description": "min=145, mean=145, max=145, sum=290 (2)",
            "tab": "General information",
            "score": 145.0
          },
          "Electrical Engineering - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Electrical Engineering - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Electrical Engineering - # prompt tokens": {
            "description": "min=480.524, mean=480.524, max=480.524, sum=961.048 (2)",
            "tab": "General information",
            "score": 480.5241379310345
          },
          "Electrical Engineering - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.661,
        "details": {
          "description": "min=0.661, mean=0.661, max=0.661, sum=1.323 (2)",
          "tab": "Accuracy",
          "Elementary Mathematics - Observed inference time (s)": {
            "description": "min=1.026, mean=1.026, max=1.026, sum=2.052 (2)",
            "tab": "Efficiency",
            "score": 1.0261994568759172
          },
          "Elementary Mathematics - # eval": {
            "description": "min=378, mean=378, max=378, sum=756 (2)",
            "tab": "General information",
            "score": 378.0
          },
          "Elementary Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Elementary Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Elementary Mathematics - # prompt tokens": {
            "description": "min=599.828, mean=599.828, max=599.828, sum=1199.656 (2)",
            "tab": "General information",
            "score": 599.8280423280423
          },
          "Elementary Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - EM",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.659,
        "details": {
          "description": "min=0.659, mean=0.659, max=0.659, sum=1.317 (2)",
          "tab": "Accuracy",
          "Formal Logic - Observed inference time (s)": {
            "description": "min=1.016, mean=1.016, max=1.016, sum=2.032 (2)",
            "tab": "Efficiency",
            "score": 1.0157842484731523
          },
          "Formal Logic - # eval": {
            "description": "min=126, mean=126, max=126, sum=252 (2)",
            "tab": "General information",
            "score": 126.0
          },
          "Formal Logic - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Formal Logic - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Formal Logic - # prompt tokens": {
            "description": "min=623.508, mean=623.508, max=623.508, sum=1247.016 (2)",
            "tab": "General information",
            "score": 623.5079365079365
          },
          "Formal Logic - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School World History - EM",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.911,
        "details": {
          "description": "min=0.911, mean=0.911, max=0.911, sum=1.823 (2)",
          "tab": "Accuracy",
          "High School Biology - Observed inference time (s)": {
            "description": "min=1.026, mean=1.026, max=1.026, sum=2.052 (2)",
            "tab": "Efficiency",
            "score": 1.026222055189071
          },
          "High School Chemistry - Observed inference time (s)": {
            "description": "min=1.054, mean=1.054, max=1.054, sum=2.109 (2)",
            "tab": "Efficiency",
            "score": 1.054317417990398
          },
          "High School Computer Science - Observed inference time (s)": {
            "description": "min=1.519, mean=1.519, max=1.519, sum=3.039 (2)",
            "tab": "Efficiency",
            "score": 1.519298493862152
          },
          "High School European History - Observed inference time (s)": {
            "description": "min=2.108, mean=2.108, max=2.108, sum=4.215 (2)",
            "tab": "Efficiency",
            "score": 2.107529640197754
          },
          "High School Geography - Observed inference time (s)": {
            "description": "min=1.159, mean=1.159, max=1.159, sum=2.319 (2)",
            "tab": "Efficiency",
            "score": 1.1594982544581096
          },
          "High School Government And Politics - Observed inference time (s)": {
            "description": "min=1.056, mean=1.056, max=1.056, sum=2.112 (2)",
            "tab": "Efficiency",
            "score": 1.0561638829621627
          },
          "High School Macroeconomics - Observed inference time (s)": {
            "description": "min=1.016, mean=1.016, max=1.016, sum=2.033 (2)",
            "tab": "Efficiency",
            "score": 1.0163854268880992
          },
          "High School Mathematics - Observed inference time (s)": {
            "description": "min=1.018, mean=1.018, max=1.018, sum=2.036 (2)",
            "tab": "Efficiency",
            "score": 1.0180342506479334
          },
          "High School Microeconomics - Observed inference time (s)": {
            "description": "min=0.905, mean=0.905, max=0.905, sum=1.811 (2)",
            "tab": "Efficiency",
            "score": 0.9054926122937884
          },
          "High School Physics - Observed inference time (s)": {
            "description": "min=1.252, mean=1.252, max=1.252, sum=2.503 (2)",
            "tab": "Efficiency",
            "score": 1.2517439276966829
          },
          "High School Psychology - Observed inference time (s)": {
            "description": "min=0.954, mean=0.954, max=0.954, sum=1.909 (2)",
            "tab": "Efficiency",
            "score": 0.9543260762450891
          },
          "High School Statistics - Observed inference time (s)": {
            "description": "min=1.329, mean=1.329, max=1.329, sum=2.657 (2)",
            "tab": "Efficiency",
            "score": 1.3287169370386336
          },
          "High School US History - Observed inference time (s)": {
            "description": "min=2.056, mean=2.056, max=2.056, sum=4.112 (2)",
            "tab": "Efficiency",
            "score": 2.0560385222528494
          },
          "High School World History - Observed inference time (s)": {
            "description": "min=1.276, mean=1.276, max=1.276, sum=2.553 (2)",
            "tab": "Efficiency",
            "score": 1.2764891250224053
          },
          "High School Biology - # eval": {
            "description": "min=310, mean=310, max=310, sum=620 (2)",
            "tab": "General information",
            "score": 310.0
          },
          "High School Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Biology - # prompt tokens": {
            "description": "min=501.255, mean=501.255, max=501.255, sum=1002.51 (2)",
            "tab": "General information",
            "score": 501.2548387096774
          },
          "High School Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # eval": {
            "description": "min=203, mean=203, max=203, sum=406 (2)",
            "tab": "General information",
            "score": 203.0
          },
          "High School Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # prompt tokens": {
            "description": "min=515.473, mean=515.473, max=515.473, sum=1030.946 (2)",
            "tab": "General information",
            "score": 515.4729064039409
          },
          "High School Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "High School Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # prompt tokens": {
            "description": "min=954.08, mean=954.08, max=954.08, sum=1908.16 (2)",
            "tab": "General information",
            "score": 954.08
          },
          "High School Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # eval": {
            "description": "min=165, mean=165, max=165, sum=330 (2)",
            "tab": "General information",
            "score": 165.0
          },
          "High School European History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School European History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # prompt tokens": {
            "description": "min=2823.23, mean=2823.23, max=2823.23, sum=5646.461 (2)",
            "tab": "General information",
            "score": 2823.230303030303
          },
          "High School European History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # eval": {
            "description": "min=198, mean=198, max=198, sum=396 (2)",
            "tab": "General information",
            "score": 198.0
          },
          "High School Geography - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Geography - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # prompt tokens": {
            "description": "min=392.939, mean=392.939, max=392.939, sum=785.879 (2)",
            "tab": "General information",
            "score": 392.93939393939394
          },
          "High School Geography - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # eval": {
            "description": "min=193, mean=193, max=193, sum=386 (2)",
            "tab": "General information",
            "score": 193.0
          },
          "High School Government And Politics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Government And Politics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # prompt tokens": {
            "description": "min=475.44, mean=475.44, max=475.44, sum=950.881 (2)",
            "tab": "General information",
            "score": 475.440414507772
          },
          "High School Government And Politics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # eval": {
            "description": "min=390, mean=390, max=390, sum=780 (2)",
            "tab": "General information",
            "score": 390.0
          },
          "High School Macroeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Macroeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # prompt tokens": {
            "description": "min=395.962, mean=395.962, max=395.962, sum=791.923 (2)",
            "tab": "General information",
            "score": 395.96153846153845
          },
          "High School Macroeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # eval": {
            "description": "min=270, mean=270, max=270, sum=540 (2)",
            "tab": "General information",
            "score": 270.0
          },
          "High School Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # prompt tokens": {
            "description": "min=580.393, mean=580.393, max=580.393, sum=1160.785 (2)",
            "tab": "General information",
            "score": 580.3925925925926
          },
          "High School Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # eval": {
            "description": "min=238, mean=238, max=238, sum=476 (2)",
            "tab": "General information",
            "score": 238.0
          },
          "High School Microeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Microeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # prompt tokens": {
            "description": "min=414.361, mean=414.361, max=414.361, sum=828.723 (2)",
            "tab": "General information",
            "score": 414.3613445378151
          },
          "High School Microeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # eval": {
            "description": "min=151, mean=151, max=151, sum=302 (2)",
            "tab": "General information",
            "score": 151.0
          },
          "High School Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # prompt tokens": {
            "description": "min=592.252, mean=592.252, max=592.252, sum=1184.503 (2)",
            "tab": "General information",
            "score": 592.2516556291391
          },
          "High School Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # eval": {
            "description": "min=545, mean=545, max=545, sum=1090 (2)",
            "tab": "General information",
            "score": 545.0
          },
          "High School Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # prompt tokens": {
            "description": "min=496.51, mean=496.51, max=496.51, sum=993.02 (2)",
            "tab": "General information",
            "score": 496.5100917431193
          },
          "High School Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # eval": {
            "description": "min=216, mean=216, max=216, sum=432 (2)",
            "tab": "General information",
            "score": 216.0
          },
          "High School Statistics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Statistics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # prompt tokens": {
            "description": "min=860.532, mean=860.532, max=860.532, sum=1721.065 (2)",
            "tab": "General information",
            "score": 860.5324074074074
          },
          "High School Statistics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # eval": {
            "description": "min=204, mean=204, max=204, sum=408 (2)",
            "tab": "General information",
            "score": 204.0
          },
          "High School US History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School US History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # prompt tokens": {
            "description": "min=2239.544, mean=2239.544, max=2239.544, sum=4479.088 (2)",
            "tab": "General information",
            "score": 2239.544117647059
          },
          "High School US History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # eval": {
            "description": "min=237, mean=237, max=237, sum=474 (2)",
            "tab": "General information",
            "score": 237.0
          },
          "High School World History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School World History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # prompt tokens": {
            "description": "min=1437.051, mean=1437.051, max=1437.051, sum=2874.101 (2)",
            "tab": "General information",
            "score": 1437.0506329113923
          },
          "High School World History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Sexuality - EM",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.924,
        "details": {
          "description": "min=0.924, mean=0.924, max=0.924, sum=1.847 (2)",
          "tab": "Accuracy",
          "Human Aging - Observed inference time (s)": {
            "description": "min=0.884, mean=0.884, max=0.884, sum=1.768 (2)",
            "tab": "Efficiency",
            "score": 0.8839223662833996
          },
          "Human Sexuality - Observed inference time (s)": {
            "description": "min=1.095, mean=1.095, max=1.095, sum=2.191 (2)",
            "tab": "Efficiency",
            "score": 1.0953879956980699
          },
          "Human Aging - # eval": {
            "description": "min=223, mean=223, max=223, sum=446 (2)",
            "tab": "General information",
            "score": 223.0
          },
          "Human Aging - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Aging - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Aging - # prompt tokens": {
            "description": "min=323.906, mean=323.906, max=323.906, sum=647.812 (2)",
            "tab": "General information",
            "score": 323.90582959641256
          },
          "Human Aging - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # eval": {
            "description": "min=131, mean=131, max=131, sum=262 (2)",
            "tab": "General information",
            "score": 131.0
          },
          "Human Sexuality - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Sexuality - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # prompt tokens": {
            "description": "min=338.74, mean=338.74, max=338.74, sum=677.481 (2)",
            "tab": "General information",
            "score": 338.74045801526717
          },
          "Human Sexuality - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - EM",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.909,
        "details": {
          "description": "min=0.909, mean=0.909, max=0.909, sum=1.818 (2)",
          "tab": "Accuracy",
          "International Law - Observed inference time (s)": {
            "description": "min=1.104, mean=1.104, max=1.104, sum=2.208 (2)",
            "tab": "Efficiency",
            "score": 1.1039516984923812
          },
          "International Law - # eval": {
            "description": "min=121, mean=121, max=121, sum=242 (2)",
            "tab": "General information",
            "score": 121.0
          },
          "International Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "International Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "International Law - # prompt tokens": {
            "description": "min=651.686, mean=651.686, max=651.686, sum=1303.372 (2)",
            "tab": "General information",
            "score": 651.6859504132232
          },
          "International Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - EM",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.877,
        "details": {
          "description": "min=0.877, mean=0.877, max=0.877, sum=1.755 (2)",
          "tab": "Accuracy",
          "Logical Fallacies - Observed inference time (s)": {
            "description": "min=1.094, mean=1.094, max=1.094, sum=2.188 (2)",
            "tab": "Efficiency",
            "score": 1.0941538839983793
          },
          "Logical Fallacies - # eval": {
            "description": "min=163, mean=163, max=163, sum=326 (2)",
            "tab": "General information",
            "score": 163.0
          },
          "Logical Fallacies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Logical Fallacies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Logical Fallacies - # prompt tokens": {
            "description": "min=443.969, mean=443.969, max=443.969, sum=887.939 (2)",
            "tab": "General information",
            "score": 443.96932515337426
          },
          "Logical Fallacies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - EM",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.625,
        "details": {
          "description": "min=0.625, mean=0.625, max=0.625, sum=1.25 (2)",
          "tab": "Accuracy",
          "Machine Learning - Observed inference time (s)": {
            "description": "min=1.11, mean=1.11, max=1.11, sum=2.22 (2)",
            "tab": "Efficiency",
            "score": 1.110024324485234
          },
          "Machine Learning - # eval": {
            "description": "min=112, mean=112, max=112, sum=224 (2)",
            "tab": "General information",
            "score": 112.0
          },
          "Machine Learning - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Machine Learning - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Machine Learning - # prompt tokens": {
            "description": "min=705.973, mean=705.973, max=705.973, sum=1411.946 (2)",
            "tab": "General information",
            "score": 705.9732142857143
          },
          "Machine Learning - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - EM",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.903,
        "details": {
          "description": "min=0.903, mean=0.903, max=0.903, sum=1.806 (2)",
          "tab": "Accuracy",
          "Management - Observed inference time (s)": {
            "description": "min=1.154, mean=1.154, max=1.154, sum=2.308 (2)",
            "tab": "Efficiency",
            "score": 1.153875772235463
          },
          "Management - # eval": {
            "description": "min=103, mean=103, max=103, sum=206 (2)",
            "tab": "General information",
            "score": 103.0
          },
          "Management - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Management - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Management - # prompt tokens": {
            "description": "min=284.68, mean=284.68, max=284.68, sum=569.359 (2)",
            "tab": "General information",
            "score": 284.6796116504854
          },
          "Management - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - EM",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.94,
        "details": {
          "description": "min=0.94, mean=0.94, max=0.94, sum=1.88 (2)",
          "tab": "Accuracy",
          "Marketing - Observed inference time (s)": {
            "description": "min=1.031, mean=1.031, max=1.031, sum=2.063 (2)",
            "tab": "Efficiency",
            "score": 1.0312827428181965
          },
          "Marketing - # eval": {
            "description": "min=234, mean=234, max=234, sum=468 (2)",
            "tab": "General information",
            "score": 234.0
          },
          "Marketing - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Marketing - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Marketing - # prompt tokens": {
            "description": "min=428.726, mean=428.726, max=428.726, sum=857.453 (2)",
            "tab": "General information",
            "score": 428.7264957264957
          },
          "Marketing - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - EM",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.83,
        "details": {
          "description": "min=0.83, mean=0.83, max=0.83, sum=1.66 (2)",
          "tab": "Accuracy",
          "Medical Genetics - Observed inference time (s)": {
            "description": "min=1.068, mean=1.068, max=1.068, sum=2.136 (2)",
            "tab": "Efficiency",
            "score": 1.0681284523010255
          },
          "Medical Genetics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Medical Genetics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Medical Genetics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Medical Genetics - # prompt tokens": {
            "description": "min=334.69, mean=334.69, max=334.69, sum=669.38 (2)",
            "tab": "General information",
            "score": 334.69
          },
          "Medical Genetics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - EM",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.894,
        "details": {
          "description": "min=0.894, mean=0.894, max=0.894, sum=1.788 (2)",
          "tab": "Accuracy",
          "Miscellaneous - Observed inference time (s)": {
            "description": "min=0.894, mean=0.894, max=0.894, sum=1.788 (2)",
            "tab": "Efficiency",
            "score": 0.8939257733818824
          },
          "Miscellaneous - # eval": {
            "description": "min=783, mean=783, max=783, sum=1566 (2)",
            "tab": "General information",
            "score": 783.0
          },
          "Miscellaneous - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Miscellaneous - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Miscellaneous - # prompt tokens": {
            "description": "min=325.215, mean=325.215, max=325.215, sum=650.429 (2)",
            "tab": "General information",
            "score": 325.2145593869732
          },
          "Miscellaneous - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - EM",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.562,
        "details": {
          "description": "min=0.562, mean=0.562, max=0.562, sum=1.124 (2)",
          "tab": "Accuracy",
          "Moral Disputes - Observed inference time (s)": {
            "description": "min=0.988, mean=0.988, max=0.988, sum=1.976 (2)",
            "tab": "Efficiency",
            "score": 0.9880901995421834
          },
          "Moral Scenarios - Observed inference time (s)": {
            "description": "min=0.968, mean=0.968, max=0.968, sum=1.935 (2)",
            "tab": "Efficiency",
            "score": 0.9677273009742439
          },
          "Moral Disputes - # eval": {
            "description": "min=346, mean=346, max=346, sum=692 (2)",
            "tab": "General information",
            "score": 346.0
          },
          "Moral Disputes - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Disputes - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Disputes - # prompt tokens": {
            "description": "min=494.63, mean=494.63, max=494.63, sum=989.26 (2)",
            "tab": "General information",
            "score": 494.6300578034682
          },
          "Moral Disputes - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # eval": {
            "description": "min=895, mean=895, max=895, sum=1790 (2)",
            "tab": "General information",
            "score": 895.0
          },
          "Moral Scenarios - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Scenarios - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # prompt tokens": {
            "description": "min=687.566, mean=687.566, max=687.566, sum=1375.133 (2)",
            "tab": "General information",
            "score": 687.5664804469274
          },
          "Moral Scenarios - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - EM",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.856,
        "details": {
          "description": "min=0.856, mean=0.856, max=0.856, sum=1.712 (2)",
          "tab": "Accuracy",
          "Nutrition - Observed inference time (s)": {
            "description": "min=0.912, mean=0.912, max=0.912, sum=1.824 (2)",
            "tab": "Efficiency",
            "score": 0.9120152238147711
          },
          "Nutrition - # eval": {
            "description": "min=306, mean=306, max=306, sum=612 (2)",
            "tab": "General information",
            "score": 306.0
          },
          "Nutrition - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Nutrition - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Nutrition - # prompt tokens": {
            "description": "min=589.663, mean=589.663, max=589.663, sum=1179.327 (2)",
            "tab": "General information",
            "score": 589.6633986928105
          },
          "Nutrition - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - EM",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.87,
        "details": {
          "description": "min=0.87, mean=0.87, max=0.87, sum=1.741 (2)",
          "tab": "Accuracy",
          "Prehistory - Observed inference time (s)": {
            "description": "min=0.916, mean=0.916, max=0.916, sum=1.831 (2)",
            "tab": "Efficiency",
            "score": 0.9155398577819636
          },
          "Prehistory - # eval": {
            "description": "min=324, mean=324, max=324, sum=648 (2)",
            "tab": "General information",
            "score": 324.0
          },
          "Prehistory - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Prehistory - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Prehistory - # prompt tokens": {
            "description": "min=538.179, mean=538.179, max=538.179, sum=1076.358 (2)",
            "tab": "General information",
            "score": 538.179012345679
          },
          "Prehistory - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Public Relations - EM",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.773,
        "details": {
          "description": "min=0.773, mean=0.773, max=0.773, sum=1.545 (2)",
          "tab": "Accuracy",
          "Public Relations - Observed inference time (s)": {
            "description": "min=0.79, mean=0.79, max=0.79, sum=1.579 (2)",
            "tab": "Efficiency",
            "score": 0.7896393559195779
          },
          "Public Relations - # eval": {
            "description": "min=110, mean=110, max=110, sum=220 (2)",
            "tab": "General information",
            "score": 110.0
          },
          "Public Relations - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Public Relations - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Public Relations - # prompt tokens": {
            "description": "min=426.982, mean=426.982, max=426.982, sum=853.964 (2)",
            "tab": "General information",
            "score": 426.9818181818182
          },
          "Public Relations - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - EM",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.829,
        "details": {
          "description": "min=0.829, mean=0.829, max=0.829, sum=1.657 (2)",
          "tab": "Accuracy",
          "Security Studies - Observed inference time (s)": {
            "description": "min=1.254, mean=1.254, max=1.254, sum=2.508 (2)",
            "tab": "Efficiency",
            "score": 1.2542338507516044
          },
          "Security Studies - # eval": {
            "description": "min=245, mean=245, max=245, sum=490 (2)",
            "tab": "General information",
            "score": 245.0
          },
          "Security Studies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Security Studies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Security Studies - # prompt tokens": {
            "description": "min=1185.8, mean=1185.8, max=1185.8, sum=2371.6 (2)",
            "tab": "General information",
            "score": 1185.8
          },
          "Security Studies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - EM",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.91,
        "details": {
          "description": "min=0.91, mean=0.91, max=0.91, sum=1.821 (2)",
          "tab": "Accuracy",
          "Sociology - Observed inference time (s)": {
            "description": "min=0.84, mean=0.84, max=0.84, sum=1.681 (2)",
            "tab": "Efficiency",
            "score": 0.8403987184685854
          },
          "Sociology - # eval": {
            "description": "min=201, mean=201, max=201, sum=402 (2)",
            "tab": "General information",
            "score": 201.0
          },
          "Sociology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Sociology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Sociology - # prompt tokens": {
            "description": "min=459.642, mean=459.642, max=459.642, sum=919.284 (2)",
            "tab": "General information",
            "score": 459.64179104477614
          },
          "Sociology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - EM",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.572,
        "details": {
          "description": "min=0.572, mean=0.572, max=0.572, sum=1.145 (2)",
          "tab": "Accuracy",
          "Virology - Observed inference time (s)": {
            "description": "min=1.029, mean=1.029, max=1.029, sum=2.059 (2)",
            "tab": "Efficiency",
            "score": 1.0293473134557884
          },
          "Virology - # eval": {
            "description": "min=166, mean=166, max=166, sum=332 (2)",
            "tab": "General information",
            "score": 166.0
          },
          "Virology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Virology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Virology - # prompt tokens": {
            "description": "min=337.06, mean=337.06, max=337.06, sum=674.12 (2)",
            "tab": "General information",
            "score": 337.06024096385545
          },
          "Virology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - EM",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.877,
        "details": {
          "description": "min=0.877, mean=0.877, max=0.877, sum=1.754 (2)",
          "tab": "Accuracy",
          "World Religions - Observed inference time (s)": {
            "description": "min=0.963, mean=0.963, max=0.963, sum=1.926 (2)",
            "tab": "Efficiency",
            "score": 0.9628847495854249
          },
          "World Religions - # eval": {
            "description": "min=171, mean=171, max=171, sum=342 (2)",
            "tab": "General information",
            "score": 171.0
          },
          "World Religions - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "World Religions - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "World Religions - # prompt tokens": {
            "description": "min=270.187, mean=270.187, max=270.187, sum=540.374 (2)",
            "tab": "General information",
            "score": 270.187134502924
          },
          "World Religions - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.142,
        "details": {
          "description": null,
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    }
  ]
}