{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_mmlu/google_gemini-2.0-flash-exp/1770835937.459157",
  "retrieved_timestamp": "1770835937.459157",
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Gemini 2.0 Flash Experimental",
    "id": "google/gemini-2.0-flash-exp",
    "developer": "google",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU All Subjects",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.797,
        "details": {
          "description": "min=0.554, mean=0.797, max=0.969, sum=90.902 (114)",
          "tab": "Accuracy",
          "MMLU All Subjects - Observed inference time (s)": {
            "description": "min=0.379, mean=0.422, max=0.926, sum=48.097 (114)",
            "tab": "Efficiency",
            "score": 0.4219020959728089
          },
          "MMLU All Subjects - # eval": {
            "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
            "tab": "General information",
            "score": 246.35087719298247
          },
          "MMLU All Subjects - # train": {
            "description": "min=5, mean=5, max=5, sum=570 (114)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU All Subjects - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU All Subjects - # prompt tokens": {
            "description": "min=268.164, mean=632.617, max=2797.424, sum=72118.345 (114)",
            "tab": "General information",
            "score": 632.6170571214202
          },
          "MMLU All Subjects - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "anatomy",
            "astronomy",
            "business_ethics",
            "clinical_knowledge",
            "college_biology",
            "college_chemistry",
            "college_computer_science",
            "college_mathematics",
            "college_medicine",
            "college_physics",
            "computer_security",
            "conceptual_physics",
            "econometrics",
            "electrical_engineering",
            "elementary_mathematics",
            "formal_logic",
            "global_facts",
            "high_school_biology",
            "high_school_chemistry",
            "high_school_computer_science",
            "high_school_european_history",
            "high_school_geography",
            "high_school_government_and_politics",
            "high_school_macroeconomics",
            "high_school_mathematics",
            "high_school_microeconomics",
            "high_school_physics",
            "high_school_psychology",
            "high_school_statistics",
            "high_school_us_history",
            "high_school_world_history",
            "human_aging",
            "human_sexuality",
            "international_law",
            "jurisprudence",
            "logical_fallacies",
            "machine_learning",
            "management",
            "marketing",
            "medical_genetics",
            "miscellaneous",
            "moral_disputes",
            "moral_scenarios",
            "nutrition",
            "philosophy",
            "prehistory",
            "professional_accounting",
            "professional_law",
            "professional_medicine",
            "professional_psychology",
            "public_relations",
            "security_studies",
            "sociology",
            "us_foreign_policy",
            "virology",
            "world_religions"
          ],
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": [
            "mmlu_abstract_algebra",
            "mmlu_anatomy",
            "mmlu_astronomy",
            "mmlu_business_ethics",
            "mmlu_clinical_knowledge",
            "mmlu_college_biology",
            "mmlu_college_chemistry",
            "mmlu_college_computer_science",
            "mmlu_college_mathematics",
            "mmlu_college_medicine",
            "mmlu_college_physics",
            "mmlu_computer_security",
            "mmlu_conceptual_physics",
            "mmlu_econometrics",
            "mmlu_electrical_engineering",
            "mmlu_elementary_mathematics",
            "mmlu_formal_logic",
            "mmlu_global_facts",
            "mmlu_high_school_biology",
            "mmlu_high_school_chemistry",
            "mmlu_high_school_computer_science",
            "mmlu_high_school_european_history",
            "mmlu_high_school_geography",
            "mmlu_high_school_government_and_politics",
            "mmlu_high_school_macroeconomics",
            "mmlu_high_school_mathematics",
            "mmlu_high_school_microeconomics",
            "mmlu_high_school_physics",
            "mmlu_high_school_psychology",
            "mmlu_high_school_statistics",
            "mmlu_high_school_us_history",
            "mmlu_high_school_world_history",
            "mmlu_human_aging",
            "mmlu_human_sexuality",
            "mmlu_international_law",
            "mmlu_jurisprudence",
            "mmlu_logical_fallacies",
            "mmlu_machine_learning",
            "mmlu_management",
            "mmlu_marketing",
            "mmlu_medical_genetics",
            "mmlu_miscellaneous",
            "mmlu_moral_disputes",
            "mmlu_moral_scenarios",
            "mmlu_nutrition",
            "mmlu_philosophy",
            "mmlu_prehistory",
            "mmlu_professional_accounting",
            "mmlu_professional_law",
            "mmlu_professional_medicine",
            "mmlu_professional_psychology",
            "mmlu_public_relations",
            "mmlu_security_studies",
            "mmlu_sociology",
            "mmlu_us_foreign_policy",
            "mmlu_virology",
            "mmlu_world_religions"
          ]
        }
      }
    },
    {
      "evaluation_name": "Abstract Algebra",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Abstract Algebra",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.72,
        "details": {
          "description": "min=0.72, mean=0.72, max=0.72, sum=1.44 (2)",
          "tab": "Accuracy",
          "Abstract Algebra - Observed inference time (s)": {
            "description": "min=0.408, mean=0.408, max=0.408, sum=0.816 (2)",
            "tab": "Efficiency",
            "score": 0.4077691292762756
          },
          "Abstract Algebra - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Abstract Algebra - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Abstract Algebra - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Abstract Algebra - # prompt tokens": {
            "description": "min=383.97, mean=383.97, max=383.97, sum=767.94 (2)",
            "tab": "General information",
            "score": 383.97
          },
          "Abstract Algebra - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "abstract_algebra",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_abstract_algebra"
        }
      }
    },
    {
      "evaluation_name": "Anatomy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Anatomy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.807,
        "details": {
          "description": "min=0.807, mean=0.807, max=0.807, sum=1.615 (2)",
          "tab": "Accuracy",
          "Anatomy - Observed inference time (s)": {
            "description": "min=0.926, mean=0.926, max=0.926, sum=1.852 (2)",
            "tab": "Efficiency",
            "score": 0.9258230227011222
          },
          "Anatomy - # eval": {
            "description": "min=135, mean=135, max=135, sum=270 (2)",
            "tab": "General information",
            "score": 135.0
          },
          "Anatomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Anatomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Anatomy - # prompt tokens": {
            "description": "min=344.356, mean=344.356, max=344.356, sum=688.711 (2)",
            "tab": "General information",
            "score": 344.35555555555555
          },
          "Anatomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "anatomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_anatomy"
        }
      }
    },
    {
      "evaluation_name": "College Physics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on College Physics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.696,
        "details": {
          "description": "min=0.696, mean=0.696, max=0.696, sum=1.392 (2)",
          "tab": "Accuracy",
          "College Chemistry - Observed inference time (s)": {
            "description": "min=0.405, mean=0.405, max=0.405, sum=0.809 (2)",
            "tab": "Efficiency",
            "score": 0.4045387363433838
          },
          "College Biology - Observed inference time (s)": {
            "description": "min=0.47, mean=0.47, max=0.47, sum=0.941 (2)",
            "tab": "Efficiency",
            "score": 0.4703653355439504
          },
          "College Computer Science - Observed inference time (s)": {
            "description": "min=0.436, mean=0.436, max=0.436, sum=0.872 (2)",
            "tab": "Efficiency",
            "score": 0.4358289122581482
          },
          "College Mathematics - Observed inference time (s)": {
            "description": "min=0.413, mean=0.413, max=0.413, sum=0.827 (2)",
            "tab": "Efficiency",
            "score": 0.413386971950531
          },
          "College Medicine - Observed inference time (s)": {
            "description": "min=0.426, mean=0.426, max=0.426, sum=0.852 (2)",
            "tab": "Efficiency",
            "score": 0.4259330606184943
          },
          "College Physics - Observed inference time (s)": {
            "description": "min=0.456, mean=0.456, max=0.456, sum=0.912 (2)",
            "tab": "Efficiency",
            "score": 0.4557511432498109
          },
          "College Chemistry - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Chemistry - # prompt tokens": {
            "description": "min=570.02, mean=570.02, max=570.02, sum=1140.04 (2)",
            "tab": "General information",
            "score": 570.02
          },
          "College Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # eval": {
            "description": "min=144, mean=144, max=144, sum=288 (2)",
            "tab": "General information",
            "score": 144.0
          },
          "College Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # prompt tokens": {
            "description": "min=482.799, mean=482.799, max=482.799, sum=965.597 (2)",
            "tab": "General information",
            "score": 482.7986111111111
          },
          "College Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # prompt tokens": {
            "description": "min=857.86, mean=857.86, max=857.86, sum=1715.72 (2)",
            "tab": "General information",
            "score": 857.86
          },
          "College Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # prompt tokens": {
            "description": "min=626.69, mean=626.69, max=626.69, sum=1253.38 (2)",
            "tab": "General information",
            "score": 626.69
          },
          "College Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # eval": {
            "description": "min=173, mean=173, max=173, sum=346 (2)",
            "tab": "General information",
            "score": 173.0
          },
          "College Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # prompt tokens": {
            "description": "min=513.37, mean=513.37, max=513.37, sum=1026.74 (2)",
            "tab": "General information",
            "score": 513.3699421965318
          },
          "College Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # eval": {
            "description": "min=102, mean=102, max=102, sum=204 (2)",
            "tab": "General information",
            "score": 102.0
          },
          "College Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # prompt tokens": {
            "description": "min=507.471, mean=507.471, max=507.471, sum=1014.941 (2)",
            "tab": "General information",
            "score": 507.47058823529414
          },
          "College Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "college_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_college_physics"
        }
      }
    },
    {
      "evaluation_name": "Computer Security",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Computer Security",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.83,
        "details": {
          "description": "min=0.83, mean=0.83, max=0.83, sum=1.66 (2)",
          "tab": "Accuracy",
          "Computer Security - Observed inference time (s)": {
            "description": "min=0.407, mean=0.407, max=0.407, sum=0.813 (2)",
            "tab": "Efficiency",
            "score": 0.4065685248374939
          },
          "Computer Security - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Computer Security - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Computer Security - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Computer Security - # prompt tokens": {
            "description": "min=380.91, mean=380.91, max=380.91, sum=761.82 (2)",
            "tab": "General information",
            "score": 380.91
          },
          "Computer Security - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "computer_security",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_computer_security"
        }
      }
    },
    {
      "evaluation_name": "Econometrics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Econometrics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.693,
        "details": {
          "description": "min=0.693, mean=0.693, max=0.693, sum=1.386 (2)",
          "tab": "Accuracy",
          "Econometrics - Observed inference time (s)": {
            "description": "min=0.41, mean=0.41, max=0.41, sum=0.819 (2)",
            "tab": "Efficiency",
            "score": 0.4097107544279935
          },
          "Econometrics - # eval": {
            "description": "min=114, mean=114, max=114, sum=228 (2)",
            "tab": "General information",
            "score": 114.0
          },
          "Econometrics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Econometrics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Econometrics - # prompt tokens": {
            "description": "min=634.553, mean=634.553, max=634.553, sum=1269.105 (2)",
            "tab": "General information",
            "score": 634.5526315789474
          },
          "Econometrics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "econometrics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_econometrics"
        }
      }
    },
    {
      "evaluation_name": "Global Facts",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Global Facts",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.66,
        "details": {
          "description": "min=0.66, mean=0.66, max=0.66, sum=1.32 (2)",
          "tab": "Accuracy",
          "Global Facts - Observed inference time (s)": {
            "description": "min=0.415, mean=0.415, max=0.415, sum=0.83 (2)",
            "tab": "Efficiency",
            "score": 0.4148475766181946
          },
          "Global Facts - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Global Facts - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Global Facts - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Global Facts - # prompt tokens": {
            "description": "min=456.54, mean=456.54, max=456.54, sum=913.08 (2)",
            "tab": "General information",
            "score": 456.54
          },
          "Global Facts - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "global_facts",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_global_facts"
        }
      }
    },
    {
      "evaluation_name": "Jurisprudence",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Jurisprudence",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.898,
        "details": {
          "description": "min=0.898, mean=0.898, max=0.898, sum=1.796 (2)",
          "tab": "Accuracy",
          "Jurisprudence - Observed inference time (s)": {
            "description": "min=0.442, mean=0.442, max=0.442, sum=0.884 (2)",
            "tab": "Efficiency",
            "score": 0.4418119721942478
          },
          "Jurisprudence - # eval": {
            "description": "min=108, mean=108, max=108, sum=216 (2)",
            "tab": "General information",
            "score": 108.0
          },
          "Jurisprudence - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Jurisprudence - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Jurisprudence - # prompt tokens": {
            "description": "min=407.87, mean=407.87, max=407.87, sum=815.741 (2)",
            "tab": "General information",
            "score": 407.8703703703704
          },
          "Jurisprudence - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "jurisprudence",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_jurisprudence"
        }
      }
    },
    {
      "evaluation_name": "Philosophy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Philosophy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.887,
        "details": {
          "description": "min=0.887, mean=0.887, max=0.887, sum=1.775 (2)",
          "tab": "Accuracy",
          "Philosophy - Observed inference time (s)": {
            "description": "min=0.409, mean=0.409, max=0.409, sum=0.817 (2)",
            "tab": "Efficiency",
            "score": 0.40853408831875426
          },
          "Philosophy - # eval": {
            "description": "min=311, mean=311, max=311, sum=622 (2)",
            "tab": "General information",
            "score": 311.0
          },
          "Philosophy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Philosophy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Philosophy - # prompt tokens": {
            "description": "min=340.907, mean=340.907, max=340.907, sum=681.814 (2)",
            "tab": "General information",
            "score": 340.90675241157555
          },
          "Philosophy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "philosophy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_philosophy"
        }
      }
    },
    {
      "evaluation_name": "Professional Psychology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Professional Psychology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.876,
        "details": {
          "description": "min=0.876, mean=0.876, max=0.876, sum=1.752 (2)",
          "tab": "Accuracy",
          "Professional Medicine - Observed inference time (s)": {
            "description": "min=0.467, mean=0.467, max=0.467, sum=0.934 (2)",
            "tab": "Efficiency",
            "score": 0.46713243337238536
          },
          "Professional Accounting - Observed inference time (s)": {
            "description": "min=0.386, mean=0.386, max=0.386, sum=0.771 (2)",
            "tab": "Efficiency",
            "score": 0.38551004812227074
          },
          "Professional Law - Observed inference time (s)": {
            "description": "min=0.429, mean=0.429, max=0.429, sum=0.859 (2)",
            "tab": "Efficiency",
            "score": 0.4294954424886691
          },
          "Professional Psychology - Observed inference time (s)": {
            "description": "min=0.397, mean=0.397, max=0.397, sum=0.793 (2)",
            "tab": "Efficiency",
            "score": 0.39653347715053683
          },
          "Professional Medicine - # eval": {
            "description": "min=272, mean=272, max=272, sum=544 (2)",
            "tab": "General information",
            "score": 272.0
          },
          "Professional Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Medicine - # prompt tokens": {
            "description": "min=1113.092, mean=1113.092, max=1113.092, sum=2226.184 (2)",
            "tab": "General information",
            "score": 1113.0919117647059
          },
          "Professional Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # eval": {
            "description": "min=282, mean=282, max=282, sum=564 (2)",
            "tab": "General information",
            "score": 282.0
          },
          "Professional Accounting - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Accounting - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # prompt tokens": {
            "description": "min=755.418, mean=755.418, max=755.418, sum=1510.837 (2)",
            "tab": "General information",
            "score": 755.418439716312
          },
          "Professional Accounting - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # eval": {
            "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
            "tab": "General information",
            "score": 1534.0
          },
          "Professional Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # prompt tokens": {
            "description": "min=1685.119, mean=1685.119, max=1685.119, sum=3370.239 (2)",
            "tab": "General information",
            "score": 1685.119295958279
          },
          "Professional Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # eval": {
            "description": "min=612, mean=612, max=612, sum=1224 (2)",
            "tab": "General information",
            "score": 612.0
          },
          "Professional Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # prompt tokens": {
            "description": "min=594.363, mean=594.363, max=594.363, sum=1188.725 (2)",
            "tab": "General information",
            "score": 594.3627450980392
          },
          "Professional Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "professional_psychology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_professional_psychology"
        }
      }
    },
    {
      "evaluation_name": "Us Foreign Policy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Us Foreign Policy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.78,
        "details": {
          "description": "min=0.78, mean=0.78, max=0.78, sum=1.56 (2)",
          "tab": "Accuracy",
          "Us Foreign Policy - Observed inference time (s)": {
            "description": "min=0.414, mean=0.414, max=0.414, sum=0.829 (2)",
            "tab": "Efficiency",
            "score": 0.4144425654411316
          },
          "Us Foreign Policy - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Us Foreign Policy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Us Foreign Policy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Us Foreign Policy - # prompt tokens": {
            "description": "min=438.2, mean=438.2, max=438.2, sum=876.4 (2)",
            "tab": "General information",
            "score": 438.2
          },
          "Us Foreign Policy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "us_foreign_policy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_us_foreign_policy"
        }
      }
    },
    {
      "evaluation_name": "Astronomy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Astronomy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.928,
        "details": {
          "description": "min=0.928, mean=0.928, max=0.928, sum=1.855 (2)",
          "tab": "Accuracy",
          "Astronomy - Observed inference time (s)": {
            "description": "min=0.432, mean=0.432, max=0.432, sum=0.864 (2)",
            "tab": "Efficiency",
            "score": 0.43207096739819173
          },
          "Astronomy - # eval": {
            "description": "min=152, mean=152, max=152, sum=304 (2)",
            "tab": "General information",
            "score": 152.0
          },
          "Astronomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Astronomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Astronomy - # prompt tokens": {
            "description": "min=602.421, mean=602.421, max=602.421, sum=1204.842 (2)",
            "tab": "General information",
            "score": 602.421052631579
          },
          "Astronomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "astronomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_astronomy"
        }
      }
    },
    {
      "evaluation_name": "Business Ethics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Business Ethics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.73,
        "details": {
          "description": "min=0.73, mean=0.73, max=0.73, sum=1.46 (2)",
          "tab": "Accuracy",
          "Business Ethics - Observed inference time (s)": {
            "description": "min=0.441, mean=0.441, max=0.441, sum=0.883 (2)",
            "tab": "Efficiency",
            "score": 0.441267569065094
          },
          "Business Ethics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Business Ethics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Business Ethics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Business Ethics - # prompt tokens": {
            "description": "min=552.87, mean=552.87, max=552.87, sum=1105.74 (2)",
            "tab": "General information",
            "score": 552.87
          },
          "Business Ethics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "business_ethics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_business_ethics"
        }
      }
    },
    {
      "evaluation_name": "Clinical Knowledge",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Clinical Knowledge",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.879,
        "details": {
          "description": "min=0.879, mean=0.879, max=0.879, sum=1.758 (2)",
          "tab": "Accuracy",
          "Clinical Knowledge - Observed inference time (s)": {
            "description": "min=0.439, mean=0.439, max=0.439, sum=0.878 (2)",
            "tab": "Efficiency",
            "score": 0.43878708245619286
          },
          "Clinical Knowledge - # eval": {
            "description": "min=265, mean=265, max=265, sum=530 (2)",
            "tab": "General information",
            "score": 265.0
          },
          "Clinical Knowledge - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Clinical Knowledge - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Clinical Knowledge - # prompt tokens": {
            "description": "min=402.592, mean=402.592, max=402.592, sum=805.185 (2)",
            "tab": "General information",
            "score": 402.5924528301887
          },
          "Clinical Knowledge - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "clinical_knowledge",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_clinical_knowledge"
        }
      }
    },
    {
      "evaluation_name": "Conceptual Physics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Conceptual Physics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.813,
        "details": {
          "description": "min=0.813, mean=0.813, max=0.813, sum=1.626 (2)",
          "tab": "Accuracy",
          "Conceptual Physics - Observed inference time (s)": {
            "description": "min=0.398, mean=0.398, max=0.398, sum=0.796 (2)",
            "tab": "Efficiency",
            "score": 0.3981509147806371
          },
          "Conceptual Physics - # eval": {
            "description": "min=235, mean=235, max=235, sum=470 (2)",
            "tab": "General information",
            "score": 235.0
          },
          "Conceptual Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Conceptual Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Conceptual Physics - # prompt tokens": {
            "description": "min=309.213, mean=309.213, max=309.213, sum=618.426 (2)",
            "tab": "General information",
            "score": 309.21276595744683
          },
          "Conceptual Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "conceptual_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_conceptual_physics"
        }
      }
    },
    {
      "evaluation_name": "Electrical Engineering",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Electrical Engineering",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.834,
        "details": {
          "description": "min=0.834, mean=0.834, max=0.834, sum=1.669 (2)",
          "tab": "Accuracy",
          "Electrical Engineering - Observed inference time (s)": {
            "description": "min=0.476, mean=0.476, max=0.476, sum=0.952 (2)",
            "tab": "Efficiency",
            "score": 0.47606519830637967
          },
          "Electrical Engineering - # eval": {
            "description": "min=145, mean=145, max=145, sum=290 (2)",
            "tab": "General information",
            "score": 145.0
          },
          "Electrical Engineering - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Electrical Engineering - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Electrical Engineering - # prompt tokens": {
            "description": "min=474.786, mean=474.786, max=474.786, sum=949.572 (2)",
            "tab": "General information",
            "score": 474.78620689655173
          },
          "Electrical Engineering - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "electrical_engineering",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_electrical_engineering"
        }
      }
    },
    {
      "evaluation_name": "Elementary Mathematics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Elementary Mathematics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.857,
        "details": {
          "description": "min=0.857, mean=0.857, max=0.857, sum=1.714 (2)",
          "tab": "Accuracy",
          "Elementary Mathematics - Observed inference time (s)": {
            "description": "min=0.408, mean=0.408, max=0.408, sum=0.816 (2)",
            "tab": "Efficiency",
            "score": 0.4077642039647178
          },
          "Elementary Mathematics - # eval": {
            "description": "min=378, mean=378, max=378, sum=756 (2)",
            "tab": "General information",
            "score": 378.0
          },
          "Elementary Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Elementary Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Elementary Mathematics - # prompt tokens": {
            "description": "min=597.341, mean=597.341, max=597.341, sum=1194.683 (2)",
            "tab": "General information",
            "score": 597.3412698412699
          },
          "Elementary Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "elementary_mathematics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_elementary_mathematics"
        }
      }
    },
    {
      "evaluation_name": "Formal Logic",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Formal Logic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.571,
        "details": {
          "description": "min=0.571, mean=0.571, max=0.571, sum=1.143 (2)",
          "tab": "Accuracy",
          "Formal Logic - Observed inference time (s)": {
            "description": "min=0.402, mean=0.402, max=0.402, sum=0.804 (2)",
            "tab": "Efficiency",
            "score": 0.4018626610438029
          },
          "Formal Logic - # eval": {
            "description": "min=126, mean=126, max=126, sum=252 (2)",
            "tab": "General information",
            "score": 126.0
          },
          "Formal Logic - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Formal Logic - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Formal Logic - # prompt tokens": {
            "description": "min=619.563, mean=619.563, max=619.563, sum=1239.127 (2)",
            "tab": "General information",
            "score": 619.563492063492
          },
          "Formal Logic - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "formal_logic",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_formal_logic"
        }
      }
    },
    {
      "evaluation_name": "High School World History",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on High School World History",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.743,
        "details": {
          "description": "min=0.743, mean=0.743, max=0.743, sum=1.485 (2)",
          "tab": "Accuracy",
          "High School Biology - Observed inference time (s)": {
            "description": "min=0.392, mean=0.392, max=0.392, sum=0.784 (2)",
            "tab": "Efficiency",
            "score": 0.39193403643946495
          },
          "High School Chemistry - Observed inference time (s)": {
            "description": "min=0.391, mean=0.391, max=0.391, sum=0.783 (2)",
            "tab": "Efficiency",
            "score": 0.3914114583302014
          },
          "High School Computer Science - Observed inference time (s)": {
            "description": "min=0.392, mean=0.392, max=0.392, sum=0.785 (2)",
            "tab": "Efficiency",
            "score": 0.3924300479888916
          },
          "High School European History - Observed inference time (s)": {
            "description": "min=0.452, mean=0.452, max=0.452, sum=0.903 (2)",
            "tab": "Efficiency",
            "score": 0.451710438005852
          },
          "High School Geography - Observed inference time (s)": {
            "description": "min=0.386, mean=0.386, max=0.386, sum=0.773 (2)",
            "tab": "Efficiency",
            "score": 0.3862521937399199
          },
          "High School Government And Politics - Observed inference time (s)": {
            "description": "min=0.409, mean=0.409, max=0.409, sum=0.817 (2)",
            "tab": "Efficiency",
            "score": 0.40865302950607063
          },
          "High School Macroeconomics - Observed inference time (s)": {
            "description": "min=0.385, mean=0.385, max=0.385, sum=0.771 (2)",
            "tab": "Efficiency",
            "score": 0.3853575364137307
          },
          "High School Mathematics - Observed inference time (s)": {
            "description": "min=0.393, mean=0.393, max=0.393, sum=0.787 (2)",
            "tab": "Efficiency",
            "score": 0.39334204550142643
          },
          "High School Microeconomics - Observed inference time (s)": {
            "description": "min=0.384, mean=0.384, max=0.384, sum=0.768 (2)",
            "tab": "Efficiency",
            "score": 0.38397373171413646
          },
          "High School Physics - Observed inference time (s)": {
            "description": "min=0.412, mean=0.412, max=0.412, sum=0.823 (2)",
            "tab": "Efficiency",
            "score": 0.4116018955281239
          },
          "High School Psychology - Observed inference time (s)": {
            "description": "min=0.393, mean=0.393, max=0.393, sum=0.786 (2)",
            "tab": "Efficiency",
            "score": 0.3931623751964044
          },
          "High School Statistics - Observed inference time (s)": {
            "description": "min=0.449, mean=0.449, max=0.449, sum=0.898 (2)",
            "tab": "Efficiency",
            "score": 0.44901008628032824
          },
          "High School US History - Observed inference time (s)": {
            "description": "min=0.468, mean=0.468, max=0.468, sum=0.935 (2)",
            "tab": "Efficiency",
            "score": 0.46768493044610115
          },
          "High School World History - Observed inference time (s)": {
            "description": "min=0.452, mean=0.452, max=0.452, sum=0.903 (2)",
            "tab": "Efficiency",
            "score": 0.451718654310653
          },
          "High School Biology - # eval": {
            "description": "min=310, mean=310, max=310, sum=620 (2)",
            "tab": "General information",
            "score": 310.0
          },
          "High School Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Biology - # prompt tokens": {
            "description": "min=500.958, mean=500.958, max=500.958, sum=1001.916 (2)",
            "tab": "General information",
            "score": 500.958064516129
          },
          "High School Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # eval": {
            "description": "min=203, mean=203, max=203, sum=406 (2)",
            "tab": "General information",
            "score": 203.0
          },
          "High School Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # prompt tokens": {
            "description": "min=513.064, mean=513.064, max=513.064, sum=1026.128 (2)",
            "tab": "General information",
            "score": 513.064039408867
          },
          "High School Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "High School Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # prompt tokens": {
            "description": "min=935.13, mean=935.13, max=935.13, sum=1870.26 (2)",
            "tab": "General information",
            "score": 935.13
          },
          "High School Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # eval": {
            "description": "min=165, mean=165, max=165, sum=330 (2)",
            "tab": "General information",
            "score": 165.0
          },
          "High School European History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School European History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # prompt tokens": {
            "description": "min=2797.424, mean=2797.424, max=2797.424, sum=5594.848 (2)",
            "tab": "General information",
            "score": 2797.4242424242425
          },
          "High School European History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # eval": {
            "description": "min=198, mean=198, max=198, sum=396 (2)",
            "tab": "General information",
            "score": 198.0
          },
          "High School Geography - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Geography - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # prompt tokens": {
            "description": "min=394.773, mean=394.773, max=394.773, sum=789.545 (2)",
            "tab": "General information",
            "score": 394.77272727272725
          },
          "High School Geography - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # eval": {
            "description": "min=193, mean=193, max=193, sum=386 (2)",
            "tab": "General information",
            "score": 193.0
          },
          "High School Government And Politics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Government And Politics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # prompt tokens": {
            "description": "min=479.301, mean=479.301, max=479.301, sum=958.601 (2)",
            "tab": "General information",
            "score": 479.30051813471505
          },
          "High School Government And Politics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # eval": {
            "description": "min=390, mean=390, max=390, sum=780 (2)",
            "tab": "General information",
            "score": 390.0
          },
          "High School Macroeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Macroeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # prompt tokens": {
            "description": "min=396.541, mean=396.541, max=396.541, sum=793.082 (2)",
            "tab": "General information",
            "score": 396.54102564102567
          },
          "High School Macroeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # eval": {
            "description": "min=270, mean=270, max=270, sum=540 (2)",
            "tab": "General information",
            "score": 270.0
          },
          "High School Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # prompt tokens": {
            "description": "min=566.822, mean=566.822, max=566.822, sum=1133.644 (2)",
            "tab": "General information",
            "score": 566.8222222222222
          },
          "High School Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # eval": {
            "description": "min=238, mean=238, max=238, sum=476 (2)",
            "tab": "General information",
            "score": 238.0
          },
          "High School Microeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Microeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # prompt tokens": {
            "description": "min=415.954, mean=415.954, max=415.954, sum=831.908 (2)",
            "tab": "General information",
            "score": 415.953781512605
          },
          "High School Microeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # eval": {
            "description": "min=151, mean=151, max=151, sum=302 (2)",
            "tab": "General information",
            "score": 151.0
          },
          "High School Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # prompt tokens": {
            "description": "min=591.715, mean=591.715, max=591.715, sum=1183.43 (2)",
            "tab": "General information",
            "score": 591.7152317880794
          },
          "High School Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # eval": {
            "description": "min=545, mean=545, max=545, sum=1090 (2)",
            "tab": "General information",
            "score": 545.0
          },
          "High School Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # prompt tokens": {
            "description": "min=502.604, mean=502.604, max=502.604, sum=1005.207 (2)",
            "tab": "General information",
            "score": 502.60366972477067
          },
          "High School Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # eval": {
            "description": "min=216, mean=216, max=216, sum=432 (2)",
            "tab": "General information",
            "score": 216.0
          },
          "High School Statistics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Statistics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # prompt tokens": {
            "description": "min=858.931, mean=858.931, max=858.931, sum=1717.861 (2)",
            "tab": "General information",
            "score": 858.9305555555555
          },
          "High School Statistics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # eval": {
            "description": "min=204, mean=204, max=204, sum=408 (2)",
            "tab": "General information",
            "score": 204.0
          },
          "High School US History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School US History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # prompt tokens": {
            "description": "min=2205.583, mean=2205.583, max=2205.583, sum=4411.167 (2)",
            "tab": "General information",
            "score": 2205.5833333333335
          },
          "High School US History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # eval": {
            "description": "min=237, mean=237, max=237, sum=474 (2)",
            "tab": "General information",
            "score": 237.0
          },
          "High School World History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School World History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # prompt tokens": {
            "description": "min=1426.544, mean=1426.544, max=1426.544, sum=2853.089 (2)",
            "tab": "General information",
            "score": 1426.5443037974683
          },
          "High School World History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "high_school_world_history",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_high_school_world_history"
        }
      }
    },
    {
      "evaluation_name": "Human Sexuality",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Human Sexuality",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.901,
        "details": {
          "description": "min=0.901, mean=0.901, max=0.901, sum=1.802 (2)",
          "tab": "Accuracy",
          "Human Aging - Observed inference time (s)": {
            "description": "min=0.4, mean=0.4, max=0.4, sum=0.8 (2)",
            "tab": "Efficiency",
            "score": 0.3999073441253115
          },
          "Human Sexuality - Observed inference time (s)": {
            "description": "min=0.42, mean=0.42, max=0.42, sum=0.841 (2)",
            "tab": "Efficiency",
            "score": 0.4203109868610178
          },
          "Human Aging - # eval": {
            "description": "min=223, mean=223, max=223, sum=446 (2)",
            "tab": "General information",
            "score": 223.0
          },
          "Human Aging - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Aging - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Aging - # prompt tokens": {
            "description": "min=321.587, mean=321.587, max=321.587, sum=643.175 (2)",
            "tab": "General information",
            "score": 321.58744394618833
          },
          "Human Aging - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # eval": {
            "description": "min=131, mean=131, max=131, sum=262 (2)",
            "tab": "General information",
            "score": 131.0
          },
          "Human Sexuality - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Sexuality - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # prompt tokens": {
            "description": "min=347.183, mean=347.183, max=347.183, sum=694.366 (2)",
            "tab": "General information",
            "score": 347.1832061068702
          },
          "Human Sexuality - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "human_sexuality",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_human_sexuality"
        }
      }
    },
    {
      "evaluation_name": "International Law",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on International Law",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.645,
        "details": {
          "description": "min=0.645, mean=0.645, max=0.645, sum=1.289 (2)",
          "tab": "Accuracy",
          "International Law - Observed inference time (s)": {
            "description": "min=0.457, mean=0.457, max=0.457, sum=0.913 (2)",
            "tab": "Efficiency",
            "score": 0.45661053972795973
          },
          "International Law - # eval": {
            "description": "min=121, mean=121, max=121, sum=242 (2)",
            "tab": "General information",
            "score": 121.0
          },
          "International Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "International Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "International Law - # prompt tokens": {
            "description": "min=644.165, mean=644.165, max=644.165, sum=1288.331 (2)",
            "tab": "General information",
            "score": 644.1652892561983
          },
          "International Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "international_law",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_international_law"
        }
      }
    },
    {
      "evaluation_name": "Logical Fallacies",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Logical Fallacies",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.914,
        "details": {
          "description": "min=0.914, mean=0.914, max=0.914, sum=1.828 (2)",
          "tab": "Accuracy",
          "Logical Fallacies - Observed inference time (s)": {
            "description": "min=0.411, mean=0.411, max=0.411, sum=0.823 (2)",
            "tab": "Efficiency",
            "score": 0.4113436125538832
          },
          "Logical Fallacies - # eval": {
            "description": "min=163, mean=163, max=163, sum=326 (2)",
            "tab": "General information",
            "score": 163.0
          },
          "Logical Fallacies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Logical Fallacies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Logical Fallacies - # prompt tokens": {
            "description": "min=450.049, mean=450.049, max=450.049, sum=900.098 (2)",
            "tab": "General information",
            "score": 450.0490797546012
          },
          "Logical Fallacies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "logical_fallacies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_logical_fallacies"
        }
      }
    },
    {
      "evaluation_name": "Machine Learning",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Machine Learning",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.759,
        "details": {
          "description": "min=0.759, mean=0.759, max=0.759, sum=1.518 (2)",
          "tab": "Accuracy",
          "Machine Learning - Observed inference time (s)": {
            "description": "min=0.417, mean=0.417, max=0.417, sum=0.833 (2)",
            "tab": "Efficiency",
            "score": 0.4165512855563845
          },
          "Machine Learning - # eval": {
            "description": "min=112, mean=112, max=112, sum=224 (2)",
            "tab": "General information",
            "score": 112.0
          },
          "Machine Learning - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Machine Learning - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Machine Learning - # prompt tokens": {
            "description": "min=702.402, mean=702.402, max=702.402, sum=1404.804 (2)",
            "tab": "General information",
            "score": 702.4017857142857
          },
          "Machine Learning - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "machine_learning",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_machine_learning"
        }
      }
    },
    {
      "evaluation_name": "Management",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Management",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.718,
        "details": {
          "description": "min=0.718, mean=0.718, max=0.718, sum=1.437 (2)",
          "tab": "Accuracy",
          "Management - Observed inference time (s)": {
            "description": "min=0.401, mean=0.401, max=0.401, sum=0.803 (2)",
            "tab": "Efficiency",
            "score": 0.4013508292077814
          },
          "Management - # eval": {
            "description": "min=103, mean=103, max=103, sum=206 (2)",
            "tab": "General information",
            "score": 103.0
          },
          "Management - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Management - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Management - # prompt tokens": {
            "description": "min=281.301, mean=281.301, max=281.301, sum=562.602 (2)",
            "tab": "General information",
            "score": 281.3009708737864
          },
          "Management - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "management",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_management"
        }
      }
    },
    {
      "evaluation_name": "Marketing",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Marketing",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.944,
        "details": {
          "description": "min=0.944, mean=0.944, max=0.944, sum=1.889 (2)",
          "tab": "Accuracy",
          "Marketing - Observed inference time (s)": {
            "description": "min=0.401, mean=0.401, max=0.401, sum=0.801 (2)",
            "tab": "Efficiency",
            "score": 0.4005699891310472
          },
          "Marketing - # eval": {
            "description": "min=234, mean=234, max=234, sum=468 (2)",
            "tab": "General information",
            "score": 234.0
          },
          "Marketing - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Marketing - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Marketing - # prompt tokens": {
            "description": "min=428.35, mean=428.35, max=428.35, sum=856.701 (2)",
            "tab": "General information",
            "score": 428.35042735042737
          },
          "Marketing - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "marketing",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_marketing"
        }
      }
    },
    {
      "evaluation_name": "Medical Genetics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Medical Genetics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.89,
        "details": {
          "description": "min=0.89, mean=0.89, max=0.89, sum=1.78 (2)",
          "tab": "Accuracy",
          "Medical Genetics - Observed inference time (s)": {
            "description": "min=0.387, mean=0.387, max=0.387, sum=0.773 (2)",
            "tab": "Efficiency",
            "score": 0.38653050899505614
          },
          "Medical Genetics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Medical Genetics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Medical Genetics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Medical Genetics - # prompt tokens": {
            "description": "min=338.89, mean=338.89, max=338.89, sum=677.78 (2)",
            "tab": "General information",
            "score": 338.89
          },
          "Medical Genetics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "medical_genetics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_medical_genetics"
        }
      }
    },
    {
      "evaluation_name": "Miscellaneous",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Miscellaneous",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.939,
        "details": {
          "description": "min=0.939, mean=0.939, max=0.939, sum=1.877 (2)",
          "tab": "Accuracy",
          "Miscellaneous - Observed inference time (s)": {
            "description": "min=0.386, mean=0.386, max=0.386, sum=0.772 (2)",
            "tab": "Efficiency",
            "score": 0.3861832460376647
          },
          "Miscellaneous - # eval": {
            "description": "min=783, mean=783, max=783, sum=1566 (2)",
            "tab": "General information",
            "score": 783.0
          },
          "Miscellaneous - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Miscellaneous - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Miscellaneous - # prompt tokens": {
            "description": "min=314.669, mean=314.669, max=314.669, sum=629.338 (2)",
            "tab": "General information",
            "score": 314.669220945083
          },
          "Miscellaneous - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "miscellaneous",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_miscellaneous"
        }
      }
    },
    {
      "evaluation_name": "Moral Scenarios",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Moral Scenarios",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.815,
        "details": {
          "description": "min=0.815, mean=0.815, max=0.815, sum=1.629 (2)",
          "tab": "Accuracy",
          "Moral Disputes - Observed inference time (s)": {
            "description": "min=0.384, mean=0.384, max=0.384, sum=0.768 (2)",
            "tab": "Efficiency",
            "score": 0.3839988109004291
          },
          "Moral Scenarios - Observed inference time (s)": {
            "description": "min=0.405, mean=0.405, max=0.405, sum=0.81 (2)",
            "tab": "Efficiency",
            "score": 0.4048716662316349
          },
          "Moral Disputes - # eval": {
            "description": "min=346, mean=346, max=346, sum=692 (2)",
            "tab": "General information",
            "score": 346.0
          },
          "Moral Disputes - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Disputes - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Disputes - # prompt tokens": {
            "description": "min=495.003, mean=495.003, max=495.003, sum=990.006 (2)",
            "tab": "General information",
            "score": 495.0028901734104
          },
          "Moral Disputes - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # eval": {
            "description": "min=895, mean=895, max=895, sum=1790 (2)",
            "tab": "General information",
            "score": 895.0
          },
          "Moral Scenarios - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Scenarios - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # prompt tokens": {
            "description": "min=690.542, mean=690.542, max=690.542, sum=1381.084 (2)",
            "tab": "General information",
            "score": 690.5418994413408
          },
          "Moral Scenarios - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "moral_scenarios",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_moral_scenarios"
        }
      }
    },
    {
      "evaluation_name": "Nutrition",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Nutrition",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.856,
        "details": {
          "description": "min=0.856, mean=0.856, max=0.856, sum=1.712 (2)",
          "tab": "Accuracy",
          "Nutrition - Observed inference time (s)": {
            "description": "min=0.397, mean=0.397, max=0.397, sum=0.794 (2)",
            "tab": "Efficiency",
            "score": 0.39706431027331385
          },
          "Nutrition - # eval": {
            "description": "min=306, mean=306, max=306, sum=612 (2)",
            "tab": "General information",
            "score": 306.0
          },
          "Nutrition - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Nutrition - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Nutrition - # prompt tokens": {
            "description": "min=585.48, mean=585.48, max=585.48, sum=1170.961 (2)",
            "tab": "General information",
            "score": 585.4803921568628
          },
          "Nutrition - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "nutrition",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_nutrition"
        }
      }
    },
    {
      "evaluation_name": "Prehistory",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Prehistory",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.898,
        "details": {
          "description": "min=0.898, mean=0.898, max=0.898, sum=1.796 (2)",
          "tab": "Accuracy",
          "Prehistory - Observed inference time (s)": {
            "description": "min=0.39, mean=0.39, max=0.39, sum=0.78 (2)",
            "tab": "Efficiency",
            "score": 0.3900022072556578
          },
          "Prehistory - # eval": {
            "description": "min=324, mean=324, max=324, sum=648 (2)",
            "tab": "General information",
            "score": 324.0
          },
          "Prehistory - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Prehistory - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Prehistory - # prompt tokens": {
            "description": "min=540.198, mean=540.198, max=540.198, sum=1080.395 (2)",
            "tab": "General information",
            "score": 540.1975308641976
          },
          "Prehistory - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "prehistory",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_prehistory"
        }
      }
    },
    {
      "evaluation_name": "Public Relations",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Public Relations",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.791,
        "details": {
          "description": "min=0.791, mean=0.791, max=0.791, sum=1.582 (2)",
          "tab": "Accuracy",
          "Public Relations - Observed inference time (s)": {
            "description": "min=0.38, mean=0.38, max=0.38, sum=0.76 (2)",
            "tab": "Efficiency",
            "score": 0.37999111955816095
          },
          "Public Relations - # eval": {
            "description": "min=110, mean=110, max=110, sum=220 (2)",
            "tab": "General information",
            "score": 110.0
          },
          "Public Relations - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Public Relations - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Public Relations - # prompt tokens": {
            "description": "min=426.655, mean=426.655, max=426.655, sum=853.309 (2)",
            "tab": "General information",
            "score": 426.6545454545454
          },
          "Public Relations - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "public_relations",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_public_relations"
        }
      }
    },
    {
      "evaluation_name": "Security Studies",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Security Studies",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.69,
        "details": {
          "description": "min=0.69, mean=0.69, max=0.69, sum=1.38 (2)",
          "tab": "Accuracy",
          "Security Studies - Observed inference time (s)": {
            "description": "min=0.394, mean=0.394, max=0.394, sum=0.787 (2)",
            "tab": "Efficiency",
            "score": 0.3936534463142862
          },
          "Security Studies - # eval": {
            "description": "min=245, mean=245, max=245, sum=490 (2)",
            "tab": "General information",
            "score": 245.0
          },
          "Security Studies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Security Studies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Security Studies - # prompt tokens": {
            "description": "min=1193.869, mean=1193.869, max=1193.869, sum=2387.739 (2)",
            "tab": "General information",
            "score": 1193.869387755102
          },
          "Security Studies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "security_studies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_security_studies"
        }
      }
    },
    {
      "evaluation_name": "Sociology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Sociology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.786,
        "details": {
          "description": "min=0.786, mean=0.786, max=0.786, sum=1.572 (2)",
          "tab": "Accuracy",
          "Sociology - Observed inference time (s)": {
            "description": "min=0.388, mean=0.388, max=0.388, sum=0.776 (2)",
            "tab": "Efficiency",
            "score": 0.3881402205471969
          },
          "Sociology - # eval": {
            "description": "min=201, mean=201, max=201, sum=402 (2)",
            "tab": "General information",
            "score": 201.0
          },
          "Sociology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Sociology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Sociology - # prompt tokens": {
            "description": "min=456.274, mean=456.274, max=456.274, sum=912.547 (2)",
            "tab": "General information",
            "score": 456.27363184079604
          },
          "Sociology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "sociology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_sociology"
        }
      }
    },
    {
      "evaluation_name": "Virology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Virology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.554,
        "details": {
          "description": "min=0.554, mean=0.554, max=0.554, sum=1.108 (2)",
          "tab": "Accuracy",
          "Virology - Observed inference time (s)": {
            "description": "min=0.379, mean=0.379, max=0.379, sum=0.758 (2)",
            "tab": "Efficiency",
            "score": 0.3791351461985025
          },
          "Virology - # eval": {
            "description": "min=166, mean=166, max=166, sum=332 (2)",
            "tab": "General information",
            "score": 166.0
          },
          "Virology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Virology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Virology - # prompt tokens": {
            "description": "min=336.753, mean=336.753, max=336.753, sum=673.506 (2)",
            "tab": "General information",
            "score": 336.7530120481928
          },
          "Virology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "virology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_virology"
        }
      }
    },
    {
      "evaluation_name": "World Religions",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on World Religions",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.731,
        "details": {
          "description": "min=0.731, mean=0.731, max=0.731, sum=1.462 (2)",
          "tab": "Accuracy",
          "World Religions - Observed inference time (s)": {
            "description": "min=0.384, mean=0.384, max=0.384, sum=0.768 (2)",
            "tab": "Efficiency",
            "score": 0.38400994964510377
          },
          "World Religions - # eval": {
            "description": "min=171, mean=171, max=171, sum=342 (2)",
            "tab": "General information",
            "score": 171.0
          },
          "World Religions - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "World Religions - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "World Religions - # prompt tokens": {
            "description": "min=268.164, mean=268.164, max=268.164, sum=536.327 (2)",
            "tab": "General information",
            "score": 268.1637426900585
          },
          "World Religions - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "world_religions",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_world_religions"
        }
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.567,
        "details": {
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}