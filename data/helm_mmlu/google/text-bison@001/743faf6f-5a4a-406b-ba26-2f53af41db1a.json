{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_mmlu/google_text-bison@001/1765639045.712842",
  "retrieved_timestamp": "1765639045.712842",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
  ],
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "PaLM-2 (Bison)",
    "id": "google/text-bison@001",
    "developer": "google",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.692,
        "details": {
          "description": "min=0.331, mean=0.692, max=0.927, sum=78.899 (114)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - EM",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.39,
        "details": {
          "description": "min=0.39, mean=0.39, max=0.39, sum=0.78 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - EM",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.644,
        "details": {
          "description": "min=0.644, mean=0.644, max=0.644, sum=1.289 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Chemistry - EM",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.52,
        "details": {
          "description": "min=0.52, mean=0.52, max=0.52, sum=1.04 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "Computer Security - EM",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.74,
        "details": {
          "description": "min=0.74, mean=0.74, max=0.74, sum=1.48 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - EM",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.518,
        "details": {
          "description": "min=0.518, mean=0.518, max=0.518, sum=1.035 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - EM",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.38,
        "details": {
          "description": "min=0.38, mean=0.38, max=0.38, sum=0.76 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - EM",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.769,
        "details": {
          "description": "min=0.769, mean=0.769, max=0.769, sum=1.537 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - EM",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.736,
        "details": {
          "description": "min=0.736, mean=0.736, max=0.736, sum=1.473 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Medicine - EM",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.728,
        "details": {
          "description": "min=0.728, mean=0.728, max=0.728, sum=1.456 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - EM",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.87,
        "details": {
          "description": "min=0.87, mean=0.87, max=0.87, sum=1.74 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - EM",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.803,
        "details": {
          "description": "min=0.803, mean=0.803, max=0.803, sum=1.605 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - EM",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.76,
        "details": {
          "description": "min=0.76, mean=0.76, max=0.76, sum=1.52 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - EM",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.725,
        "details": {
          "description": "min=0.725, mean=0.725, max=0.725, sum=1.449 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "College Biology - EM",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.826,
        "details": {
          "description": "min=0.826, mean=0.826, max=0.826, sum=1.653 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Computer Science - EM",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.51,
        "details": {
          "description": "min=0.51, mean=0.51, max=0.51, sum=1.02 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.35,
        "details": {
          "description": "min=0.35, mean=0.35, max=0.35, sum=0.7 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Medicine - EM",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.613,
        "details": {
          "description": "min=0.613, mean=0.613, max=0.613, sum=1.225 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Physics - EM",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.51,
        "details": {
          "description": "min=0.51, mean=0.51, max=0.51, sum=1.02 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - EM",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.694,
        "details": {
          "description": "min=0.694, mean=0.694, max=0.694, sum=1.387 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - EM",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.69,
        "details": {
          "description": "min=0.69, mean=0.69, max=0.69, sum=1.379 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.487,
        "details": {
          "description": "min=0.487, mean=0.487, max=0.487, sum=0.974 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - EM",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.5,
        "details": {
          "description": "min=0.5, mean=0.5, max=0.5, sum=1 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School Biology - EM",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.829,
        "details": {
          "description": "min=0.829, mean=0.829, max=0.829, sum=1.658 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Chemistry - EM",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.596,
        "details": {
          "description": "min=0.596, mean=0.596, max=0.596, sum=1.192 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Computer Science - EM",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.74,
        "details": {
          "description": "min=0.74, mean=0.74, max=0.74, sum=1.48 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School European History - EM",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.848,
        "details": {
          "description": "min=0.848, mean=0.848, max=0.848, sum=1.697 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School Geography - EM",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.848,
        "details": {
          "description": "min=0.848, mean=0.848, max=0.848, sum=1.697 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - EM",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.927,
        "details": {
          "description": "min=0.927, mean=0.927, max=0.927, sum=1.855 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - EM",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.692,
        "details": {
          "description": "min=0.692, mean=0.692, max=0.692, sum=1.385 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - EM",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.378,
        "details": {
          "description": "min=0.378, mean=0.378, max=0.378, sum=0.756 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - EM",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.803,
        "details": {
          "description": "min=0.803, mean=0.803, max=0.803, sum=1.605 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Physics - EM",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.331,
        "details": {
          "description": "min=0.331, mean=0.331, max=0.331, sum=0.662 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Psychology - EM",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.894,
        "details": {
          "description": "min=0.894, mean=0.894, max=0.894, sum=1.787 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Statistics - EM",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.588,
        "details": {
          "description": "min=0.588, mean=0.588, max=0.588, sum=1.176 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School US History - EM",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.877,
        "details": {
          "description": "min=0.877, mean=0.877, max=0.877, sum=1.755 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School World History - EM",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.869,
        "details": {
          "description": "min=0.869, mean=0.869, max=0.869, sum=1.738 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Aging - EM",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.691,
        "details": {
          "description": "min=0.691, mean=0.691, max=0.691, sum=1.381 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Sexuality - EM",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.84,
        "details": {
          "description": "min=0.84, mean=0.84, max=0.84, sum=1.679 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - EM",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.835,
        "details": {
          "description": "min=0.835, mean=0.835, max=0.835, sum=1.669 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - EM",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.853,
        "details": {
          "description": "min=0.853, mean=0.853, max=0.853, sum=1.706 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - EM",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.562,
        "details": {
          "description": "min=0.562, mean=0.562, max=0.562, sum=1.125 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - EM",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.893,
        "details": {
          "description": "min=0.893, mean=0.893, max=0.893, sum=1.786 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - EM",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.893,
        "details": {
          "description": "min=0.893, mean=0.893, max=0.893, sum=1.786 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - EM",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.75,
        "details": {
          "description": "min=0.75, mean=0.75, max=0.75, sum=1.5 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - EM",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.866,
        "details": {
          "description": "min=0.866, mean=0.866, max=0.866, sum=1.732 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Disputes - EM",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.766,
        "details": {
          "description": "min=0.766, mean=0.766, max=0.766, sum=1.532 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - EM",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.369,
        "details": {
          "description": "min=0.369, mean=0.369, max=0.369, sum=0.737 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - EM",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.709,
        "details": {
          "description": "min=0.709, mean=0.709, max=0.709, sum=1.418 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - EM",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.812,
        "details": {
          "description": "min=0.812, mean=0.812, max=0.812, sum=1.623 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Professional Accounting - EM",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.578,
        "details": {
          "description": "min=0.578, mean=0.578, max=0.578, sum=1.156 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Law - EM",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.489,
        "details": {
          "description": "min=0.489, mean=0.489, max=0.489, sum=0.978 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Psychology - EM",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.761,
        "details": {
          "description": "min=0.761, mean=0.761, max=0.761, sum=1.523 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Public Relations - EM",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.691,
        "details": {
          "description": "min=0.691, mean=0.691, max=0.691, sum=1.382 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - EM",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.812,
        "details": {
          "description": "min=0.812, mean=0.812, max=0.812, sum=1.624 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - EM",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.92,
        "details": {
          "description": "min=0.92, mean=0.92, max=0.92, sum=1.841 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - EM",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.494,
        "details": {
          "description": "min=0.494, mean=0.494, max=0.494, sum=0.988 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - EM",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.883,
        "details": {
          "description": "min=0.883, mean=0.883, max=0.883, sum=1.766 (2)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.192,
        "details": {
          "description": null,
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU All Subjects - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 1.845,
        "details": {
          "description": "min=0.619, mean=1.845, max=23.541, sum=210.314 (114)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 46.0
      },
      "score_details": {
        "score": 1.017,
        "details": {
          "description": "min=1.017, mean=1.017, max=1.017, sum=2.033 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 37.0
      },
      "score_details": {
        "score": 0.837,
        "details": {
          "description": "min=0.837, mean=0.837, max=0.837, sum=1.673 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Chemistry - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 1.352,
        "details": {
          "description": "min=1.352, mean=1.352, max=1.352, sum=2.704 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "Computer Security - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 1.044,
        "details": {
          "description": "min=1.044, mean=1.044, max=1.044, sum=2.088 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 1.047,
        "details": {
          "description": "min=1.047, mean=1.047, max=1.047, sum=2.094 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.913,
        "details": {
          "description": "min=0.913, mean=0.913, max=0.913, sum=1.826 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.884,
        "details": {
          "description": "min=0.884, mean=0.884, max=0.884, sum=1.768 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.716,
        "details": {
          "description": "min=0.716, mean=0.716, max=0.716, sum=1.432 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Medicine - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 10.0
      },
      "score_details": {
        "score": 8.281,
        "details": {
          "description": "min=8.281, mean=8.281, max=8.281, sum=16.562 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 1.101,
        "details": {
          "description": "min=1.101, mean=1.101, max=1.101, sum=2.202 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 34.0
      },
      "score_details": {
        "score": 0.715,
        "details": {
          "description": "min=0.715, mean=0.715, max=0.715, sum=1.43 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 35.0
      },
      "score_details": {
        "score": 0.893,
        "details": {
          "description": "min=0.893, mean=0.893, max=0.893, sum=1.785 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 7.0
      },
      "score_details": {
        "score": 0.77,
        "details": {
          "description": "min=0.77, mean=0.77, max=0.77, sum=1.541 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "College Biology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.862,
        "details": {
          "description": "min=0.862, mean=0.862, max=0.862, sum=1.724 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Computer Science - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 24.0
      },
      "score_details": {
        "score": 23.541,
        "details": {
          "description": "min=23.541, mean=23.541, max=23.541, sum=47.082 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Mathematics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.931,
        "details": {
          "description": "min=0.931, mean=0.931, max=0.931, sum=1.862 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Medicine - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.947,
        "details": {
          "description": "min=0.947, mean=0.947, max=0.947, sum=1.894 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Physics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 7.0
      },
      "score_details": {
        "score": 0.928,
        "details": {
          "description": "min=0.928, mean=0.928, max=0.928, sum=1.856 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.828,
        "details": {
          "description": "min=0.828, mean=0.828, max=0.828, sum=1.656 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 11.0
      },
      "score_details": {
        "score": 10.257,
        "details": {
          "description": "min=10.257, mean=10.257, max=10.257, sum=20.514 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.751,
        "details": {
          "description": "min=0.751, mean=0.751, max=0.751, sum=1.502 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.763,
        "details": {
          "description": "min=0.763, mean=0.763, max=0.763, sum=1.525 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School Biology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.789,
        "details": {
          "description": "min=0.789, mean=0.789, max=0.789, sum=1.577 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Chemistry - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.837,
        "details": {
          "description": "min=0.837, mean=0.837, max=0.837, sum=1.675 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Computer Science - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 7.0
      },
      "score_details": {
        "score": 0.961,
        "details": {
          "description": "min=0.961, mean=0.961, max=0.961, sum=1.922 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School European History - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 23.0
      },
      "score_details": {
        "score": 1.13,
        "details": {
          "description": "min=1.13, mean=1.13, max=1.13, sum=2.26 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School Geography - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.754,
        "details": {
          "description": "min=0.754, mean=0.754, max=0.754, sum=1.508 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.688,
        "details": {
          "description": "min=0.688, mean=0.688, max=0.688, sum=1.375 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.718,
        "details": {
          "description": "min=0.718, mean=0.718, max=0.718, sum=1.437 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.782,
        "details": {
          "description": "min=0.782, mean=0.782, max=0.782, sum=1.564 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.802,
        "details": {
          "description": "min=0.802, mean=0.802, max=0.802, sum=1.603 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Physics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.86,
        "details": {
          "description": "min=0.86, mean=0.86, max=0.86, sum=1.721 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Psychology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.675,
        "details": {
          "description": "min=0.675, mean=0.675, max=0.675, sum=1.35 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Statistics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 10.0
      },
      "score_details": {
        "score": 9.407,
        "details": {
          "description": "min=9.407, mean=9.407, max=9.407, sum=18.814 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School US History - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 18.0
      },
      "score_details": {
        "score": 1.054,
        "details": {
          "description": "min=1.054, mean=1.054, max=1.054, sum=2.109 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School World History - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 12.0
      },
      "score_details": {
        "score": 0.848,
        "details": {
          "description": "min=0.848, mean=0.848, max=0.848, sum=1.695 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Aging - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.619,
        "details": {
          "description": "min=0.619, mean=0.619, max=0.619, sum=1.237 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Sexuality - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.851,
        "details": {
          "description": "min=0.851, mean=0.851, max=0.851, sum=1.702 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 0.93,
        "details": {
          "description": "min=0.93, mean=0.93, max=0.93, sum=1.859 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.813,
        "details": {
          "description": "min=0.813, mean=0.813, max=0.813, sum=1.627 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 0.916,
        "details": {
          "description": "min=0.916, mean=0.916, max=0.916, sum=1.832 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.817,
        "details": {
          "description": "min=0.817, mean=0.817, max=0.817, sum=1.633 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.789,
        "details": {
          "description": "min=0.789, mean=0.789, max=0.789, sum=1.579 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.857,
        "details": {
          "description": "min=0.857, mean=0.857, max=0.857, sum=1.713 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 2.759,
        "details": {
          "description": "min=2.759, mean=2.759, max=2.759, sum=5.518 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Disputes - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.749,
        "details": {
          "description": "min=0.749, mean=0.749, max=0.749, sum=1.497 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 0.781,
        "details": {
          "description": "min=0.781, mean=0.781, max=0.781, sum=1.561 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.81,
        "details": {
          "description": "min=0.81, mean=0.81, max=0.81, sum=1.621 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.7,
        "details": {
          "description": "min=0.7, mean=0.7, max=0.7, sum=1.399 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Professional Accounting - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 6.0
      },
      "score_details": {
        "score": 0.812,
        "details": {
          "description": "min=0.812, mean=0.812, max=0.812, sum=1.624 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Law - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 14.0
      },
      "score_details": {
        "score": 0.634,
        "details": {
          "description": "min=0.634, mean=0.634, max=0.634, sum=1.268 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Psychology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.697,
        "details": {
          "description": "min=0.697, mean=0.697, max=0.697, sum=1.394 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Public Relations - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.98,
        "details": {
          "description": "min=0.98, mean=0.98, max=0.98, sum=1.961 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 10.0
      },
      "score_details": {
        "score": 0.857,
        "details": {
          "description": "min=0.857, mean=0.857, max=0.857, sum=1.713 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 8.0
      },
      "score_details": {
        "score": 7.515,
        "details": {
          "description": "min=7.515, mean=7.515, max=7.515, sum=15.029 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4.0
      },
      "score_details": {
        "score": 0.788,
        "details": {
          "description": "min=0.788, mean=0.788, max=0.788, sum=1.577 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 0.802,
        "details": {
          "description": "min=0.802, mean=0.802, max=0.802, sum=1.604 (2)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # eval",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 247.0
      },
      "score_details": {
        "score": 246.351,
        "details": {
          "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # train",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=570 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - truncated",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 716.0
      },
      "score_details": {
        "score": 635.61,
        "details": {
          "description": "min=270.187, mean=635.61, max=2823.23, sum=72459.527 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "MMLU All Subjects - # output tokens",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=114 (114)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "anatomy",
          "astronomy",
          "business_ethics",
          "clinical_knowledge",
          "college_biology",
          "college_chemistry",
          "college_computer_science",
          "college_mathematics",
          "college_medicine",
          "college_physics",
          "computer_security",
          "conceptual_physics",
          "econometrics",
          "electrical_engineering",
          "elementary_mathematics",
          "formal_logic",
          "global_facts",
          "high_school_biology",
          "high_school_chemistry",
          "high_school_computer_science",
          "high_school_european_history",
          "high_school_geography",
          "high_school_government_and_politics",
          "high_school_macroeconomics",
          "high_school_mathematics",
          "high_school_microeconomics",
          "high_school_physics",
          "high_school_psychology",
          "high_school_statistics",
          "high_school_us_history",
          "high_school_world_history",
          "human_aging",
          "human_sexuality",
          "international_law",
          "jurisprudence",
          "logical_fallacies",
          "machine_learning",
          "management",
          "marketing",
          "medical_genetics",
          "miscellaneous",
          "moral_disputes",
          "moral_scenarios",
          "nutrition",
          "philosophy",
          "prehistory",
          "professional_accounting",
          "professional_law",
          "professional_medicine",
          "professional_psychology",
          "public_relations",
          "security_studies",
          "sociology",
          "us_foreign_policy",
          "virology",
          "world_religions"
        ],
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": [
          "mmlu_abstract_algebra",
          "mmlu_anatomy",
          "mmlu_astronomy",
          "mmlu_business_ethics",
          "mmlu_clinical_knowledge",
          "mmlu_college_biology",
          "mmlu_college_chemistry",
          "mmlu_college_computer_science",
          "mmlu_college_mathematics",
          "mmlu_college_medicine",
          "mmlu_college_physics",
          "mmlu_computer_security",
          "mmlu_conceptual_physics",
          "mmlu_econometrics",
          "mmlu_electrical_engineering",
          "mmlu_elementary_mathematics",
          "mmlu_formal_logic",
          "mmlu_global_facts",
          "mmlu_high_school_biology",
          "mmlu_high_school_chemistry",
          "mmlu_high_school_computer_science",
          "mmlu_high_school_european_history",
          "mmlu_high_school_geography",
          "mmlu_high_school_government_and_politics",
          "mmlu_high_school_macroeconomics",
          "mmlu_high_school_mathematics",
          "mmlu_high_school_microeconomics",
          "mmlu_high_school_physics",
          "mmlu_high_school_psychology",
          "mmlu_high_school_statistics",
          "mmlu_high_school_us_history",
          "mmlu_high_school_world_history",
          "mmlu_human_aging",
          "mmlu_human_sexuality",
          "mmlu_international_law",
          "mmlu_jurisprudence",
          "mmlu_logical_fallacies",
          "mmlu_machine_learning",
          "mmlu_management",
          "mmlu_marketing",
          "mmlu_medical_genetics",
          "mmlu_miscellaneous",
          "mmlu_moral_disputes",
          "mmlu_moral_scenarios",
          "mmlu_nutrition",
          "mmlu_philosophy",
          "mmlu_prehistory",
          "mmlu_professional_accounting",
          "mmlu_professional_law",
          "mmlu_professional_medicine",
          "mmlu_professional_psychology",
          "mmlu_public_relations",
          "mmlu_security_studies",
          "mmlu_sociology",
          "mmlu_us_foreign_policy",
          "mmlu_virology",
          "mmlu_world_religions"
        ]
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # eval",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # train",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - truncated",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 436.0
      },
      "score_details": {
        "score": 387.12,
        "details": {
          "description": "min=387.12, mean=387.12, max=387.12, sum=774.24 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Abstract Algebra - # output tokens",
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "abstract_algebra",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_abstract_algebra"
      }
    },
    {
      "evaluation_name": "Anatomy - # eval",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 135.0
      },
      "score_details": {
        "score": 135.0,
        "details": {
          "description": "min=135, mean=135, max=135, sum=270 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - # train",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - truncated",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 436.0
      },
      "score_details": {
        "score": 344.089,
        "details": {
          "description": "min=344.089, mean=344.089, max=344.089, sum=688.178 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "Anatomy - # output tokens",
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "anatomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_anatomy"
      }
    },
    {
      "evaluation_name": "College Chemistry - # eval",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - # train",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - truncated",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 638.0
      },
      "score_details": {
        "score": 573.7,
        "details": {
          "description": "min=573.7, mean=573.7, max=573.7, sum=1147.4 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "College Chemistry - # output tokens",
      "metric_config": {
        "evaluation_description": "The college chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_chemistry"
      }
    },
    {
      "evaluation_name": "Computer Security - # eval",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - # train",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - truncated",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 464.0
      },
      "score_details": {
        "score": 384.24,
        "details": {
          "description": "min=384.24, mean=384.24, max=384.24, sum=768.48 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Computer Security - # output tokens",
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "computer_security",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_computer_security"
      }
    },
    {
      "evaluation_name": "Econometrics - # eval",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 114.0
      },
      "score_details": {
        "score": 114.0,
        "details": {
          "description": "min=114, mean=114, max=114, sum=228 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - # train",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - truncated",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 698.0
      },
      "score_details": {
        "score": 644.395,
        "details": {
          "description": "min=644.395, mean=644.395, max=644.395, sum=1288.789 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Econometrics - # output tokens",
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "econometrics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_econometrics"
      }
    },
    {
      "evaluation_name": "Global Facts - # eval",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - # train",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - truncated",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 494.0
      },
      "score_details": {
        "score": 455.63,
        "details": {
          "description": "min=455.63, mean=455.63, max=455.63, sum=911.26 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Global Facts - # output tokens",
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "global_facts",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_global_facts"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # eval",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 108.0
      },
      "score_details": {
        "score": 108.0,
        "details": {
          "description": "min=108, mean=108, max=108, sum=216 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # train",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - truncated",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 497.0
      },
      "score_details": {
        "score": 414.444,
        "details": {
          "description": "min=414.444, mean=414.444, max=414.444, sum=828.889 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Jurisprudence - # output tokens",
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "jurisprudence",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_jurisprudence"
      }
    },
    {
      "evaluation_name": "Philosophy - # eval",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 311.0
      },
      "score_details": {
        "score": 311.0,
        "details": {
          "description": "min=311, mean=311, max=311, sum=622 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - # train",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - truncated",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 425.0
      },
      "score_details": {
        "score": 339.093,
        "details": {
          "description": "min=339.093, mean=339.093, max=339.093, sum=678.186 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Philosophy - # output tokens",
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "philosophy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_philosophy"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # eval",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 272.0
      },
      "score_details": {
        "score": 272.0,
        "details": {
          "description": "min=272, mean=272, max=272, sum=544 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # train",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - truncated",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1340.0
      },
      "score_details": {
        "score": 1104.614,
        "details": {
          "description": "min=1104.614, mean=1104.614, max=1104.614, sum=2209.228 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Professional Medicine - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_medicine"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # eval",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # train",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - truncated",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 521.0
      },
      "score_details": {
        "score": 440.48,
        "details": {
          "description": "min=440.48, mean=440.48, max=440.48, sum=880.96 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - # output tokens",
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "us_foreign_policy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_us_foreign_policy"
      }
    },
    {
      "evaluation_name": "Astronomy - # eval",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 152.0
      },
      "score_details": {
        "score": 152.0,
        "details": {
          "description": "min=152, mean=152, max=152, sum=304 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - # train",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - truncated",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 691.0
      },
      "score_details": {
        "score": 613.033,
        "details": {
          "description": "min=613.033, mean=613.033, max=613.033, sum=1226.066 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Astronomy - # output tokens",
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "astronomy",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_astronomy"
      }
    },
    {
      "evaluation_name": "Business Ethics - # eval",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - # train",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - truncated",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 684.0
      },
      "score_details": {
        "score": 559.31,
        "details": {
          "description": "min=559.31, mean=559.31, max=559.31, sum=1118.62 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Business Ethics - # output tokens",
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "business_ethics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_business_ethics"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # eval",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 265.0
      },
      "score_details": {
        "score": 265.0,
        "details": {
          "description": "min=265, mean=265, max=265, sum=530 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # train",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - truncated",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 498.0
      },
      "score_details": {
        "score": 394.77,
        "details": {
          "description": "min=394.77, mean=394.77, max=394.77, sum=789.54 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - # output tokens",
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "clinical_knowledge",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_clinical_knowledge"
      }
    },
    {
      "evaluation_name": "College Biology - # eval",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 144.0
      },
      "score_details": {
        "score": 144.0,
        "details": {
          "description": "min=144, mean=144, max=144, sum=288 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - # train",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - truncated",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 563.0
      },
      "score_details": {
        "score": 480.875,
        "details": {
          "description": "min=480.875, mean=480.875, max=480.875, sum=961.75 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Biology - # output tokens",
      "metric_config": {
        "evaluation_description": "The college biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_biology"
      }
    },
    {
      "evaluation_name": "College Computer Science - # eval",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - # train",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - truncated",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 913.0
      },
      "score_details": {
        "score": 861.96,
        "details": {
          "description": "min=861.96, mean=861.96, max=861.96, sum=1723.92 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Computer Science - # output tokens",
      "metric_config": {
        "evaluation_description": "The college computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_computer_science"
      }
    },
    {
      "evaluation_name": "College Mathematics - # eval",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - # train",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - truncated",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 670.0
      },
      "score_details": {
        "score": 636.94,
        "details": {
          "description": "min=636.94, mean=636.94, max=636.94, sum=1273.88 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Mathematics - # output tokens",
      "metric_config": {
        "evaluation_description": "The college mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_mathematics"
      }
    },
    {
      "evaluation_name": "College Medicine - # eval",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 173.0
      },
      "score_details": {
        "score": 173.0,
        "details": {
          "description": "min=173, mean=173, max=173, sum=346 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - # train",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - truncated",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 618.0
      },
      "score_details": {
        "score": 512.584,
        "details": {
          "description": "min=512.584, mean=512.584, max=512.584, sum=1025.168 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Medicine - # output tokens",
      "metric_config": {
        "evaluation_description": "The college medicine subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_medicine",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_medicine"
      }
    },
    {
      "evaluation_name": "College Physics - # eval",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 102.0
      },
      "score_details": {
        "score": 102.0,
        "details": {
          "description": "min=102, mean=102, max=102, sum=204 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - # train",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - truncated",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 562.0
      },
      "score_details": {
        "score": 513.647,
        "details": {
          "description": "min=513.647, mean=513.647, max=513.647, sum=1027.294 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "College Physics - # output tokens",
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "college_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_college_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # eval",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 235.0
      },
      "score_details": {
        "score": 235.0,
        "details": {
          "description": "min=235, mean=235, max=235, sum=470 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # train",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - truncated",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 389.0
      },
      "score_details": {
        "score": 309.477,
        "details": {
          "description": "min=309.477, mean=309.477, max=309.477, sum=618.953 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Conceptual Physics - # output tokens",
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "conceptual_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_conceptual_physics"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # eval",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 145.0
      },
      "score_details": {
        "score": 145.0,
        "details": {
          "description": "min=145, mean=145, max=145, sum=290 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # train",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - truncated",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 512.0
      },
      "score_details": {
        "score": 480.524,
        "details": {
          "description": "min=480.524, mean=480.524, max=480.524, sum=961.048 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Electrical Engineering - # output tokens",
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "electrical_engineering",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_electrical_engineering"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # eval",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 378.0
      },
      "score_details": {
        "score": 378.0,
        "details": {
          "description": "min=378, mean=378, max=378, sum=756 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # train",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - truncated",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 624.0
      },
      "score_details": {
        "score": 599.828,
        "details": {
          "description": "min=599.828, mean=599.828, max=599.828, sum=1199.656 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - # output tokens",
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "elementary_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_elementary_mathematics"
      }
    },
    {
      "evaluation_name": "Formal Logic - # eval",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 126.0
      },
      "score_details": {
        "score": 126.0,
        "details": {
          "description": "min=126, mean=126, max=126, sum=252 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - # train",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - truncated",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 729.0
      },
      "score_details": {
        "score": 623.508,
        "details": {
          "description": "min=623.508, mean=623.508, max=623.508, sum=1247.016 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "Formal Logic - # output tokens",
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "formal_logic",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_formal_logic"
      }
    },
    {
      "evaluation_name": "High School Biology - # eval",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 310.0
      },
      "score_details": {
        "score": 310.0,
        "details": {
          "description": "min=310, mean=310, max=310, sum=620 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - # train",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - truncated",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 611.0
      },
      "score_details": {
        "score": 501.255,
        "details": {
          "description": "min=501.255, mean=501.255, max=501.255, sum=1002.51 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Biology - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school biology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_biology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_biology"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # eval",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 203.0
      },
      "score_details": {
        "score": 203.0,
        "details": {
          "description": "min=203, mean=203, max=203, sum=406 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # train",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - truncated",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 583.0
      },
      "score_details": {
        "score": 515.473,
        "details": {
          "description": "min=515.473, mean=515.473, max=515.473, sum=1030.946 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Chemistry - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school chemistry subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_chemistry",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_chemistry"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # eval",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # train",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - truncated",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 999.0
      },
      "score_details": {
        "score": 954.08,
        "details": {
          "description": "min=954.08, mean=954.08, max=954.08, sum=1908.16 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School Computer Science - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school computer science subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_computer_science",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_computer_science"
      }
    },
    {
      "evaluation_name": "High School European History - # eval",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 165.0
      },
      "score_details": {
        "score": 165.0,
        "details": {
          "description": "min=165, mean=165, max=165, sum=330 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - # train",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - truncated",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 3169.0
      },
      "score_details": {
        "score": 2823.23,
        "details": {
          "description": "min=2823.23, mean=2823.23, max=2823.23, sum=5646.461 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School European History - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school european history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_european_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_european_history"
      }
    },
    {
      "evaluation_name": "High School Geography - # eval",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 198.0
      },
      "score_details": {
        "score": 198.0,
        "details": {
          "description": "min=198, mean=198, max=198, sum=396 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - # train",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - truncated",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 478.0
      },
      "score_details": {
        "score": 392.939,
        "details": {
          "description": "min=392.939, mean=392.939, max=392.939, sum=785.879 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Geography - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school geography subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_geography",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_geography"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # eval",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 193.0
      },
      "score_details": {
        "score": 193.0,
        "details": {
          "description": "min=193, mean=193, max=193, sum=386 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # train",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - truncated",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 557.0
      },
      "score_details": {
        "score": 475.44,
        "details": {
          "description": "min=475.44, mean=475.44, max=475.44, sum=950.881 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Government And Politics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school government and politics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_government_and_politics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_government_and_politics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # eval",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 390.0
      },
      "score_details": {
        "score": 390.0,
        "details": {
          "description": "min=390, mean=390, max=390, sum=780 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # train",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - truncated",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 472.0
      },
      "score_details": {
        "score": 395.962,
        "details": {
          "description": "min=395.962, mean=395.962, max=395.962, sum=791.923 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Macroeconomics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school macroeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_macroeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_macroeconomics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # eval",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 270.0
      },
      "score_details": {
        "score": 270.0,
        "details": {
          "description": "min=270, mean=270, max=270, sum=540 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # train",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - truncated",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 595.0
      },
      "score_details": {
        "score": 580.393,
        "details": {
          "description": "min=580.393, mean=580.393, max=580.393, sum=1160.785 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Mathematics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_mathematics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_mathematics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # eval",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 238.0
      },
      "score_details": {
        "score": 238.0,
        "details": {
          "description": "min=238, mean=238, max=238, sum=476 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # train",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - truncated",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 486.0
      },
      "score_details": {
        "score": 414.361,
        "details": {
          "description": "min=414.361, mean=414.361, max=414.361, sum=828.723 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Microeconomics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school microeconomics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_microeconomics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_microeconomics"
      }
    },
    {
      "evaluation_name": "High School Physics - # eval",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 151.0
      },
      "score_details": {
        "score": 151.0,
        "details": {
          "description": "min=151, mean=151, max=151, sum=302 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - # train",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - truncated",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 633.0
      },
      "score_details": {
        "score": 592.252,
        "details": {
          "description": "min=592.252, mean=592.252, max=592.252, sum=1184.503 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Physics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_physics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_physics"
      }
    },
    {
      "evaluation_name": "High School Psychology - # eval",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 545.0
      },
      "score_details": {
        "score": 545.0,
        "details": {
          "description": "min=545, mean=545, max=545, sum=1090 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - # train",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - truncated",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 595.0
      },
      "score_details": {
        "score": 496.51,
        "details": {
          "description": "min=496.51, mean=496.51, max=496.51, sum=993.02 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Psychology - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_psychology"
      }
    },
    {
      "evaluation_name": "High School Statistics - # eval",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 216.0
      },
      "score_details": {
        "score": 216.0,
        "details": {
          "description": "min=216, mean=216, max=216, sum=432 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - # train",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - truncated",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 924.0
      },
      "score_details": {
        "score": 860.532,
        "details": {
          "description": "min=860.532, mean=860.532, max=860.532, sum=1721.065 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School Statistics - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school statistics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_statistics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_statistics"
      }
    },
    {
      "evaluation_name": "High School US History - # eval",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 204.0
      },
      "score_details": {
        "score": 204.0,
        "details": {
          "description": "min=204, mean=204, max=204, sum=408 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - # train",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - truncated",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 2545.0
      },
      "score_details": {
        "score": 2239.544,
        "details": {
          "description": "min=2239.544, mean=2239.544, max=2239.544, sum=4479.088 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School US History - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school us history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_us_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_us_history"
      }
    },
    {
      "evaluation_name": "High School World History - # eval",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 237.0
      },
      "score_details": {
        "score": 237.0,
        "details": {
          "description": "min=237, mean=237, max=237, sum=474 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - # train",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - truncated",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1648.0
      },
      "score_details": {
        "score": 1437.051,
        "details": {
          "description": "min=1437.051, mean=1437.051, max=1437.051, sum=2874.101 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "High School World History - # output tokens",
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "high_school_world_history",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_high_school_world_history"
      }
    },
    {
      "evaluation_name": "Human Aging - # eval",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 223.0
      },
      "score_details": {
        "score": 223.0,
        "details": {
          "description": "min=223, mean=223, max=223, sum=446 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - # train",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - truncated",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 401.0
      },
      "score_details": {
        "score": 323.906,
        "details": {
          "description": "min=323.906, mean=323.906, max=323.906, sum=647.812 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Aging - # output tokens",
      "metric_config": {
        "evaluation_description": "The human aging subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_aging",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_aging"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # eval",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 131.0
      },
      "score_details": {
        "score": 131.0,
        "details": {
          "description": "min=131, mean=131, max=131, sum=262 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # train",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - truncated",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 437.0
      },
      "score_details": {
        "score": 338.74,
        "details": {
          "description": "min=338.74, mean=338.74, max=338.74, sum=677.481 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "Human Sexuality - # output tokens",
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "human_sexuality",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_human_sexuality"
      }
    },
    {
      "evaluation_name": "International Law - # eval",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 121.0
      },
      "score_details": {
        "score": 121.0,
        "details": {
          "description": "min=121, mean=121, max=121, sum=242 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - # train",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - truncated",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 739.0
      },
      "score_details": {
        "score": 651.686,
        "details": {
          "description": "min=651.686, mean=651.686, max=651.686, sum=1303.372 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "International Law - # output tokens",
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "international_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_international_law"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # eval",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 163.0
      },
      "score_details": {
        "score": 163.0,
        "details": {
          "description": "min=163, mean=163, max=163, sum=326 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # train",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - truncated",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 536.0
      },
      "score_details": {
        "score": 443.969,
        "details": {
          "description": "min=443.969, mean=443.969, max=443.969, sum=887.939 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Logical Fallacies - # output tokens",
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "logical_fallacies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_logical_fallacies"
      }
    },
    {
      "evaluation_name": "Machine Learning - # eval",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 112.0
      },
      "score_details": {
        "score": 112.0,
        "details": {
          "description": "min=112, mean=112, max=112, sum=224 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - # train",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - truncated",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 745.0
      },
      "score_details": {
        "score": 705.973,
        "details": {
          "description": "min=705.973, mean=705.973, max=705.973, sum=1411.946 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Machine Learning - # output tokens",
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "machine_learning",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_machine_learning"
      }
    },
    {
      "evaluation_name": "Management - # eval",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 103.0
      },
      "score_details": {
        "score": 103.0,
        "details": {
          "description": "min=103, mean=103, max=103, sum=206 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - # train",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - truncated",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 367.0
      },
      "score_details": {
        "score": 284.68,
        "details": {
          "description": "min=284.68, mean=284.68, max=284.68, sum=569.359 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Management - # output tokens",
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "management",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_management"
      }
    },
    {
      "evaluation_name": "Marketing - # eval",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 234.0
      },
      "score_details": {
        "score": 234.0,
        "details": {
          "description": "min=234, mean=234, max=234, sum=468 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - # train",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - truncated",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 514.0
      },
      "score_details": {
        "score": 428.726,
        "details": {
          "description": "min=428.726, mean=428.726, max=428.726, sum=857.453 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Marketing - # output tokens",
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "marketing",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_marketing"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # eval",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 100.0,
        "details": {
          "description": "min=100, mean=100, max=100, sum=200 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # train",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - truncated",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 420.0
      },
      "score_details": {
        "score": 334.69,
        "details": {
          "description": "min=334.69, mean=334.69, max=334.69, sum=669.38 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Medical Genetics - # output tokens",
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "medical_genetics",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_medical_genetics"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # eval",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 783.0
      },
      "score_details": {
        "score": 783.0,
        "details": {
          "description": "min=783, mean=783, max=783, sum=1566 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # train",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - truncated",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 394.0
      },
      "score_details": {
        "score": 325.215,
        "details": {
          "description": "min=325.215, mean=325.215, max=325.215, sum=650.429 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Miscellaneous - # output tokens",
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "miscellaneous",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_miscellaneous"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # eval",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 346.0
      },
      "score_details": {
        "score": 346.0,
        "details": {
          "description": "min=346, mean=346, max=346, sum=692 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # train",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - truncated",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 577.0
      },
      "score_details": {
        "score": 494.63,
        "details": {
          "description": "min=494.63, mean=494.63, max=494.63, sum=989.26 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Disputes - # output tokens",
      "metric_config": {
        "evaluation_description": "The moral disputes subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_disputes",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_disputes"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # eval",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 895.0
      },
      "score_details": {
        "score": 895.0,
        "details": {
          "description": "min=895, mean=895, max=895, sum=1790 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # train",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - truncated",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 766.0
      },
      "score_details": {
        "score": 687.566,
        "details": {
          "description": "min=687.566, mean=687.566, max=687.566, sum=1375.133 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Moral Scenarios - # output tokens",
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "moral_scenarios",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_moral_scenarios"
      }
    },
    {
      "evaluation_name": "Nutrition - # eval",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 306.0
      },
      "score_details": {
        "score": 306.0,
        "details": {
          "description": "min=306, mean=306, max=306, sum=612 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - # train",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - truncated",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 705.0
      },
      "score_details": {
        "score": 589.663,
        "details": {
          "description": "min=589.663, mean=589.663, max=589.663, sum=1179.327 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Nutrition - # output tokens",
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "nutrition",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_nutrition"
      }
    },
    {
      "evaluation_name": "Prehistory - # eval",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 324.0
      },
      "score_details": {
        "score": 324.0,
        "details": {
          "description": "min=324, mean=324, max=324, sum=648 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - # train",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - truncated",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 629.0
      },
      "score_details": {
        "score": 538.179,
        "details": {
          "description": "min=538.179, mean=538.179, max=538.179, sum=1076.358 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Prehistory - # output tokens",
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "prehistory",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_prehistory"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # eval",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 282.0
      },
      "score_details": {
        "score": 282.0,
        "details": {
          "description": "min=282, mean=282, max=282, sum=564 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # train",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - truncated",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 833.0
      },
      "score_details": {
        "score": 752.83,
        "details": {
          "description": "min=752.83, mean=752.83, max=752.83, sum=1505.66 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Accounting - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional accounting subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_accounting",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_accounting"
      }
    },
    {
      "evaluation_name": "Professional Law - # eval",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1534.0
      },
      "score_details": {
        "score": 1534.0,
        "details": {
          "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - # train",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - truncated",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1925.0
      },
      "score_details": {
        "score": 1701.909,
        "details": {
          "description": "min=1701.909, mean=1701.909, max=1701.909, sum=3403.819 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Law - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_law",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_law"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # eval",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 612.0
      },
      "score_details": {
        "score": 612.0,
        "details": {
          "description": "min=612, mean=612, max=612, sum=1224 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # train",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - truncated",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 669.0
      },
      "score_details": {
        "score": 594.446,
        "details": {
          "description": "min=594.446, mean=594.446, max=594.446, sum=1188.892 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Professional Psychology - # output tokens",
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "professional_psychology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_professional_psychology"
      }
    },
    {
      "evaluation_name": "Public Relations - # eval",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 110.0
      },
      "score_details": {
        "score": 110.0,
        "details": {
          "description": "min=110, mean=110, max=110, sum=220 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - # train",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - truncated",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 498.0
      },
      "score_details": {
        "score": 426.982,
        "details": {
          "description": "min=426.982, mean=426.982, max=426.982, sum=853.964 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Public Relations - # output tokens",
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "public_relations",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_public_relations"
      }
    },
    {
      "evaluation_name": "Security Studies - # eval",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 245.0
      },
      "score_details": {
        "score": 245.0,
        "details": {
          "description": "min=245, mean=245, max=245, sum=490 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - # train",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - truncated",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1387.0
      },
      "score_details": {
        "score": 1185.8,
        "details": {
          "description": "min=1185.8, mean=1185.8, max=1185.8, sum=2371.6 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Security Studies - # output tokens",
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "security_studies",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_security_studies"
      }
    },
    {
      "evaluation_name": "Sociology - # eval",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 201.0
      },
      "score_details": {
        "score": 201.0,
        "details": {
          "description": "min=201, mean=201, max=201, sum=402 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - # train",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - truncated",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 533.0
      },
      "score_details": {
        "score": 459.642,
        "details": {
          "description": "min=459.642, mean=459.642, max=459.642, sum=919.284 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Sociology - # output tokens",
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "sociology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_sociology"
      }
    },
    {
      "evaluation_name": "Virology - # eval",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 166.0
      },
      "score_details": {
        "score": 166.0,
        "details": {
          "description": "min=166, mean=166, max=166, sum=332 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - # train",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - truncated",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 428.0
      },
      "score_details": {
        "score": 337.06,
        "details": {
          "description": "min=337.06, mean=337.06, max=337.06, sum=674.12 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "Virology - # output tokens",
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "virology",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_virology"
      }
    },
    {
      "evaluation_name": "World Religions - # eval",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 171.0
      },
      "score_details": {
        "score": 171.0,
        "details": {
          "description": "min=171, mean=171, max=171, sum=342 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - # train",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=10 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - truncated",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 359.0
      },
      "score_details": {
        "score": 270.187,
        "details": {
          "description": "min=270.187, mean=270.187, max=270.187, sum=540.374 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    },
    {
      "evaluation_name": "World Religions - # output tokens",
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=2 (2)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": "world_religions",
        "method": "multiple_choice_joint",
        "eval_split": "test",
        "groups": "mmlu_world_religions"
      }
    }
  ]
}
