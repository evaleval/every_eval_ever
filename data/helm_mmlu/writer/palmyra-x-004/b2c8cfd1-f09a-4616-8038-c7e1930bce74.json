{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_mmlu/writer_palmyra-x-004/1770835937.459157",
  "retrieved_timestamp": "1770835937.459157",
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Palmyra-X-004",
    "id": "writer/palmyra-x-004",
    "developer": "writer",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU All Subjects",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.813,
        "details": {
          "description": "min=0.52, mean=0.813, max=0.959, sum=92.659 (114)",
          "tab": "Accuracy",
          "MMLU All Subjects - Observed inference time (s)": {
            "description": "min=0.298, mean=0.535, max=2.946, sum=60.962 (114)",
            "tab": "Efficiency",
            "score": 0.5347547453538
          },
          "MMLU All Subjects - # eval": {
            "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
            "tab": "General information",
            "score": 246.35087719298247
          },
          "MMLU All Subjects - # train": {
            "description": "min=5, mean=5, max=5, sum=570 (114)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU All Subjects - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU All Subjects - # prompt tokens": {
            "description": "min=274.52, mean=614.619, max=2797.885, sum=70066.61 (114)",
            "tab": "General information",
            "score": 614.6193817308517
          },
          "MMLU All Subjects - # output tokens": {
            "description": "min=0.968, mean=0.991, max=1, sum=112.995 (114)",
            "tab": "General information",
            "score": 0.9911842955118555
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "anatomy",
            "astronomy",
            "business_ethics",
            "clinical_knowledge",
            "college_biology",
            "college_chemistry",
            "college_computer_science",
            "college_mathematics",
            "college_medicine",
            "college_physics",
            "computer_security",
            "conceptual_physics",
            "econometrics",
            "electrical_engineering",
            "elementary_mathematics",
            "formal_logic",
            "global_facts",
            "high_school_biology",
            "high_school_chemistry",
            "high_school_computer_science",
            "high_school_european_history",
            "high_school_geography",
            "high_school_government_and_politics",
            "high_school_macroeconomics",
            "high_school_mathematics",
            "high_school_microeconomics",
            "high_school_physics",
            "high_school_psychology",
            "high_school_statistics",
            "high_school_us_history",
            "high_school_world_history",
            "human_aging",
            "human_sexuality",
            "international_law",
            "jurisprudence",
            "logical_fallacies",
            "machine_learning",
            "management",
            "marketing",
            "medical_genetics",
            "miscellaneous",
            "moral_disputes",
            "moral_scenarios",
            "nutrition",
            "philosophy",
            "prehistory",
            "professional_accounting",
            "professional_law",
            "professional_medicine",
            "professional_psychology",
            "public_relations",
            "security_studies",
            "sociology",
            "us_foreign_policy",
            "virology",
            "world_religions"
          ],
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": [
            "mmlu_abstract_algebra",
            "mmlu_anatomy",
            "mmlu_astronomy",
            "mmlu_business_ethics",
            "mmlu_clinical_knowledge",
            "mmlu_college_biology",
            "mmlu_college_chemistry",
            "mmlu_college_computer_science",
            "mmlu_college_mathematics",
            "mmlu_college_medicine",
            "mmlu_college_physics",
            "mmlu_computer_security",
            "mmlu_conceptual_physics",
            "mmlu_econometrics",
            "mmlu_electrical_engineering",
            "mmlu_elementary_mathematics",
            "mmlu_formal_logic",
            "mmlu_global_facts",
            "mmlu_high_school_biology",
            "mmlu_high_school_chemistry",
            "mmlu_high_school_computer_science",
            "mmlu_high_school_european_history",
            "mmlu_high_school_geography",
            "mmlu_high_school_government_and_politics",
            "mmlu_high_school_macroeconomics",
            "mmlu_high_school_mathematics",
            "mmlu_high_school_microeconomics",
            "mmlu_high_school_physics",
            "mmlu_high_school_psychology",
            "mmlu_high_school_statistics",
            "mmlu_high_school_us_history",
            "mmlu_high_school_world_history",
            "mmlu_human_aging",
            "mmlu_human_sexuality",
            "mmlu_international_law",
            "mmlu_jurisprudence",
            "mmlu_logical_fallacies",
            "mmlu_machine_learning",
            "mmlu_management",
            "mmlu_marketing",
            "mmlu_medical_genetics",
            "mmlu_miscellaneous",
            "mmlu_moral_disputes",
            "mmlu_moral_scenarios",
            "mmlu_nutrition",
            "mmlu_philosophy",
            "mmlu_prehistory",
            "mmlu_professional_accounting",
            "mmlu_professional_law",
            "mmlu_professional_medicine",
            "mmlu_professional_psychology",
            "mmlu_public_relations",
            "mmlu_security_studies",
            "mmlu_sociology",
            "mmlu_us_foreign_policy",
            "mmlu_virology",
            "mmlu_world_religions"
          ]
        }
      }
    },
    {
      "evaluation_name": "Abstract Algebra",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Abstract Algebra",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.75,
        "details": {
          "description": "min=0.75, mean=0.75, max=0.75, sum=1.5 (2)",
          "tab": "Accuracy",
          "Abstract Algebra - Observed inference time (s)": {
            "description": "min=0.722, mean=0.722, max=0.722, sum=1.444 (2)",
            "tab": "Efficiency",
            "score": 0.7220739269256592
          },
          "Abstract Algebra - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Abstract Algebra - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Abstract Algebra - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Abstract Algebra - # prompt tokens": {
            "description": "min=373.43, mean=373.43, max=373.43, sum=746.86 (2)",
            "tab": "General information",
            "score": 373.43
          },
          "Abstract Algebra - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "abstract_algebra",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_abstract_algebra"
        }
      }
    },
    {
      "evaluation_name": "Anatomy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Anatomy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.822,
        "details": {
          "description": "min=0.822, mean=0.822, max=0.822, sum=1.644 (2)",
          "tab": "Accuracy",
          "Anatomy - Observed inference time (s)": {
            "description": "min=0.323, mean=0.323, max=0.323, sum=0.646 (2)",
            "tab": "Efficiency",
            "score": 0.3229873922136095
          },
          "Anatomy - # eval": {
            "description": "min=135, mean=135, max=135, sum=270 (2)",
            "tab": "General information",
            "score": 135.0
          },
          "Anatomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Anatomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Anatomy - # prompt tokens": {
            "description": "min=353.874, mean=353.874, max=353.874, sum=707.748 (2)",
            "tab": "General information",
            "score": 353.8740740740741
          },
          "Anatomy - # output tokens": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.985 (2)",
            "tab": "General information",
            "score": 0.9925925925925926
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "anatomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_anatomy"
        }
      }
    },
    {
      "evaluation_name": "College Physics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on College Physics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.647,
        "details": {
          "description": "min=0.647, mean=0.647, max=0.647, sum=1.294 (2)",
          "tab": "Accuracy",
          "College Chemistry - Observed inference time (s)": {
            "description": "min=0.316, mean=0.316, max=0.316, sum=0.632 (2)",
            "tab": "Efficiency",
            "score": 0.316190505027771
          },
          "College Biology - Observed inference time (s)": {
            "description": "min=2.087, mean=2.087, max=2.087, sum=4.175 (2)",
            "tab": "Efficiency",
            "score": 2.0873730795250998
          },
          "College Computer Science - Observed inference time (s)": {
            "description": "min=1.575, mean=1.575, max=1.575, sum=3.15 (2)",
            "tab": "Efficiency",
            "score": 1.574983057975769
          },
          "College Mathematics - Observed inference time (s)": {
            "description": "min=1.58, mean=1.58, max=1.58, sum=3.16 (2)",
            "tab": "Efficiency",
            "score": 1.5799101972579956
          },
          "College Medicine - Observed inference time (s)": {
            "description": "min=1.786, mean=1.786, max=1.786, sum=3.572 (2)",
            "tab": "Efficiency",
            "score": 1.786004883705536
          },
          "College Physics - Observed inference time (s)": {
            "description": "min=1.112, mean=1.112, max=1.112, sum=2.225 (2)",
            "tab": "Efficiency",
            "score": 1.1123062372207642
          },
          "College Chemistry - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Chemistry - # prompt tokens": {
            "description": "min=549.28, mean=549.28, max=549.28, sum=1098.56 (2)",
            "tab": "General information",
            "score": 549.28
          },
          "College Chemistry - # output tokens": {
            "description": "min=0.97, mean=0.97, max=0.97, sum=1.94 (2)",
            "tab": "General information",
            "score": 0.97
          },
          "College Biology - # eval": {
            "description": "min=144, mean=144, max=144, sum=288 (2)",
            "tab": "General information",
            "score": 144.0
          },
          "College Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # prompt tokens": {
            "description": "min=473.875, mean=473.875, max=473.875, sum=947.75 (2)",
            "tab": "General information",
            "score": 473.875
          },
          "College Biology - # output tokens": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.986 (2)",
            "tab": "General information",
            "score": 0.9930555555555556
          },
          "College Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # prompt tokens": {
            "description": "min=828.29, mean=828.29, max=828.29, sum=1656.58 (2)",
            "tab": "General information",
            "score": 828.29
          },
          "College Computer Science - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "College Mathematics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # prompt tokens": {
            "description": "min=594.51, mean=594.51, max=594.51, sum=1189.02 (2)",
            "tab": "General information",
            "score": 594.51
          },
          "College Mathematics - # output tokens": {
            "description": "min=0.98, mean=0.98, max=0.98, sum=1.96 (2)",
            "tab": "General information",
            "score": 0.98
          },
          "College Medicine - # eval": {
            "description": "min=173, mean=173, max=173, sum=346 (2)",
            "tab": "General information",
            "score": 173.0
          },
          "College Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # prompt tokens": {
            "description": "min=502.705, mean=502.705, max=502.705, sum=1005.41 (2)",
            "tab": "General information",
            "score": 502.70520231213874
          },
          "College Medicine - # output tokens": {
            "description": "min=0.994, mean=0.994, max=0.994, sum=1.988 (2)",
            "tab": "General information",
            "score": 0.9942196531791907
          },
          "College Physics - # eval": {
            "description": "min=102, mean=102, max=102, sum=204 (2)",
            "tab": "General information",
            "score": 102.0
          },
          "College Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # prompt tokens": {
            "description": "min=503.569, mean=503.569, max=503.569, sum=1007.137 (2)",
            "tab": "General information",
            "score": 503.5686274509804
          },
          "College Physics - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "college_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_college_physics"
        }
      }
    },
    {
      "evaluation_name": "Computer Security",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Computer Security",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.82,
        "details": {
          "description": "min=0.82, mean=0.82, max=0.82, sum=1.64 (2)",
          "tab": "Accuracy",
          "Computer Security - Observed inference time (s)": {
            "description": "min=0.309, mean=0.309, max=0.309, sum=0.618 (2)",
            "tab": "Efficiency",
            "score": 0.3091639161109924
          },
          "Computer Security - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Computer Security - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Computer Security - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Computer Security - # prompt tokens": {
            "description": "min=378.51, mean=378.51, max=378.51, sum=757.02 (2)",
            "tab": "General information",
            "score": 378.51
          },
          "Computer Security - # output tokens": {
            "description": "min=0.99, mean=0.99, max=0.99, sum=1.98 (2)",
            "tab": "General information",
            "score": 0.99
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "computer_security",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_computer_security"
        }
      }
    },
    {
      "evaluation_name": "Econometrics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Econometrics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.684,
        "details": {
          "description": "min=0.684, mean=0.684, max=0.684, sum=1.368 (2)",
          "tab": "Accuracy",
          "Econometrics - Observed inference time (s)": {
            "description": "min=0.322, mean=0.322, max=0.322, sum=0.644 (2)",
            "tab": "Efficiency",
            "score": 0.32210456070147064
          },
          "Econometrics - # eval": {
            "description": "min=114, mean=114, max=114, sum=228 (2)",
            "tab": "General information",
            "score": 114.0
          },
          "Econometrics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Econometrics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Econometrics - # prompt tokens": {
            "description": "min=614.421, mean=614.421, max=614.421, sum=1228.842 (2)",
            "tab": "General information",
            "score": 614.421052631579
          },
          "Econometrics - # output tokens": {
            "description": "min=0.991, mean=0.991, max=0.991, sum=1.982 (2)",
            "tab": "General information",
            "score": 0.9912280701754386
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "econometrics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_econometrics"
        }
      }
    },
    {
      "evaluation_name": "Global Facts",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Global Facts",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.62,
        "details": {
          "description": "min=0.62, mean=0.62, max=0.62, sum=1.24 (2)",
          "tab": "Accuracy",
          "Global Facts - Observed inference time (s)": {
            "description": "min=0.311, mean=0.311, max=0.311, sum=0.621 (2)",
            "tab": "Efficiency",
            "score": 0.31063568592071533
          },
          "Global Facts - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Global Facts - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Global Facts - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Global Facts - # prompt tokens": {
            "description": "min=399.71, mean=399.71, max=399.71, sum=799.42 (2)",
            "tab": "General information",
            "score": 399.71
          },
          "Global Facts - # output tokens": {
            "description": "min=0.98, mean=0.98, max=0.98, sum=1.96 (2)",
            "tab": "General information",
            "score": 0.98
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "global_facts",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_global_facts"
        }
      }
    },
    {
      "evaluation_name": "Jurisprudence",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Jurisprudence",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.843,
        "details": {
          "description": "min=0.843, mean=0.843, max=0.843, sum=1.685 (2)",
          "tab": "Accuracy",
          "Jurisprudence - Observed inference time (s)": {
            "description": "min=0.298, mean=0.298, max=0.298, sum=0.597 (2)",
            "tab": "Efficiency",
            "score": 0.29833372433980304
          },
          "Jurisprudence - # eval": {
            "description": "min=108, mean=108, max=108, sum=216 (2)",
            "tab": "General information",
            "score": 108.0
          },
          "Jurisprudence - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Jurisprudence - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Jurisprudence - # prompt tokens": {
            "description": "min=394.63, mean=394.63, max=394.63, sum=789.259 (2)",
            "tab": "General information",
            "score": 394.6296296296296
          },
          "Jurisprudence - # output tokens": {
            "description": "min=0.991, mean=0.991, max=0.991, sum=1.981 (2)",
            "tab": "General information",
            "score": 0.9907407407407407
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "jurisprudence",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_jurisprudence"
        }
      }
    },
    {
      "evaluation_name": "Philosophy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Philosophy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.83,
        "details": {
          "description": "min=0.83, mean=0.83, max=0.83, sum=1.659 (2)",
          "tab": "Accuracy",
          "Philosophy - Observed inference time (s)": {
            "description": "min=0.306, mean=0.306, max=0.306, sum=0.612 (2)",
            "tab": "Efficiency",
            "score": 0.30590631187537093
          },
          "Philosophy - # eval": {
            "description": "min=311, mean=311, max=311, sum=622 (2)",
            "tab": "General information",
            "score": 311.0
          },
          "Philosophy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Philosophy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Philosophy - # prompt tokens": {
            "description": "min=329.084, mean=329.084, max=329.084, sum=658.167 (2)",
            "tab": "General information",
            "score": 329.08360128617363
          },
          "Philosophy - # output tokens": {
            "description": "min=0.994, mean=0.994, max=0.994, sum=1.987 (2)",
            "tab": "General information",
            "score": 0.9935691318327974
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "philosophy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_philosophy"
        }
      }
    },
    {
      "evaluation_name": "Professional Psychology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Professional Psychology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.845,
        "details": {
          "description": "min=0.845, mean=0.845, max=0.845, sum=1.69 (2)",
          "tab": "Accuracy",
          "Professional Medicine - Observed inference time (s)": {
            "description": "min=0.42, mean=0.42, max=0.42, sum=0.841 (2)",
            "tab": "Efficiency",
            "score": 0.42044701295740466
          },
          "Professional Accounting - Observed inference time (s)": {
            "description": "min=0.352, mean=0.352, max=0.352, sum=0.704 (2)",
            "tab": "Efficiency",
            "score": 0.35206349944391996
          },
          "Professional Law - Observed inference time (s)": {
            "description": "min=2.946, mean=2.946, max=2.946, sum=5.892 (2)",
            "tab": "Efficiency",
            "score": 2.9459040923410784
          },
          "Professional Psychology - Observed inference time (s)": {
            "description": "min=0.342, mean=0.342, max=0.342, sum=0.683 (2)",
            "tab": "Efficiency",
            "score": 0.34150391076904496
          },
          "Professional Medicine - # eval": {
            "description": "min=272, mean=272, max=272, sum=544 (2)",
            "tab": "General information",
            "score": 272.0
          },
          "Professional Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Medicine - # prompt tokens": {
            "description": "min=1094.489, mean=1094.489, max=1094.489, sum=2188.978 (2)",
            "tab": "General information",
            "score": 1094.4889705882354
          },
          "Professional Medicine - # output tokens": {
            "description": "min=0.989, mean=0.989, max=0.989, sum=1.978 (2)",
            "tab": "General information",
            "score": 0.9889705882352942
          },
          "Professional Accounting - # eval": {
            "description": "min=282, mean=282, max=282, sum=564 (2)",
            "tab": "General information",
            "score": 282.0
          },
          "Professional Accounting - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Accounting - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # prompt tokens": {
            "description": "min=658.585, mean=658.585, max=658.585, sum=1317.17 (2)",
            "tab": "General information",
            "score": 658.5851063829788
          },
          "Professional Accounting - # output tokens": {
            "description": "min=0.968, mean=0.968, max=0.968, sum=1.936 (2)",
            "tab": "General information",
            "score": 0.9680851063829787
          },
          "Professional Law - # eval": {
            "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
            "tab": "General information",
            "score": 1534.0
          },
          "Professional Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # prompt tokens": {
            "description": "min=1637.601, mean=1637.601, max=1637.601, sum=3275.202 (2)",
            "tab": "General information",
            "score": 1637.6010430247718
          },
          "Professional Law - # output tokens": {
            "description": "min=0.995, mean=0.995, max=0.995, sum=1.99 (2)",
            "tab": "General information",
            "score": 0.9947848761408083
          },
          "Professional Psychology - # eval": {
            "description": "min=612, mean=612, max=612, sum=1224 (2)",
            "tab": "General information",
            "score": 612.0
          },
          "Professional Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # prompt tokens": {
            "description": "min=575.098, mean=575.098, max=575.098, sum=1150.196 (2)",
            "tab": "General information",
            "score": 575.0980392156863
          },
          "Professional Psychology - # output tokens": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.987 (2)",
            "tab": "General information",
            "score": 0.9934640522875817
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "professional_psychology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_professional_psychology"
        }
      }
    },
    {
      "evaluation_name": "Us Foreign Policy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Us Foreign Policy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.92,
        "details": {
          "description": "min=0.92, mean=0.92, max=0.92, sum=1.84 (2)",
          "tab": "Accuracy",
          "Us Foreign Policy - Observed inference time (s)": {
            "description": "min=0.312, mean=0.312, max=0.312, sum=0.624 (2)",
            "tab": "Efficiency",
            "score": 0.31222330808639526
          },
          "Us Foreign Policy - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Us Foreign Policy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Us Foreign Policy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Us Foreign Policy - # prompt tokens": {
            "description": "min=422.79, mean=422.79, max=422.79, sum=845.58 (2)",
            "tab": "General information",
            "score": 422.79
          },
          "Us Foreign Policy - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "us_foreign_policy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_us_foreign_policy"
        }
      }
    },
    {
      "evaluation_name": "Astronomy",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Astronomy",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.928,
        "details": {
          "description": "min=0.928, mean=0.928, max=0.928, sum=1.855 (2)",
          "tab": "Accuracy",
          "Astronomy - Observed inference time (s)": {
            "description": "min=0.326, mean=0.326, max=0.326, sum=0.653 (2)",
            "tab": "Efficiency",
            "score": 0.3264871161235006
          },
          "Astronomy - # eval": {
            "description": "min=152, mean=152, max=152, sum=304 (2)",
            "tab": "General information",
            "score": 152.0
          },
          "Astronomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Astronomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Astronomy - # prompt tokens": {
            "description": "min=579.684, mean=579.684, max=579.684, sum=1159.368 (2)",
            "tab": "General information",
            "score": 579.6842105263158
          },
          "Astronomy - # output tokens": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.987 (2)",
            "tab": "General information",
            "score": 0.993421052631579
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "astronomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_astronomy"
        }
      }
    },
    {
      "evaluation_name": "Business Ethics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Business Ethics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.76,
        "details": {
          "description": "min=0.76, mean=0.76, max=0.76, sum=1.52 (2)",
          "tab": "Accuracy",
          "Business Ethics - Observed inference time (s)": {
            "description": "min=0.321, mean=0.321, max=0.321, sum=0.643 (2)",
            "tab": "Efficiency",
            "score": 0.3212712168693542
          },
          "Business Ethics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Business Ethics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Business Ethics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Business Ethics - # prompt tokens": {
            "description": "min=569.52, mean=569.52, max=569.52, sum=1139.04 (2)",
            "tab": "General information",
            "score": 569.52
          },
          "Business Ethics - # output tokens": {
            "description": "min=0.98, mean=0.98, max=0.98, sum=1.96 (2)",
            "tab": "General information",
            "score": 0.98
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "business_ethics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_business_ethics"
        }
      }
    },
    {
      "evaluation_name": "Clinical Knowledge",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Clinical Knowledge",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.879,
        "details": {
          "description": "min=0.879, mean=0.879, max=0.879, sum=1.758 (2)",
          "tab": "Accuracy",
          "Clinical Knowledge - Observed inference time (s)": {
            "description": "min=0.477, mean=0.477, max=0.477, sum=0.953 (2)",
            "tab": "Efficiency",
            "score": 0.4765495894090185
          },
          "Clinical Knowledge - # eval": {
            "description": "min=265, mean=265, max=265, sum=530 (2)",
            "tab": "General information",
            "score": 265.0
          },
          "Clinical Knowledge - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Clinical Knowledge - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Clinical Knowledge - # prompt tokens": {
            "description": "min=397.928, mean=397.928, max=397.928, sum=795.857 (2)",
            "tab": "General information",
            "score": 397.92830188679244
          },
          "Clinical Knowledge - # output tokens": {
            "description": "min=0.992, mean=0.992, max=0.992, sum=1.985 (2)",
            "tab": "General information",
            "score": 0.9924528301886792
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "clinical_knowledge",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_clinical_knowledge"
        }
      }
    },
    {
      "evaluation_name": "Conceptual Physics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Conceptual Physics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.885,
        "details": {
          "description": "min=0.885, mean=0.885, max=0.885, sum=1.77 (2)",
          "tab": "Accuracy",
          "Conceptual Physics - Observed inference time (s)": {
            "description": "min=0.347, mean=0.347, max=0.347, sum=0.693 (2)",
            "tab": "Efficiency",
            "score": 0.3465714748869551
          },
          "Conceptual Physics - # eval": {
            "description": "min=235, mean=235, max=235, sum=470 (2)",
            "tab": "General information",
            "score": 235.0
          },
          "Conceptual Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Conceptual Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Conceptual Physics - # prompt tokens": {
            "description": "min=304.834, mean=304.834, max=304.834, sum=609.668 (2)",
            "tab": "General information",
            "score": 304.83404255319147
          },
          "Conceptual Physics - # output tokens": {
            "description": "min=0.996, mean=0.996, max=0.996, sum=1.991 (2)",
            "tab": "General information",
            "score": 0.9957446808510638
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "conceptual_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_conceptual_physics"
        }
      }
    },
    {
      "evaluation_name": "Electrical Engineering",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Electrical Engineering",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.793,
        "details": {
          "description": "min=0.793, mean=0.793, max=0.793, sum=1.586 (2)",
          "tab": "Accuracy",
          "Electrical Engineering - Observed inference time (s)": {
            "description": "min=0.305, mean=0.305, max=0.305, sum=0.611 (2)",
            "tab": "Efficiency",
            "score": 0.3054168865598481
          },
          "Electrical Engineering - # eval": {
            "description": "min=145, mean=145, max=145, sum=290 (2)",
            "tab": "General information",
            "score": 145.0
          },
          "Electrical Engineering - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Electrical Engineering - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Electrical Engineering - # prompt tokens": {
            "description": "min=435.607, mean=435.607, max=435.607, sum=871.214 (2)",
            "tab": "General information",
            "score": 435.60689655172416
          },
          "Electrical Engineering - # output tokens": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.986 (2)",
            "tab": "General information",
            "score": 0.993103448275862
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "electrical_engineering",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_electrical_engineering"
        }
      }
    },
    {
      "evaluation_name": "Elementary Mathematics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Elementary Mathematics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.841,
        "details": {
          "description": "min=0.841, mean=0.841, max=0.841, sum=1.683 (2)",
          "tab": "Accuracy",
          "Elementary Mathematics - Observed inference time (s)": {
            "description": "min=0.313, mean=0.313, max=0.313, sum=0.627 (2)",
            "tab": "Efficiency",
            "score": 0.31325215069705215
          },
          "Elementary Mathematics - # eval": {
            "description": "min=378, mean=378, max=378, sum=756 (2)",
            "tab": "General information",
            "score": 378.0
          },
          "Elementary Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Elementary Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Elementary Mathematics - # prompt tokens": {
            "description": "min=531.854, mean=531.854, max=531.854, sum=1063.709 (2)",
            "tab": "General information",
            "score": 531.8544973544973
          },
          "Elementary Mathematics - # output tokens": {
            "description": "min=0.995, mean=0.995, max=0.995, sum=1.989 (2)",
            "tab": "General information",
            "score": 0.9947089947089947
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "elementary_mathematics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_elementary_mathematics"
        }
      }
    },
    {
      "evaluation_name": "Formal Logic",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Formal Logic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.579,
        "details": {
          "description": "min=0.579, mean=0.579, max=0.579, sum=1.159 (2)",
          "tab": "Accuracy",
          "Formal Logic - Observed inference time (s)": {
            "description": "min=1.035, mean=1.035, max=1.035, sum=2.07 (2)",
            "tab": "Efficiency",
            "score": 1.034958042795696
          },
          "Formal Logic - # eval": {
            "description": "min=126, mean=126, max=126, sum=252 (2)",
            "tab": "General information",
            "score": 126.0
          },
          "Formal Logic - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Formal Logic - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Formal Logic - # prompt tokens": {
            "description": "min=601.778, mean=601.778, max=601.778, sum=1203.556 (2)",
            "tab": "General information",
            "score": 601.7777777777778
          },
          "Formal Logic - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "formal_logic",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_formal_logic"
        }
      }
    },
    {
      "evaluation_name": "High School World History",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on High School World History",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.911,
        "details": {
          "description": "min=0.911, mean=0.911, max=0.911, sum=1.823 (2)",
          "tab": "Accuracy",
          "High School Biology - Observed inference time (s)": {
            "description": "min=0.562, mean=0.562, max=0.562, sum=1.123 (2)",
            "tab": "Efficiency",
            "score": 0.561508382520368
          },
          "High School Chemistry - Observed inference time (s)": {
            "description": "min=0.349, mean=0.349, max=0.349, sum=0.698 (2)",
            "tab": "Efficiency",
            "score": 0.34899539900530735
          },
          "High School Computer Science - Observed inference time (s)": {
            "description": "min=0.423, mean=0.423, max=0.423, sum=0.845 (2)",
            "tab": "Efficiency",
            "score": 0.4227438974380493
          },
          "High School European History - Observed inference time (s)": {
            "description": "min=0.899, mean=0.899, max=0.899, sum=1.799 (2)",
            "tab": "Efficiency",
            "score": 0.8994465018763687
          },
          "High School Geography - Observed inference time (s)": {
            "description": "min=0.324, mean=0.324, max=0.324, sum=0.647 (2)",
            "tab": "Efficiency",
            "score": 0.3236422189558395
          },
          "High School Government And Politics - Observed inference time (s)": {
            "description": "min=0.314, mean=0.314, max=0.314, sum=0.627 (2)",
            "tab": "Efficiency",
            "score": 0.31354672550537427
          },
          "High School Macroeconomics - Observed inference time (s)": {
            "description": "min=0.314, mean=0.314, max=0.314, sum=0.628 (2)",
            "tab": "Efficiency",
            "score": 0.31394460568061244
          },
          "High School Mathematics - Observed inference time (s)": {
            "description": "min=0.315, mean=0.315, max=0.315, sum=0.63 (2)",
            "tab": "Efficiency",
            "score": 0.3151667806837294
          },
          "High School Microeconomics - Observed inference time (s)": {
            "description": "min=0.315, mean=0.315, max=0.315, sum=0.63 (2)",
            "tab": "Efficiency",
            "score": 0.3151869453301951
          },
          "High School Physics - Observed inference time (s)": {
            "description": "min=0.32, mean=0.32, max=0.32, sum=0.639 (2)",
            "tab": "Efficiency",
            "score": 0.31971652302520953
          },
          "High School Psychology - Observed inference time (s)": {
            "description": "min=0.315, mean=0.315, max=0.315, sum=0.63 (2)",
            "tab": "Efficiency",
            "score": 0.3149662079067405
          },
          "High School Statistics - Observed inference time (s)": {
            "description": "min=0.386, mean=0.386, max=0.386, sum=0.772 (2)",
            "tab": "Efficiency",
            "score": 0.3859624167283376
          },
          "High School US History - Observed inference time (s)": {
            "description": "min=0.651, mean=0.651, max=0.651, sum=1.303 (2)",
            "tab": "Efficiency",
            "score": 0.6513510615217919
          },
          "High School World History - Observed inference time (s)": {
            "description": "min=0.472, mean=0.472, max=0.472, sum=0.945 (2)",
            "tab": "Efficiency",
            "score": 0.4723552480528626
          },
          "High School Biology - # eval": {
            "description": "min=310, mean=310, max=310, sum=620 (2)",
            "tab": "General information",
            "score": 310.0
          },
          "High School Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Biology - # prompt tokens": {
            "description": "min=513.671, mean=513.671, max=513.671, sum=1027.342 (2)",
            "tab": "General information",
            "score": 513.6709677419354
          },
          "High School Biology - # output tokens": {
            "description": "min=0.994, mean=0.994, max=0.994, sum=1.987 (2)",
            "tab": "General information",
            "score": 0.9935483870967742
          },
          "High School Chemistry - # eval": {
            "description": "min=203, mean=203, max=203, sum=406 (2)",
            "tab": "General information",
            "score": 203.0
          },
          "High School Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # prompt tokens": {
            "description": "min=496.704, mean=496.704, max=496.704, sum=993.409 (2)",
            "tab": "General information",
            "score": 496.70443349753697
          },
          "High School Chemistry - # output tokens": {
            "description": "min=0.985, mean=0.985, max=0.985, sum=1.97 (2)",
            "tab": "General information",
            "score": 0.9852216748768473
          },
          "High School Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "High School Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # prompt tokens": {
            "description": "min=867.78, mean=867.78, max=867.78, sum=1735.56 (2)",
            "tab": "General information",
            "score": 867.78
          },
          "High School Computer Science - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School European History - # eval": {
            "description": "min=165, mean=165, max=165, sum=330 (2)",
            "tab": "General information",
            "score": 165.0
          },
          "High School European History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School European History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # prompt tokens": {
            "description": "min=2797.885, mean=2797.885, max=2797.885, sum=5595.77 (2)",
            "tab": "General information",
            "score": 2797.8848484848486
          },
          "High School European History - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          },
          "High School Geography - # eval": {
            "description": "min=198, mean=198, max=198, sum=396 (2)",
            "tab": "General information",
            "score": 198.0
          },
          "High School Geography - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Geography - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # prompt tokens": {
            "description": "min=372.035, mean=372.035, max=372.035, sum=744.071 (2)",
            "tab": "General information",
            "score": 372.0353535353535
          },
          "High School Geography - # output tokens": {
            "description": "min=0.99, mean=0.99, max=0.99, sum=1.98 (2)",
            "tab": "General information",
            "score": 0.98989898989899
          },
          "High School Government And Politics - # eval": {
            "description": "min=193, mean=193, max=193, sum=386 (2)",
            "tab": "General information",
            "score": 193.0
          },
          "High School Government And Politics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Government And Politics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # prompt tokens": {
            "description": "min=465.824, mean=465.824, max=465.824, sum=931.648 (2)",
            "tab": "General information",
            "score": 465.8238341968912
          },
          "High School Government And Politics - # output tokens": {
            "description": "min=0.979, mean=0.979, max=0.979, sum=1.959 (2)",
            "tab": "General information",
            "score": 0.9792746113989638
          },
          "High School Macroeconomics - # eval": {
            "description": "min=390, mean=390, max=390, sum=780 (2)",
            "tab": "General information",
            "score": 390.0
          },
          "High School Macroeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Macroeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # prompt tokens": {
            "description": "min=370.908, mean=370.908, max=370.908, sum=741.815 (2)",
            "tab": "General information",
            "score": 370.9076923076923
          },
          "High School Macroeconomics - # output tokens": {
            "description": "min=0.992, mean=0.992, max=0.992, sum=1.985 (2)",
            "tab": "General information",
            "score": 0.9923076923076923
          },
          "High School Mathematics - # eval": {
            "description": "min=270, mean=270, max=270, sum=540 (2)",
            "tab": "General information",
            "score": 270.0
          },
          "High School Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # prompt tokens": {
            "description": "min=532.356, mean=532.356, max=532.356, sum=1064.711 (2)",
            "tab": "General information",
            "score": 532.3555555555556
          },
          "High School Mathematics - # output tokens": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.985 (2)",
            "tab": "General information",
            "score": 0.9925925925925926
          },
          "High School Microeconomics - # eval": {
            "description": "min=238, mean=238, max=238, sum=476 (2)",
            "tab": "General information",
            "score": 238.0
          },
          "High School Microeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Microeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # prompt tokens": {
            "description": "min=399.013, mean=399.013, max=399.013, sum=798.025 (2)",
            "tab": "General information",
            "score": 399.0126050420168
          },
          "High School Microeconomics - # output tokens": {
            "description": "min=0.987, mean=0.987, max=0.987, sum=1.975 (2)",
            "tab": "General information",
            "score": 0.9873949579831933
          },
          "High School Physics - # eval": {
            "description": "min=151, mean=151, max=151, sum=302 (2)",
            "tab": "General information",
            "score": 151.0
          },
          "High School Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # prompt tokens": {
            "description": "min=560.457, mean=560.457, max=560.457, sum=1120.914 (2)",
            "tab": "General information",
            "score": 560.4569536423841
          },
          "High School Physics - # output tokens": {
            "description": "min=0.974, mean=0.974, max=0.974, sum=1.947 (2)",
            "tab": "General information",
            "score": 0.9735099337748344
          },
          "High School Psychology - # eval": {
            "description": "min=545, mean=545, max=545, sum=1090 (2)",
            "tab": "General information",
            "score": 545.0
          },
          "High School Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # prompt tokens": {
            "description": "min=495.242, mean=495.242, max=495.242, sum=990.484 (2)",
            "tab": "General information",
            "score": 495.2422018348624
          },
          "High School Psychology - # output tokens": {
            "description": "min=0.996, mean=0.996, max=0.996, sum=1.993 (2)",
            "tab": "General information",
            "score": 0.9963302752293578
          },
          "High School Statistics - # eval": {
            "description": "min=216, mean=216, max=216, sum=432 (2)",
            "tab": "General information",
            "score": 216.0
          },
          "High School Statistics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Statistics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # prompt tokens": {
            "description": "min=795.639, mean=795.639, max=795.639, sum=1591.278 (2)",
            "tab": "General information",
            "score": 795.6388888888889
          },
          "High School Statistics - # output tokens": {
            "description": "min=0.977, mean=0.977, max=0.977, sum=1.954 (2)",
            "tab": "General information",
            "score": 0.9768518518518519
          },
          "High School US History - # eval": {
            "description": "min=204, mean=204, max=204, sum=408 (2)",
            "tab": "General information",
            "score": 204.0
          },
          "High School US History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School US History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # prompt tokens": {
            "description": "min=2217.809, mean=2217.809, max=2217.809, sum=4435.618 (2)",
            "tab": "General information",
            "score": 2217.8088235294117
          },
          "High School US History - # output tokens": {
            "description": "min=0.99, mean=0.99, max=0.99, sum=1.98 (2)",
            "tab": "General information",
            "score": 0.9901960784313726
          },
          "High School World History - # eval": {
            "description": "min=237, mean=237, max=237, sum=474 (2)",
            "tab": "General information",
            "score": 237.0
          },
          "High School World History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School World History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # prompt tokens": {
            "description": "min=1428.173, mean=1428.173, max=1428.173, sum=2856.346 (2)",
            "tab": "General information",
            "score": 1428.1729957805908
          },
          "High School World History - # output tokens": {
            "description": "min=0.996, mean=0.996, max=0.996, sum=1.992 (2)",
            "tab": "General information",
            "score": 0.9957805907172996
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "high_school_world_history",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_high_school_world_history"
        }
      }
    },
    {
      "evaluation_name": "Human Sexuality",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Human Sexuality",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.924,
        "details": {
          "description": "min=0.924, mean=0.924, max=0.924, sum=1.847 (2)",
          "tab": "Accuracy",
          "Human Aging - Observed inference time (s)": {
            "description": "min=0.322, mean=0.322, max=0.322, sum=0.644 (2)",
            "tab": "Efficiency",
            "score": 0.3221198432648663
          },
          "Human Sexuality - Observed inference time (s)": {
            "description": "min=0.319, mean=0.319, max=0.319, sum=0.638 (2)",
            "tab": "Efficiency",
            "score": 0.31875184474100593
          },
          "Human Aging - # eval": {
            "description": "min=223, mean=223, max=223, sum=446 (2)",
            "tab": "General information",
            "score": 223.0
          },
          "Human Aging - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Aging - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Aging - # prompt tokens": {
            "description": "min=319.888, mean=319.888, max=319.888, sum=639.776 (2)",
            "tab": "General information",
            "score": 319.88789237668163
          },
          "Human Aging - # output tokens": {
            "description": "min=0.996, mean=0.996, max=0.996, sum=1.991 (2)",
            "tab": "General information",
            "score": 0.9955156950672646
          },
          "Human Sexuality - # eval": {
            "description": "min=131, mean=131, max=131, sum=262 (2)",
            "tab": "General information",
            "score": 131.0
          },
          "Human Sexuality - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Sexuality - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # prompt tokens": {
            "description": "min=341.168, mean=341.168, max=341.168, sum=682.336 (2)",
            "tab": "General information",
            "score": 341.1679389312977
          },
          "Human Sexuality - # output tokens": {
            "description": "min=0.992, mean=0.992, max=0.992, sum=1.985 (2)",
            "tab": "General information",
            "score": 0.9923664122137404
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "human_sexuality",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_human_sexuality"
        }
      }
    },
    {
      "evaluation_name": "International Law",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on International Law",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.901,
        "details": {
          "description": "min=0.901, mean=0.901, max=0.901, sum=1.802 (2)",
          "tab": "Accuracy",
          "International Law - Observed inference time (s)": {
            "description": "min=0.336, mean=0.336, max=0.336, sum=0.671 (2)",
            "tab": "Efficiency",
            "score": 0.33550412989844963
          },
          "International Law - # eval": {
            "description": "min=121, mean=121, max=121, sum=242 (2)",
            "tab": "General information",
            "score": 121.0
          },
          "International Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "International Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "International Law - # prompt tokens": {
            "description": "min=639.818, mean=639.818, max=639.818, sum=1279.636 (2)",
            "tab": "General information",
            "score": 639.8181818181819
          },
          "International Law - # output tokens": {
            "description": "min=0.983, mean=0.983, max=0.983, sum=1.967 (2)",
            "tab": "General information",
            "score": 0.9834710743801653
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "international_law",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_international_law"
        }
      }
    },
    {
      "evaluation_name": "Logical Fallacies",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Logical Fallacies",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.877,
        "details": {
          "description": "min=0.877, mean=0.877, max=0.877, sum=1.755 (2)",
          "tab": "Accuracy",
          "Logical Fallacies - Observed inference time (s)": {
            "description": "min=0.312, mean=0.312, max=0.312, sum=0.624 (2)",
            "tab": "Efficiency",
            "score": 0.3120760069302986
          },
          "Logical Fallacies - # eval": {
            "description": "min=163, mean=163, max=163, sum=326 (2)",
            "tab": "General information",
            "score": 163.0
          },
          "Logical Fallacies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Logical Fallacies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Logical Fallacies - # prompt tokens": {
            "description": "min=449.564, mean=449.564, max=449.564, sum=899.129 (2)",
            "tab": "General information",
            "score": 449.5644171779141
          },
          "Logical Fallacies - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "logical_fallacies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_logical_fallacies"
        }
      }
    },
    {
      "evaluation_name": "Machine Learning",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Machine Learning",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.679,
        "details": {
          "description": "min=0.679, mean=0.679, max=0.679, sum=1.357 (2)",
          "tab": "Accuracy",
          "Machine Learning - Observed inference time (s)": {
            "description": "min=0.337, mean=0.337, max=0.337, sum=0.674 (2)",
            "tab": "Efficiency",
            "score": 0.3368471988609859
          },
          "Machine Learning - # eval": {
            "description": "min=112, mean=112, max=112, sum=224 (2)",
            "tab": "General information",
            "score": 112.0
          },
          "Machine Learning - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Machine Learning - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Machine Learning - # prompt tokens": {
            "description": "min=668.054, mean=668.054, max=668.054, sum=1336.107 (2)",
            "tab": "General information",
            "score": 668.0535714285714
          },
          "Machine Learning - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "machine_learning",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_machine_learning"
        }
      }
    },
    {
      "evaluation_name": "Management",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Management",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.903,
        "details": {
          "description": "min=0.903, mean=0.903, max=0.903, sum=1.806 (2)",
          "tab": "Accuracy",
          "Management - Observed inference time (s)": {
            "description": "min=0.31, mean=0.31, max=0.31, sum=0.621 (2)",
            "tab": "Efficiency",
            "score": 0.3103753525076561
          },
          "Management - # eval": {
            "description": "min=103, mean=103, max=103, sum=206 (2)",
            "tab": "General information",
            "score": 103.0
          },
          "Management - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Management - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Management - # prompt tokens": {
            "description": "min=283.786, mean=283.786, max=283.786, sum=567.573 (2)",
            "tab": "General information",
            "score": 283.7864077669903
          },
          "Management - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "management",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_management"
        }
      }
    },
    {
      "evaluation_name": "Marketing",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Marketing",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.932,
        "details": {
          "description": "min=0.932, mean=0.932, max=0.932, sum=1.863 (2)",
          "tab": "Accuracy",
          "Marketing - Observed inference time (s)": {
            "description": "min=0.314, mean=0.314, max=0.314, sum=0.628 (2)",
            "tab": "Efficiency",
            "score": 0.3138112644863944
          },
          "Marketing - # eval": {
            "description": "min=234, mean=234, max=234, sum=468 (2)",
            "tab": "General information",
            "score": 234.0
          },
          "Marketing - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Marketing - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Marketing - # prompt tokens": {
            "description": "min=404.218, mean=404.218, max=404.218, sum=808.436 (2)",
            "tab": "General information",
            "score": 404.21794871794873
          },
          "Marketing - # output tokens": {
            "description": "min=0.991, mean=0.991, max=0.991, sum=1.983 (2)",
            "tab": "General information",
            "score": 0.9914529914529915
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "marketing",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_marketing"
        }
      }
    },
    {
      "evaluation_name": "Medical Genetics",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Medical Genetics",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.87,
        "details": {
          "description": "min=0.87, mean=0.87, max=0.87, sum=1.74 (2)",
          "tab": "Accuracy",
          "Medical Genetics - Observed inference time (s)": {
            "description": "min=0.31, mean=0.31, max=0.31, sum=0.619 (2)",
            "tab": "Efficiency",
            "score": 0.3096977710723877
          },
          "Medical Genetics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Medical Genetics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Medical Genetics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Medical Genetics - # prompt tokens": {
            "description": "min=340.99, mean=340.99, max=340.99, sum=681.98 (2)",
            "tab": "General information",
            "score": 340.99
          },
          "Medical Genetics - # output tokens": {
            "description": "min=0.97, mean=0.97, max=0.97, sum=1.94 (2)",
            "tab": "General information",
            "score": 0.97
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "medical_genetics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_medical_genetics"
        }
      }
    },
    {
      "evaluation_name": "Miscellaneous",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Miscellaneous",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.934,
        "details": {
          "description": "min=0.934, mean=0.934, max=0.934, sum=1.867 (2)",
          "tab": "Accuracy",
          "Miscellaneous - Observed inference time (s)": {
            "description": "min=0.311, mean=0.311, max=0.311, sum=0.621 (2)",
            "tab": "Efficiency",
            "score": 0.3106613128730316
          },
          "Miscellaneous - # eval": {
            "description": "min=783, mean=783, max=783, sum=1566 (2)",
            "tab": "General information",
            "score": 783.0
          },
          "Miscellaneous - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Miscellaneous - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Miscellaneous - # prompt tokens": {
            "description": "min=299.911, mean=299.911, max=299.911, sum=599.821 (2)",
            "tab": "General information",
            "score": 299.9106002554278
          },
          "Miscellaneous - # output tokens": {
            "description": "min=0.99, mean=0.99, max=0.99, sum=1.98 (2)",
            "tab": "General information",
            "score": 0.9897828863346104
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "miscellaneous",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_miscellaneous"
        }
      }
    },
    {
      "evaluation_name": "Moral Scenarios",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Moral Scenarios",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.825,
        "details": {
          "description": "min=0.825, mean=0.825, max=0.825, sum=1.649 (2)",
          "tab": "Accuracy",
          "Moral Disputes - Observed inference time (s)": {
            "description": "min=0.313, mean=0.313, max=0.313, sum=0.626 (2)",
            "tab": "Efficiency",
            "score": 0.31282479501184013
          },
          "Moral Scenarios - Observed inference time (s)": {
            "description": "min=0.335, mean=0.335, max=0.335, sum=0.67 (2)",
            "tab": "Efficiency",
            "score": 0.3348748574709759
          },
          "Moral Disputes - # eval": {
            "description": "min=346, mean=346, max=346, sum=692 (2)",
            "tab": "General information",
            "score": 346.0
          },
          "Moral Disputes - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Disputes - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Disputes - # prompt tokens": {
            "description": "min=476.113, mean=476.113, max=476.113, sum=952.225 (2)",
            "tab": "General information",
            "score": 476.1127167630058
          },
          "Moral Disputes - # output tokens": {
            "description": "min=0.994, mean=0.994, max=0.994, sum=1.988 (2)",
            "tab": "General information",
            "score": 0.9942196531791907
          },
          "Moral Scenarios - # eval": {
            "description": "min=895, mean=895, max=895, sum=1790 (2)",
            "tab": "General information",
            "score": 895.0
          },
          "Moral Scenarios - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Scenarios - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # prompt tokens": {
            "description": "min=656.455, mean=656.455, max=656.455, sum=1312.909 (2)",
            "tab": "General information",
            "score": 656.454748603352
          },
          "Moral Scenarios - # output tokens": {
            "description": "min=0.993, mean=0.993, max=0.993, sum=1.987 (2)",
            "tab": "General information",
            "score": 0.9932960893854749
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "moral_scenarios",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_moral_scenarios"
        }
      }
    },
    {
      "evaluation_name": "Nutrition",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Nutrition",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.869,
        "details": {
          "description": "min=0.869, mean=0.869, max=0.869, sum=1.739 (2)",
          "tab": "Accuracy",
          "Nutrition - Observed inference time (s)": {
            "description": "min=0.332, mean=0.332, max=0.332, sum=0.664 (2)",
            "tab": "Efficiency",
            "score": 0.33182784311132496
          },
          "Nutrition - # eval": {
            "description": "min=306, mean=306, max=306, sum=612 (2)",
            "tab": "General information",
            "score": 306.0
          },
          "Nutrition - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Nutrition - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Nutrition - # prompt tokens": {
            "description": "min=586.814, mean=586.814, max=586.814, sum=1173.627 (2)",
            "tab": "General information",
            "score": 586.8137254901961
          },
          "Nutrition - # output tokens": {
            "description": "min=0.997, mean=0.997, max=0.997, sum=1.993 (2)",
            "tab": "General information",
            "score": 0.9967320261437909
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "nutrition",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_nutrition"
        }
      }
    },
    {
      "evaluation_name": "Prehistory",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Prehistory",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.917,
        "details": {
          "description": "min=0.917, mean=0.917, max=0.917, sum=1.833 (2)",
          "tab": "Accuracy",
          "Prehistory - Observed inference time (s)": {
            "description": "min=0.316, mean=0.316, max=0.316, sum=0.632 (2)",
            "tab": "Efficiency",
            "score": 0.3158548356574259
          },
          "Prehistory - # eval": {
            "description": "min=324, mean=324, max=324, sum=648 (2)",
            "tab": "General information",
            "score": 324.0
          },
          "Prehistory - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Prehistory - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Prehistory - # prompt tokens": {
            "description": "min=514.528, mean=514.528, max=514.528, sum=1029.056 (2)",
            "tab": "General information",
            "score": 514.5277777777778
          },
          "Prehistory - # output tokens": {
            "description": "min=0.988, mean=0.988, max=0.988, sum=1.975 (2)",
            "tab": "General information",
            "score": 0.9876543209876543
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "prehistory",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_prehistory"
        }
      }
    },
    {
      "evaluation_name": "Public Relations",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Public Relations",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.791,
        "details": {
          "description": "min=0.791, mean=0.791, max=0.791, sum=1.582 (2)",
          "tab": "Accuracy",
          "Public Relations - Observed inference time (s)": {
            "description": "min=0.328, mean=0.328, max=0.328, sum=0.657 (2)",
            "tab": "Efficiency",
            "score": 0.32829454161904076
          },
          "Public Relations - # eval": {
            "description": "min=110, mean=110, max=110, sum=220 (2)",
            "tab": "General information",
            "score": 110.0
          },
          "Public Relations - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Public Relations - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Public Relations - # prompt tokens": {
            "description": "min=405.318, mean=405.318, max=405.318, sum=810.636 (2)",
            "tab": "General information",
            "score": 405.3181818181818
          },
          "Public Relations - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "public_relations",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_public_relations"
        }
      }
    },
    {
      "evaluation_name": "Security Studies",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Security Studies",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.849,
        "details": {
          "description": "min=0.849, mean=0.849, max=0.849, sum=1.698 (2)",
          "tab": "Accuracy",
          "Security Studies - Observed inference time (s)": {
            "description": "min=0.443, mean=0.443, max=0.443, sum=0.886 (2)",
            "tab": "Efficiency",
            "score": 0.44323594618816764
          },
          "Security Studies - # eval": {
            "description": "min=245, mean=245, max=245, sum=490 (2)",
            "tab": "General information",
            "score": 245.0
          },
          "Security Studies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Security Studies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Security Studies - # prompt tokens": {
            "description": "min=1164.473, mean=1164.473, max=1164.473, sum=2328.947 (2)",
            "tab": "General information",
            "score": 1164.4734693877551
          },
          "Security Studies - # output tokens": {
            "description": "min=0.992, mean=0.992, max=0.992, sum=1.984 (2)",
            "tab": "General information",
            "score": 0.9918367346938776
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "security_studies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_security_studies"
        }
      }
    },
    {
      "evaluation_name": "Sociology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Sociology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.915,
        "details": {
          "description": "min=0.915, mean=0.915, max=0.915, sum=1.831 (2)",
          "tab": "Accuracy",
          "Sociology - Observed inference time (s)": {
            "description": "min=0.337, mean=0.337, max=0.337, sum=0.674 (2)",
            "tab": "Efficiency",
            "score": 0.336861949654954
          },
          "Sociology - # eval": {
            "description": "min=201, mean=201, max=201, sum=402 (2)",
            "tab": "General information",
            "score": 201.0
          },
          "Sociology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Sociology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Sociology - # prompt tokens": {
            "description": "min=445.517, mean=445.517, max=445.517, sum=891.035 (2)",
            "tab": "General information",
            "score": 445.51741293532336
          },
          "Sociology - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=2 (2)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "sociology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_sociology"
        }
      }
    },
    {
      "evaluation_name": "Virology",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on Virology",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.584,
        "details": {
          "description": "min=0.584, mean=0.584, max=0.584, sum=1.169 (2)",
          "tab": "Accuracy",
          "Virology - Observed inference time (s)": {
            "description": "min=0.328, mean=0.328, max=0.328, sum=0.656 (2)",
            "tab": "Efficiency",
            "score": 0.32804813155208723
          },
          "Virology - # eval": {
            "description": "min=166, mean=166, max=166, sum=332 (2)",
            "tab": "General information",
            "score": 166.0
          },
          "Virology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Virology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Virology - # prompt tokens": {
            "description": "min=343.018, mean=343.018, max=343.018, sum=686.036 (2)",
            "tab": "General information",
            "score": 343.01807228915663
          },
          "Virology - # output tokens": {
            "description": "min=0.994, mean=0.994, max=0.994, sum=1.988 (2)",
            "tab": "General information",
            "score": 0.9939759036144579
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "virology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_virology"
        }
      }
    },
    {
      "evaluation_name": "World Religions",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on World Religions",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.842,
        "details": {
          "description": "min=0.842, mean=0.842, max=0.842, sum=1.684 (2)",
          "tab": "Accuracy",
          "World Religions - Observed inference time (s)": {
            "description": "min=0.376, mean=0.376, max=0.376, sum=0.752 (2)",
            "tab": "Efficiency",
            "score": 0.3761981662951018
          },
          "World Religions - # eval": {
            "description": "min=171, mean=171, max=171, sum=342 (2)",
            "tab": "General information",
            "score": 171.0
          },
          "World Religions - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "World Religions - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "World Religions - # prompt tokens": {
            "description": "min=274.52, mean=274.52, max=274.52, sum=549.041 (2)",
            "tab": "General information",
            "score": 274.5204678362573
          },
          "World Religions - # output tokens": {
            "description": "min=0.994, mean=0.994, max=0.994, sum=1.988 (2)",
            "tab": "General information",
            "score": 0.9941520467836257
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "world_religions",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_world_religions"
        }
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.629,
        "details": {
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}