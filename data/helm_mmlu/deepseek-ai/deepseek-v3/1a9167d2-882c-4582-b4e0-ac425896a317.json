{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_mmlu/deepseek-ai_deepseek-v3/1770830564.5477738",
  "retrieved_timestamp": "1770830564.5477738",
  "source_metadata": {
    "source_name": "helm_mmlu",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "DeepSeek v3",
    "id": "deepseek-ai/deepseek-v3",
    "developer": "deepseek-ai",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "MMLU All Subjects - EM",
      "source_data": {
        "dataset_name": "MMLU All Subjects",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.872,
        "details": {
          "description": "min=0.596, mean=0.872, max=0.979, sum=99.412 (114)",
          "tab": "Accuracy",
          "MMLU All Subjects - Observed inference time (s)": {
            "description": "min=0.495, mean=1.354, max=6.344, sum=154.309 (114)",
            "tab": "Efficiency",
            "score": 1.353587049503403
          },
          "MMLU All Subjects - # eval": {
            "description": "min=100, mean=246.351, max=1534, sum=28084 (114)",
            "tab": "General information",
            "score": 246.35087719298247
          },
          "MMLU All Subjects - # train": {
            "description": "min=5, mean=5, max=5, sum=570 (114)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU All Subjects - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU All Subjects - # prompt tokens": {
            "description": "min=268.918, mean=607.861, max=2773.188, sum=69296.195 (114)",
            "tab": "General information",
            "score": 607.8613565650774
          },
          "MMLU All Subjects - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (114)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "anatomy",
            "astronomy",
            "business_ethics",
            "clinical_knowledge",
            "college_biology",
            "college_chemistry",
            "college_computer_science",
            "college_mathematics",
            "college_medicine",
            "college_physics",
            "computer_security",
            "conceptual_physics",
            "econometrics",
            "electrical_engineering",
            "elementary_mathematics",
            "formal_logic",
            "global_facts",
            "high_school_biology",
            "high_school_chemistry",
            "high_school_computer_science",
            "high_school_european_history",
            "high_school_geography",
            "high_school_government_and_politics",
            "high_school_macroeconomics",
            "high_school_mathematics",
            "high_school_microeconomics",
            "high_school_physics",
            "high_school_psychology",
            "high_school_statistics",
            "high_school_us_history",
            "high_school_world_history",
            "human_aging",
            "human_sexuality",
            "international_law",
            "jurisprudence",
            "logical_fallacies",
            "machine_learning",
            "management",
            "marketing",
            "medical_genetics",
            "miscellaneous",
            "moral_disputes",
            "moral_scenarios",
            "nutrition",
            "philosophy",
            "prehistory",
            "professional_accounting",
            "professional_law",
            "professional_medicine",
            "professional_psychology",
            "public_relations",
            "security_studies",
            "sociology",
            "us_foreign_policy",
            "virology",
            "world_religions"
          ],
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": [
            "mmlu_abstract_algebra",
            "mmlu_anatomy",
            "mmlu_astronomy",
            "mmlu_business_ethics",
            "mmlu_clinical_knowledge",
            "mmlu_college_biology",
            "mmlu_college_chemistry",
            "mmlu_college_computer_science",
            "mmlu_college_mathematics",
            "mmlu_college_medicine",
            "mmlu_college_physics",
            "mmlu_computer_security",
            "mmlu_conceptual_physics",
            "mmlu_econometrics",
            "mmlu_electrical_engineering",
            "mmlu_elementary_mathematics",
            "mmlu_formal_logic",
            "mmlu_global_facts",
            "mmlu_high_school_biology",
            "mmlu_high_school_chemistry",
            "mmlu_high_school_computer_science",
            "mmlu_high_school_european_history",
            "mmlu_high_school_geography",
            "mmlu_high_school_government_and_politics",
            "mmlu_high_school_macroeconomics",
            "mmlu_high_school_mathematics",
            "mmlu_high_school_microeconomics",
            "mmlu_high_school_physics",
            "mmlu_high_school_psychology",
            "mmlu_high_school_statistics",
            "mmlu_high_school_us_history",
            "mmlu_high_school_world_history",
            "mmlu_human_aging",
            "mmlu_human_sexuality",
            "mmlu_international_law",
            "mmlu_jurisprudence",
            "mmlu_logical_fallacies",
            "mmlu_machine_learning",
            "mmlu_management",
            "mmlu_marketing",
            "mmlu_medical_genetics",
            "mmlu_miscellaneous",
            "mmlu_moral_disputes",
            "mmlu_moral_scenarios",
            "mmlu_nutrition",
            "mmlu_philosophy",
            "mmlu_prehistory",
            "mmlu_professional_accounting",
            "mmlu_professional_law",
            "mmlu_professional_medicine",
            "mmlu_professional_psychology",
            "mmlu_public_relations",
            "mmlu_security_studies",
            "mmlu_sociology",
            "mmlu_us_foreign_policy",
            "mmlu_virology",
            "mmlu_world_religions"
          ]
        }
      }
    },
    {
      "evaluation_name": "Abstract Algebra - EM",
      "source_data": {
        "dataset_name": "Abstract Algebra",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The abstract algebra subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.84,
        "details": {
          "description": "min=0.84, mean=0.84, max=0.84, sum=1.68 (2)",
          "tab": "Accuracy",
          "Abstract Algebra - Observed inference time (s)": {
            "description": "min=0.585, mean=0.585, max=0.585, sum=1.171 (2)",
            "tab": "Efficiency",
            "score": 0.5853858423233033
          },
          "Abstract Algebra - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Abstract Algebra - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Abstract Algebra - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Abstract Algebra - # prompt tokens": {
            "description": "min=373.01, mean=373.01, max=373.01, sum=746.02 (2)",
            "tab": "General information",
            "score": 373.01
          },
          "Abstract Algebra - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "abstract_algebra",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_abstract_algebra"
        }
      }
    },
    {
      "evaluation_name": "Anatomy - EM",
      "source_data": {
        "dataset_name": "Anatomy",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The anatomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.867,
        "details": {
          "description": "min=0.867, mean=0.867, max=0.867, sum=1.733 (2)",
          "tab": "Accuracy",
          "Anatomy - Observed inference time (s)": {
            "description": "min=1.804, mean=1.804, max=1.804, sum=3.607 (2)",
            "tab": "Efficiency",
            "score": 1.8037012683020697
          },
          "Anatomy - # eval": {
            "description": "min=135, mean=135, max=135, sum=270 (2)",
            "tab": "General information",
            "score": 135.0
          },
          "Anatomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Anatomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Anatomy - # prompt tokens": {
            "description": "min=332.119, mean=332.119, max=332.119, sum=664.237 (2)",
            "tab": "General information",
            "score": 332.1185185185185
          },
          "Anatomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "anatomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_anatomy"
        }
      }
    },
    {
      "evaluation_name": "College Physics - EM",
      "source_data": {
        "dataset_name": "College Physics",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The college physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.814,
        "details": {
          "description": "min=0.814, mean=0.814, max=0.814, sum=1.627 (2)",
          "tab": "Accuracy",
          "College Chemistry - Observed inference time (s)": {
            "description": "min=0.557, mean=0.557, max=0.557, sum=1.113 (2)",
            "tab": "Efficiency",
            "score": 0.5567307829856872
          },
          "College Biology - Observed inference time (s)": {
            "description": "min=0.776, mean=0.776, max=0.776, sum=1.553 (2)",
            "tab": "Efficiency",
            "score": 0.7763584835661782
          },
          "College Computer Science - Observed inference time (s)": {
            "description": "min=0.505, mean=0.505, max=0.505, sum=1.01 (2)",
            "tab": "Efficiency",
            "score": 0.5047655653953552
          },
          "College Mathematics - Observed inference time (s)": {
            "description": "min=0.495, mean=0.495, max=0.495, sum=0.989 (2)",
            "tab": "Efficiency",
            "score": 0.4945454502105713
          },
          "College Medicine - Observed inference time (s)": {
            "description": "min=1.811, mean=1.811, max=1.811, sum=3.623 (2)",
            "tab": "Efficiency",
            "score": 1.8114735322191535
          },
          "College Physics - Observed inference time (s)": {
            "description": "min=6.344, mean=6.344, max=6.344, sum=12.687 (2)",
            "tab": "Efficiency",
            "score": 6.343635446885052
          },
          "College Chemistry - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Chemistry - # prompt tokens": {
            "description": "min=541.32, mean=541.32, max=541.32, sum=1082.64 (2)",
            "tab": "General information",
            "score": 541.32
          },
          "College Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # eval": {
            "description": "min=144, mean=144, max=144, sum=288 (2)",
            "tab": "General information",
            "score": 144.0
          },
          "College Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Biology - # prompt tokens": {
            "description": "min=456.201, mean=456.201, max=456.201, sum=912.403 (2)",
            "tab": "General information",
            "score": 456.2013888888889
          },
          "College Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Computer Science - # prompt tokens": {
            "description": "min=828.34, mean=828.34, max=828.34, sum=1656.68 (2)",
            "tab": "General information",
            "score": 828.34
          },
          "College Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "College Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Mathematics - # prompt tokens": {
            "description": "min=592.74, mean=592.74, max=592.74, sum=1185.48 (2)",
            "tab": "General information",
            "score": 592.74
          },
          "College Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # eval": {
            "description": "min=173, mean=173, max=173, sum=346 (2)",
            "tab": "General information",
            "score": 173.0
          },
          "College Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Medicine - # prompt tokens": {
            "description": "min=486.971, mean=486.971, max=486.971, sum=973.942 (2)",
            "tab": "General information",
            "score": 486.97109826589593
          },
          "College Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # eval": {
            "description": "min=102, mean=102, max=102, sum=204 (2)",
            "tab": "General information",
            "score": 102.0
          },
          "College Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "College Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "College Physics - # prompt tokens": {
            "description": "min=492.804, mean=492.804, max=492.804, sum=985.608 (2)",
            "tab": "General information",
            "score": 492.80392156862746
          },
          "College Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "college_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_college_physics"
        }
      }
    },
    {
      "evaluation_name": "Computer Security - EM",
      "source_data": {
        "dataset_name": "Computer Security",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The computer security subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.86,
        "details": {
          "description": "min=0.86, mean=0.86, max=0.86, sum=1.72 (2)",
          "tab": "Accuracy",
          "Computer Security - Observed inference time (s)": {
            "description": "min=0.545, mean=0.545, max=0.545, sum=1.089 (2)",
            "tab": "Efficiency",
            "score": 0.5446710443496704
          },
          "Computer Security - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Computer Security - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Computer Security - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Computer Security - # prompt tokens": {
            "description": "min=375.06, mean=375.06, max=375.06, sum=750.12 (2)",
            "tab": "General information",
            "score": 375.06
          },
          "Computer Security - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "computer_security",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_computer_security"
        }
      }
    },
    {
      "evaluation_name": "Econometrics - EM",
      "source_data": {
        "dataset_name": "Econometrics",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The econometrics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.746,
        "details": {
          "description": "min=0.746, mean=0.746, max=0.746, sum=1.491 (2)",
          "tab": "Accuracy",
          "Econometrics - Observed inference time (s)": {
            "description": "min=0.554, mean=0.554, max=0.554, sum=1.107 (2)",
            "tab": "Efficiency",
            "score": 0.5537264849010267
          },
          "Econometrics - # eval": {
            "description": "min=114, mean=114, max=114, sum=228 (2)",
            "tab": "General information",
            "score": 114.0
          },
          "Econometrics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Econometrics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Econometrics - # prompt tokens": {
            "description": "min=613.535, mean=613.535, max=613.535, sum=1227.07 (2)",
            "tab": "General information",
            "score": 613.5350877192982
          },
          "Econometrics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "econometrics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_econometrics"
        }
      }
    },
    {
      "evaluation_name": "Global Facts - EM",
      "source_data": {
        "dataset_name": "Global Facts",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The global facts subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.68,
        "details": {
          "description": "min=0.68, mean=0.68, max=0.68, sum=1.36 (2)",
          "tab": "Accuracy",
          "Global Facts - Observed inference time (s)": {
            "description": "min=0.978, mean=0.978, max=0.978, sum=1.955 (2)",
            "tab": "Efficiency",
            "score": 0.9775782990455627
          },
          "Global Facts - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Global Facts - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Global Facts - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Global Facts - # prompt tokens": {
            "description": "min=398.63, mean=398.63, max=398.63, sum=797.26 (2)",
            "tab": "General information",
            "score": 398.63
          },
          "Global Facts - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "global_facts",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_global_facts"
        }
      }
    },
    {
      "evaluation_name": "Jurisprudence - EM",
      "source_data": {
        "dataset_name": "Jurisprudence",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The jurisprudence subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.898,
        "details": {
          "description": "min=0.898, mean=0.898, max=0.898, sum=1.796 (2)",
          "tab": "Accuracy",
          "Jurisprudence - Observed inference time (s)": {
            "description": "min=0.834, mean=0.834, max=0.834, sum=1.668 (2)",
            "tab": "Efficiency",
            "score": 0.8338986083313271
          },
          "Jurisprudence - # eval": {
            "description": "min=108, mean=108, max=108, sum=216 (2)",
            "tab": "General information",
            "score": 108.0
          },
          "Jurisprudence - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Jurisprudence - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Jurisprudence - # prompt tokens": {
            "description": "min=387.694, mean=387.694, max=387.694, sum=775.389 (2)",
            "tab": "General information",
            "score": 387.69444444444446
          },
          "Jurisprudence - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "jurisprudence",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_jurisprudence"
        }
      }
    },
    {
      "evaluation_name": "Philosophy - EM",
      "source_data": {
        "dataset_name": "Philosophy",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The philosophy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.9,
        "details": {
          "description": "min=0.9, mean=0.9, max=0.9, sum=1.801 (2)",
          "tab": "Accuracy",
          "Philosophy - Observed inference time (s)": {
            "description": "min=0.836, mean=0.836, max=0.836, sum=1.673 (2)",
            "tab": "Efficiency",
            "score": 0.836391413710125
          },
          "Philosophy - # eval": {
            "description": "min=311, mean=311, max=311, sum=622 (2)",
            "tab": "General information",
            "score": 311.0
          },
          "Philosophy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Philosophy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Philosophy - # prompt tokens": {
            "description": "min=323.569, mean=323.569, max=323.569, sum=647.138 (2)",
            "tab": "General information",
            "score": 323.56913183279744
          },
          "Philosophy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "philosophy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_philosophy"
        }
      }
    },
    {
      "evaluation_name": "Professional Psychology - EM",
      "source_data": {
        "dataset_name": "Professional Psychology",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The professional psychology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.887,
        "details": {
          "description": "min=0.887, mean=0.887, max=0.887, sum=1.775 (2)",
          "tab": "Accuracy",
          "Professional Medicine - Observed inference time (s)": {
            "description": "min=0.635, mean=0.635, max=0.635, sum=1.269 (2)",
            "tab": "Efficiency",
            "score": 0.6345776915550232
          },
          "Professional Accounting - Observed inference time (s)": {
            "description": "min=1.224, mean=1.224, max=1.224, sum=2.448 (2)",
            "tab": "Efficiency",
            "score": 1.2240875671941338
          },
          "Professional Law - Observed inference time (s)": {
            "description": "min=0.707, mean=0.707, max=0.707, sum=1.413 (2)",
            "tab": "Efficiency",
            "score": 0.7066206168941911
          },
          "Professional Psychology - Observed inference time (s)": {
            "description": "min=0.672, mean=0.672, max=0.672, sum=1.345 (2)",
            "tab": "Efficiency",
            "score": 0.6723053728053773
          },
          "Professional Medicine - # eval": {
            "description": "min=272, mean=272, max=272, sum=544 (2)",
            "tab": "General information",
            "score": 272.0
          },
          "Professional Medicine - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Medicine - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Medicine - # prompt tokens": {
            "description": "min=1052.765, mean=1052.765, max=1052.765, sum=2105.529 (2)",
            "tab": "General information",
            "score": 1052.764705882353
          },
          "Professional Medicine - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # eval": {
            "description": "min=282, mean=282, max=282, sum=564 (2)",
            "tab": "General information",
            "score": 282.0
          },
          "Professional Accounting - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Accounting - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Accounting - # prompt tokens": {
            "description": "min=659.613, mean=659.613, max=659.613, sum=1319.227 (2)",
            "tab": "General information",
            "score": 659.613475177305
          },
          "Professional Accounting - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # eval": {
            "description": "min=1534, mean=1534, max=1534, sum=3068 (2)",
            "tab": "General information",
            "score": 1534.0
          },
          "Professional Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Law - # prompt tokens": {
            "description": "min=1629.421, mean=1629.421, max=1629.421, sum=3258.842 (2)",
            "tab": "General information",
            "score": 1629.4211212516298
          },
          "Professional Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # eval": {
            "description": "min=612, mean=612, max=612, sum=1224 (2)",
            "tab": "General information",
            "score": 612.0
          },
          "Professional Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Professional Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Professional Psychology - # prompt tokens": {
            "description": "min=574.508, mean=574.508, max=574.508, sum=1149.016 (2)",
            "tab": "General information",
            "score": 574.5081699346405
          },
          "Professional Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "professional_psychology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_professional_psychology"
        }
      }
    },
    {
      "evaluation_name": "Us Foreign Policy - EM",
      "source_data": {
        "dataset_name": "Us Foreign Policy",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The us foreign policy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.92,
        "details": {
          "description": "min=0.92, mean=0.92, max=0.92, sum=1.84 (2)",
          "tab": "Accuracy",
          "Us Foreign Policy - Observed inference time (s)": {
            "description": "min=0.578, mean=0.578, max=0.578, sum=1.156 (2)",
            "tab": "Efficiency",
            "score": 0.5778071475028992
          },
          "Us Foreign Policy - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Us Foreign Policy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Us Foreign Policy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Us Foreign Policy - # prompt tokens": {
            "description": "min=426.43, mean=426.43, max=426.43, sum=852.86 (2)",
            "tab": "General information",
            "score": 426.43
          },
          "Us Foreign Policy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "us_foreign_policy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_us_foreign_policy"
        }
      }
    },
    {
      "evaluation_name": "Astronomy - EM",
      "source_data": {
        "dataset_name": "Astronomy",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The astronomy subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.921,
        "details": {
          "description": "min=0.921, mean=0.921, max=0.921, sum=1.842 (2)",
          "tab": "Accuracy",
          "Astronomy - Observed inference time (s)": {
            "description": "min=0.681, mean=0.681, max=0.681, sum=1.363 (2)",
            "tab": "Efficiency",
            "score": 0.6812541327978435
          },
          "Astronomy - # eval": {
            "description": "min=152, mean=152, max=152, sum=304 (2)",
            "tab": "General information",
            "score": 152.0
          },
          "Astronomy - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Astronomy - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Astronomy - # prompt tokens": {
            "description": "min=575.836, mean=575.836, max=575.836, sum=1151.671 (2)",
            "tab": "General information",
            "score": 575.8355263157895
          },
          "Astronomy - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "astronomy",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_astronomy"
        }
      }
    },
    {
      "evaluation_name": "Business Ethics - EM",
      "source_data": {
        "dataset_name": "Business Ethics",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The business ethics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.89,
        "details": {
          "description": "min=0.89, mean=0.89, max=0.89, sum=1.78 (2)",
          "tab": "Accuracy",
          "Business Ethics - Observed inference time (s)": {
            "description": "min=4.691, mean=4.691, max=4.691, sum=9.381 (2)",
            "tab": "Efficiency",
            "score": 4.690641319751739
          },
          "Business Ethics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Business Ethics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Business Ethics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Business Ethics - # prompt tokens": {
            "description": "min=550.46, mean=550.46, max=550.46, sum=1100.92 (2)",
            "tab": "General information",
            "score": 550.46
          },
          "Business Ethics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "business_ethics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_business_ethics"
        }
      }
    },
    {
      "evaluation_name": "Clinical Knowledge - EM",
      "source_data": {
        "dataset_name": "Clinical Knowledge",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The clinical knowledge subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.913,
        "details": {
          "description": "min=0.913, mean=0.913, max=0.913, sum=1.826 (2)",
          "tab": "Accuracy",
          "Clinical Knowledge - Observed inference time (s)": {
            "description": "min=0.906, mean=0.906, max=0.906, sum=1.812 (2)",
            "tab": "Efficiency",
            "score": 0.9061050837894655
          },
          "Clinical Knowledge - # eval": {
            "description": "min=265, mean=265, max=265, sum=530 (2)",
            "tab": "General information",
            "score": 265.0
          },
          "Clinical Knowledge - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Clinical Knowledge - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Clinical Knowledge - # prompt tokens": {
            "description": "min=387.449, mean=387.449, max=387.449, sum=774.898 (2)",
            "tab": "General information",
            "score": 387.4490566037736
          },
          "Clinical Knowledge - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "clinical_knowledge",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_clinical_knowledge"
        }
      }
    },
    {
      "evaluation_name": "Conceptual Physics - EM",
      "source_data": {
        "dataset_name": "Conceptual Physics",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The conceptual physics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.94,
        "details": {
          "description": "min=0.94, mean=0.94, max=0.94, sum=1.881 (2)",
          "tab": "Accuracy",
          "Conceptual Physics - Observed inference time (s)": {
            "description": "min=0.627, mean=0.627, max=0.627, sum=1.253 (2)",
            "tab": "Efficiency",
            "score": 0.6267383788494354
          },
          "Conceptual Physics - # eval": {
            "description": "min=235, mean=235, max=235, sum=470 (2)",
            "tab": "General information",
            "score": 235.0
          },
          "Conceptual Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Conceptual Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Conceptual Physics - # prompt tokens": {
            "description": "min=300.591, mean=300.591, max=300.591, sum=601.183 (2)",
            "tab": "General information",
            "score": 300.59148936170214
          },
          "Conceptual Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "conceptual_physics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_conceptual_physics"
        }
      }
    },
    {
      "evaluation_name": "Electrical Engineering - EM",
      "source_data": {
        "dataset_name": "Electrical Engineering",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The electrical engineering subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.869,
        "details": {
          "description": "min=0.869, mean=0.869, max=0.869, sum=1.738 (2)",
          "tab": "Accuracy",
          "Electrical Engineering - Observed inference time (s)": {
            "description": "min=2.459, mean=2.459, max=2.459, sum=4.918 (2)",
            "tab": "Efficiency",
            "score": 2.4591504623150002
          },
          "Electrical Engineering - # eval": {
            "description": "min=145, mean=145, max=145, sum=290 (2)",
            "tab": "General information",
            "score": 145.0
          },
          "Electrical Engineering - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Electrical Engineering - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Electrical Engineering - # prompt tokens": {
            "description": "min=431.91, mean=431.91, max=431.91, sum=863.821 (2)",
            "tab": "General information",
            "score": 431.9103448275862
          },
          "Electrical Engineering - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "electrical_engineering",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_electrical_engineering"
        }
      }
    },
    {
      "evaluation_name": "Elementary Mathematics - EM",
      "source_data": {
        "dataset_name": "Elementary Mathematics",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The elementary mathematics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.942,
        "details": {
          "description": "min=0.942, mean=0.942, max=0.942, sum=1.884 (2)",
          "tab": "Accuracy",
          "Elementary Mathematics - Observed inference time (s)": {
            "description": "min=1.651, mean=1.651, max=1.651, sum=3.301 (2)",
            "tab": "Efficiency",
            "score": 1.650515148879359
          },
          "Elementary Mathematics - # eval": {
            "description": "min=378, mean=378, max=378, sum=756 (2)",
            "tab": "General information",
            "score": 378.0
          },
          "Elementary Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Elementary Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Elementary Mathematics - # prompt tokens": {
            "description": "min=531.693, mean=531.693, max=531.693, sum=1063.386 (2)",
            "tab": "General information",
            "score": 531.6931216931217
          },
          "Elementary Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "elementary_mathematics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_elementary_mathematics"
        }
      }
    },
    {
      "evaluation_name": "Formal Logic - EM",
      "source_data": {
        "dataset_name": "Formal Logic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The formal logic subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.77,
        "details": {
          "description": "min=0.77, mean=0.77, max=0.77, sum=1.54 (2)",
          "tab": "Accuracy",
          "Formal Logic - Observed inference time (s)": {
            "description": "min=0.513, mean=0.513, max=0.513, sum=1.026 (2)",
            "tab": "Efficiency",
            "score": 0.5130742864003257
          },
          "Formal Logic - # eval": {
            "description": "min=126, mean=126, max=126, sum=252 (2)",
            "tab": "General information",
            "score": 126.0
          },
          "Formal Logic - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Formal Logic - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Formal Logic - # prompt tokens": {
            "description": "min=604.119, mean=604.119, max=604.119, sum=1208.238 (2)",
            "tab": "General information",
            "score": 604.1190476190476
          },
          "Formal Logic - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "formal_logic",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_formal_logic"
        }
      }
    },
    {
      "evaluation_name": "High School World History - EM",
      "source_data": {
        "dataset_name": "High School World History",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The high school world history subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.928,
        "details": {
          "description": "min=0.928, mean=0.928, max=0.928, sum=1.857 (2)",
          "tab": "Accuracy",
          "High School Biology - Observed inference time (s)": {
            "description": "min=2.647, mean=2.647, max=2.647, sum=5.294 (2)",
            "tab": "Efficiency",
            "score": 2.6472030393538937
          },
          "High School Chemistry - Observed inference time (s)": {
            "description": "min=3.847, mean=3.847, max=3.847, sum=7.695 (2)",
            "tab": "Efficiency",
            "score": 3.8474940337571018
          },
          "High School Computer Science - Observed inference time (s)": {
            "description": "min=2.761, mean=2.761, max=2.761, sum=5.523 (2)",
            "tab": "Efficiency",
            "score": 2.7613840389251707
          },
          "High School European History - Observed inference time (s)": {
            "description": "min=1.944, mean=1.944, max=1.944, sum=3.888 (2)",
            "tab": "Efficiency",
            "score": 1.9442455436244155
          },
          "High School Geography - Observed inference time (s)": {
            "description": "min=0.607, mean=0.607, max=0.607, sum=1.215 (2)",
            "tab": "Efficiency",
            "score": 0.6073213755482375
          },
          "High School Government And Politics - Observed inference time (s)": {
            "description": "min=2.403, mean=2.403, max=2.403, sum=4.805 (2)",
            "tab": "Efficiency",
            "score": 2.4025608480285485
          },
          "High School Macroeconomics - Observed inference time (s)": {
            "description": "min=0.654, mean=0.654, max=0.654, sum=1.308 (2)",
            "tab": "Efficiency",
            "score": 0.6539444972307255
          },
          "High School Mathematics - Observed inference time (s)": {
            "description": "min=2.285, mean=2.285, max=2.285, sum=4.57 (2)",
            "tab": "Efficiency",
            "score": 2.285083364557337
          },
          "High School Microeconomics - Observed inference time (s)": {
            "description": "min=1.265, mean=1.265, max=1.265, sum=2.531 (2)",
            "tab": "Efficiency",
            "score": 1.2653034544792496
          },
          "High School Physics - Observed inference time (s)": {
            "description": "min=1.036, mean=1.036, max=1.036, sum=2.072 (2)",
            "tab": "Efficiency",
            "score": 1.0361600064283965
          },
          "High School Psychology - Observed inference time (s)": {
            "description": "min=1.658, mean=1.658, max=1.658, sum=3.315 (2)",
            "tab": "Efficiency",
            "score": 1.6576398372650147
          },
          "High School Statistics - Observed inference time (s)": {
            "description": "min=0.513, mean=0.513, max=0.513, sum=1.027 (2)",
            "tab": "Efficiency",
            "score": 0.5133153398831686
          },
          "High School US History - Observed inference time (s)": {
            "description": "min=0.791, mean=0.791, max=0.791, sum=1.582 (2)",
            "tab": "Efficiency",
            "score": 0.7908881224837958
          },
          "High School World History - Observed inference time (s)": {
            "description": "min=1.65, mean=1.65, max=1.65, sum=3.301 (2)",
            "tab": "Efficiency",
            "score": 1.6504118030081318
          },
          "High School Biology - # eval": {
            "description": "min=310, mean=310, max=310, sum=620 (2)",
            "tab": "General information",
            "score": 310.0
          },
          "High School Biology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Biology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Biology - # prompt tokens": {
            "description": "min=505.561, mean=505.561, max=505.561, sum=1011.123 (2)",
            "tab": "General information",
            "score": 505.56129032258065
          },
          "High School Biology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # eval": {
            "description": "min=203, mean=203, max=203, sum=406 (2)",
            "tab": "General information",
            "score": 203.0
          },
          "High School Chemistry - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Chemistry - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Chemistry - # prompt tokens": {
            "description": "min=479.32, mean=479.32, max=479.32, sum=958.64 (2)",
            "tab": "General information",
            "score": 479.320197044335
          },
          "High School Chemistry - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "High School Computer Science - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Computer Science - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Computer Science - # prompt tokens": {
            "description": "min=871.42, mean=871.42, max=871.42, sum=1742.84 (2)",
            "tab": "General information",
            "score": 871.42
          },
          "High School Computer Science - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # eval": {
            "description": "min=165, mean=165, max=165, sum=330 (2)",
            "tab": "General information",
            "score": 165.0
          },
          "High School European History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School European History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School European History - # prompt tokens": {
            "description": "min=2773.188, mean=2773.188, max=2773.188, sum=5546.376 (2)",
            "tab": "General information",
            "score": 2773.1878787878786
          },
          "High School European History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # eval": {
            "description": "min=198, mean=198, max=198, sum=396 (2)",
            "tab": "General information",
            "score": 198.0
          },
          "High School Geography - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Geography - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Geography - # prompt tokens": {
            "description": "min=369.53, mean=369.53, max=369.53, sum=739.061 (2)",
            "tab": "General information",
            "score": 369.530303030303
          },
          "High School Geography - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # eval": {
            "description": "min=193, mean=193, max=193, sum=386 (2)",
            "tab": "General information",
            "score": 193.0
          },
          "High School Government And Politics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Government And Politics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Government And Politics - # prompt tokens": {
            "description": "min=463.767, mean=463.767, max=463.767, sum=927.534 (2)",
            "tab": "General information",
            "score": 463.76683937823833
          },
          "High School Government And Politics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # eval": {
            "description": "min=390, mean=390, max=390, sum=780 (2)",
            "tab": "General information",
            "score": 390.0
          },
          "High School Macroeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Macroeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Macroeconomics - # prompt tokens": {
            "description": "min=370.418, mean=370.418, max=370.418, sum=740.836 (2)",
            "tab": "General information",
            "score": 370.4179487179487
          },
          "High School Macroeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # eval": {
            "description": "min=270, mean=270, max=270, sum=540 (2)",
            "tab": "General information",
            "score": 270.0
          },
          "High School Mathematics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Mathematics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Mathematics - # prompt tokens": {
            "description": "min=520.57, mean=520.57, max=520.57, sum=1041.141 (2)",
            "tab": "General information",
            "score": 520.5703703703704
          },
          "High School Mathematics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # eval": {
            "description": "min=238, mean=238, max=238, sum=476 (2)",
            "tab": "General information",
            "score": 238.0
          },
          "High School Microeconomics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Microeconomics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Microeconomics - # prompt tokens": {
            "description": "min=399.782, mean=399.782, max=399.782, sum=799.563 (2)",
            "tab": "General information",
            "score": 399.781512605042
          },
          "High School Microeconomics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # eval": {
            "description": "min=151, mean=151, max=151, sum=302 (2)",
            "tab": "General information",
            "score": 151.0
          },
          "High School Physics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Physics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Physics - # prompt tokens": {
            "description": "min=559.967, mean=559.967, max=559.967, sum=1119.934 (2)",
            "tab": "General information",
            "score": 559.9668874172186
          },
          "High School Physics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # eval": {
            "description": "min=545, mean=545, max=545, sum=1090 (2)",
            "tab": "General information",
            "score": 545.0
          },
          "High School Psychology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Psychology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Psychology - # prompt tokens": {
            "description": "min=480.22, mean=480.22, max=480.22, sum=960.44 (2)",
            "tab": "General information",
            "score": 480.2201834862385
          },
          "High School Psychology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # eval": {
            "description": "min=216, mean=216, max=216, sum=432 (2)",
            "tab": "General information",
            "score": 216.0
          },
          "High School Statistics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School Statistics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School Statistics - # prompt tokens": {
            "description": "min=796.333, mean=796.333, max=796.333, sum=1592.667 (2)",
            "tab": "General information",
            "score": 796.3333333333334
          },
          "High School Statistics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # eval": {
            "description": "min=204, mean=204, max=204, sum=408 (2)",
            "tab": "General information",
            "score": 204.0
          },
          "High School US History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School US History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School US History - # prompt tokens": {
            "description": "min=2202.103, mean=2202.103, max=2202.103, sum=4404.206 (2)",
            "tab": "General information",
            "score": 2202.1029411764707
          },
          "High School US History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # eval": {
            "description": "min=237, mean=237, max=237, sum=474 (2)",
            "tab": "General information",
            "score": 237.0
          },
          "High School World History - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "High School World History - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "High School World History - # prompt tokens": {
            "description": "min=1403.051, mean=1403.051, max=1403.051, sum=2806.101 (2)",
            "tab": "General information",
            "score": 1403.0506329113923
          },
          "High School World History - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "high_school_world_history",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_high_school_world_history"
        }
      }
    },
    {
      "evaluation_name": "Human Sexuality - EM",
      "source_data": {
        "dataset_name": "Human Sexuality",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The human sexuality subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.924,
        "details": {
          "description": "min=0.924, mean=0.924, max=0.924, sum=1.847 (2)",
          "tab": "Accuracy",
          "Human Aging - Observed inference time (s)": {
            "description": "min=0.553, mean=0.553, max=0.553, sum=1.106 (2)",
            "tab": "Efficiency",
            "score": 0.5531257503235821
          },
          "Human Sexuality - Observed inference time (s)": {
            "description": "min=0.511, mean=0.511, max=0.511, sum=1.022 (2)",
            "tab": "Efficiency",
            "score": 0.5109815524734613
          },
          "Human Aging - # eval": {
            "description": "min=223, mean=223, max=223, sum=446 (2)",
            "tab": "General information",
            "score": 223.0
          },
          "Human Aging - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Aging - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Aging - # prompt tokens": {
            "description": "min=315.26, mean=315.26, max=315.26, sum=630.52 (2)",
            "tab": "General information",
            "score": 315.26008968609864
          },
          "Human Aging - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # eval": {
            "description": "min=131, mean=131, max=131, sum=262 (2)",
            "tab": "General information",
            "score": 131.0
          },
          "Human Sexuality - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Human Sexuality - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Human Sexuality - # prompt tokens": {
            "description": "min=341.29, mean=341.29, max=341.29, sum=682.58 (2)",
            "tab": "General information",
            "score": 341.29007633587787
          },
          "Human Sexuality - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "human_sexuality",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_human_sexuality"
        }
      }
    },
    {
      "evaluation_name": "International Law - EM",
      "source_data": {
        "dataset_name": "International Law",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The international law subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.95,
        "details": {
          "description": "min=0.95, mean=0.95, max=0.95, sum=1.901 (2)",
          "tab": "Accuracy",
          "International Law - Observed inference time (s)": {
            "description": "min=0.886, mean=0.886, max=0.886, sum=1.772 (2)",
            "tab": "Efficiency",
            "score": 0.8861682651456723
          },
          "International Law - # eval": {
            "description": "min=121, mean=121, max=121, sum=242 (2)",
            "tab": "General information",
            "score": 121.0
          },
          "International Law - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "International Law - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "International Law - # prompt tokens": {
            "description": "min=639.339, mean=639.339, max=639.339, sum=1278.678 (2)",
            "tab": "General information",
            "score": 639.3388429752066
          },
          "International Law - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "international_law",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_international_law"
        }
      }
    },
    {
      "evaluation_name": "Logical Fallacies - EM",
      "source_data": {
        "dataset_name": "Logical Fallacies",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The logical fallacies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.914,
        "details": {
          "description": "min=0.914, mean=0.914, max=0.914, sum=1.828 (2)",
          "tab": "Accuracy",
          "Logical Fallacies - Observed inference time (s)": {
            "description": "min=0.919, mean=0.919, max=0.919, sum=1.838 (2)",
            "tab": "Efficiency",
            "score": 0.9191862732354849
          },
          "Logical Fallacies - # eval": {
            "description": "min=163, mean=163, max=163, sum=326 (2)",
            "tab": "General information",
            "score": 163.0
          },
          "Logical Fallacies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Logical Fallacies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Logical Fallacies - # prompt tokens": {
            "description": "min=442.239, mean=442.239, max=442.239, sum=884.479 (2)",
            "tab": "General information",
            "score": 442.23926380368096
          },
          "Logical Fallacies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "logical_fallacies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_logical_fallacies"
        }
      }
    },
    {
      "evaluation_name": "Machine Learning - EM",
      "source_data": {
        "dataset_name": "Machine Learning",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The machine learning subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.786,
        "details": {
          "description": "min=0.786, mean=0.786, max=0.786, sum=1.571 (2)",
          "tab": "Accuracy",
          "Machine Learning - Observed inference time (s)": {
            "description": "min=0.518, mean=0.518, max=0.518, sum=1.036 (2)",
            "tab": "Efficiency",
            "score": 0.5179938631398338
          },
          "Machine Learning - # eval": {
            "description": "min=112, mean=112, max=112, sum=224 (2)",
            "tab": "General information",
            "score": 112.0
          },
          "Machine Learning - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Machine Learning - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Machine Learning - # prompt tokens": {
            "description": "min=666.277, mean=666.277, max=666.277, sum=1332.554 (2)",
            "tab": "General information",
            "score": 666.2767857142857
          },
          "Machine Learning - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "machine_learning",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_machine_learning"
        }
      }
    },
    {
      "evaluation_name": "Management - EM",
      "source_data": {
        "dataset_name": "Management",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The management subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.903,
        "details": {
          "description": "min=0.903, mean=0.903, max=0.903, sum=1.806 (2)",
          "tab": "Accuracy",
          "Management - Observed inference time (s)": {
            "description": "min=4.248, mean=4.248, max=4.248, sum=8.497 (2)",
            "tab": "Efficiency",
            "score": 4.248399836345784
          },
          "Management - # eval": {
            "description": "min=103, mean=103, max=103, sum=206 (2)",
            "tab": "General information",
            "score": 103.0
          },
          "Management - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Management - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Management - # prompt tokens": {
            "description": "min=277.379, mean=277.379, max=277.379, sum=554.757 (2)",
            "tab": "General information",
            "score": 277.378640776699
          },
          "Management - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "management",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_management"
        }
      }
    },
    {
      "evaluation_name": "Marketing - EM",
      "source_data": {
        "dataset_name": "Marketing",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The marketing subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.949,
        "details": {
          "description": "min=0.949, mean=0.949, max=0.949, sum=1.897 (2)",
          "tab": "Accuracy",
          "Marketing - Observed inference time (s)": {
            "description": "min=1.645, mean=1.645, max=1.645, sum=3.29 (2)",
            "tab": "Efficiency",
            "score": 1.6448312304977677
          },
          "Marketing - # eval": {
            "description": "min=234, mean=234, max=234, sum=468 (2)",
            "tab": "General information",
            "score": 234.0
          },
          "Marketing - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Marketing - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Marketing - # prompt tokens": {
            "description": "min=398.675, mean=398.675, max=398.675, sum=797.35 (2)",
            "tab": "General information",
            "score": 398.6752136752137
          },
          "Marketing - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "marketing",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_marketing"
        }
      }
    },
    {
      "evaluation_name": "Medical Genetics - EM",
      "source_data": {
        "dataset_name": "Medical Genetics",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The medical genetics subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.96,
        "details": {
          "description": "min=0.96, mean=0.96, max=0.96, sum=1.92 (2)",
          "tab": "Accuracy",
          "Medical Genetics - Observed inference time (s)": {
            "description": "min=0.527, mean=0.527, max=0.527, sum=1.054 (2)",
            "tab": "Efficiency",
            "score": 0.5272433400154114
          },
          "Medical Genetics - # eval": {
            "description": "min=100, mean=100, max=100, sum=200 (2)",
            "tab": "General information",
            "score": 100.0
          },
          "Medical Genetics - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Medical Genetics - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Medical Genetics - # prompt tokens": {
            "description": "min=328.48, mean=328.48, max=328.48, sum=656.96 (2)",
            "tab": "General information",
            "score": 328.48
          },
          "Medical Genetics - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "medical_genetics",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_medical_genetics"
        }
      }
    },
    {
      "evaluation_name": "Miscellaneous - EM",
      "source_data": {
        "dataset_name": "Miscellaneous",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The miscellaneous subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.949,
        "details": {
          "description": "min=0.949, mean=0.949, max=0.949, sum=1.898 (2)",
          "tab": "Accuracy",
          "Miscellaneous - Observed inference time (s)": {
            "description": "min=2.642, mean=2.642, max=2.642, sum=5.284 (2)",
            "tab": "Efficiency",
            "score": 2.6419809954681006
          },
          "Miscellaneous - # eval": {
            "description": "min=783, mean=783, max=783, sum=1566 (2)",
            "tab": "General information",
            "score": 783.0
          },
          "Miscellaneous - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Miscellaneous - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Miscellaneous - # prompt tokens": {
            "description": "min=296.626, mean=296.626, max=296.626, sum=593.252 (2)",
            "tab": "General information",
            "score": 296.6257982120051
          },
          "Miscellaneous - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "miscellaneous",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_miscellaneous"
        }
      }
    },
    {
      "evaluation_name": "Moral Scenarios - EM",
      "source_data": {
        "dataset_name": "Moral Scenarios",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The moral scenarios subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.808,
        "details": {
          "description": "min=0.808, mean=0.808, max=0.808, sum=1.616 (2)",
          "tab": "Accuracy",
          "Moral Disputes - Observed inference time (s)": {
            "description": "min=0.637, mean=0.637, max=0.637, sum=1.275 (2)",
            "tab": "Efficiency",
            "score": 0.6374224183187319
          },
          "Moral Scenarios - Observed inference time (s)": {
            "description": "min=0.624, mean=0.624, max=0.624, sum=1.247 (2)",
            "tab": "Efficiency",
            "score": 0.6235519771469372
          },
          "Moral Disputes - # eval": {
            "description": "min=346, mean=346, max=346, sum=692 (2)",
            "tab": "General information",
            "score": 346.0
          },
          "Moral Disputes - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Disputes - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Disputes - # prompt tokens": {
            "description": "min=477.78, mean=477.78, max=477.78, sum=955.561 (2)",
            "tab": "General information",
            "score": 477.78034682080926
          },
          "Moral Disputes - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # eval": {
            "description": "min=895, mean=895, max=895, sum=1790 (2)",
            "tab": "General information",
            "score": 895.0
          },
          "Moral Scenarios - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Moral Scenarios - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Moral Scenarios - # prompt tokens": {
            "description": "min=662.517, mean=662.517, max=662.517, sum=1325.035 (2)",
            "tab": "General information",
            "score": 662.5173184357542
          },
          "Moral Scenarios - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "moral_scenarios",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_moral_scenarios"
        }
      }
    },
    {
      "evaluation_name": "Nutrition - EM",
      "source_data": {
        "dataset_name": "Nutrition",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The nutrition subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.918,
        "details": {
          "description": "min=0.918, mean=0.918, max=0.918, sum=1.837 (2)",
          "tab": "Accuracy",
          "Nutrition - Observed inference time (s)": {
            "description": "min=1.989, mean=1.989, max=1.989, sum=3.977 (2)",
            "tab": "Efficiency",
            "score": 1.9886824734070723
          },
          "Nutrition - # eval": {
            "description": "min=306, mean=306, max=306, sum=612 (2)",
            "tab": "General information",
            "score": 306.0
          },
          "Nutrition - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Nutrition - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Nutrition - # prompt tokens": {
            "description": "min=570.337, mean=570.337, max=570.337, sum=1140.673 (2)",
            "tab": "General information",
            "score": 570.3366013071895
          },
          "Nutrition - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "nutrition",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_nutrition"
        }
      }
    },
    {
      "evaluation_name": "Prehistory - EM",
      "source_data": {
        "dataset_name": "Prehistory",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The prehistory subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.923,
        "details": {
          "description": "min=0.923, mean=0.923, max=0.923, sum=1.846 (2)",
          "tab": "Accuracy",
          "Prehistory - Observed inference time (s)": {
            "description": "min=0.909, mean=0.909, max=0.909, sum=1.819 (2)",
            "tab": "Efficiency",
            "score": 0.9094557386857492
          },
          "Prehistory - # eval": {
            "description": "min=324, mean=324, max=324, sum=648 (2)",
            "tab": "General information",
            "score": 324.0
          },
          "Prehistory - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Prehistory - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Prehistory - # prompt tokens": {
            "description": "min=505.194, mean=505.194, max=505.194, sum=1010.389 (2)",
            "tab": "General information",
            "score": 505.19444444444446
          },
          "Prehistory - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "prehistory",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_prehistory"
        }
      }
    },
    {
      "evaluation_name": "Public Relations - EM",
      "source_data": {
        "dataset_name": "Public Relations",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The public relations subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.809,
        "details": {
          "description": "min=0.809, mean=0.809, max=0.809, sum=1.618 (2)",
          "tab": "Accuracy",
          "Public Relations - Observed inference time (s)": {
            "description": "min=0.847, mean=0.847, max=0.847, sum=1.695 (2)",
            "tab": "Efficiency",
            "score": 0.8472580974752253
          },
          "Public Relations - # eval": {
            "description": "min=110, mean=110, max=110, sum=220 (2)",
            "tab": "General information",
            "score": 110.0
          },
          "Public Relations - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Public Relations - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Public Relations - # prompt tokens": {
            "description": "min=402.009, mean=402.009, max=402.009, sum=804.018 (2)",
            "tab": "General information",
            "score": 402.0090909090909
          },
          "Public Relations - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "public_relations",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_public_relations"
        }
      }
    },
    {
      "evaluation_name": "Security Studies - EM",
      "source_data": {
        "dataset_name": "Security Studies",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The security studies subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.837,
        "details": {
          "description": "min=0.837, mean=0.837, max=0.837, sum=1.673 (2)",
          "tab": "Accuracy",
          "Security Studies - Observed inference time (s)": {
            "description": "min=0.659, mean=0.659, max=0.659, sum=1.318 (2)",
            "tab": "Efficiency",
            "score": 0.6588058092156235
          },
          "Security Studies - # eval": {
            "description": "min=245, mean=245, max=245, sum=490 (2)",
            "tab": "General information",
            "score": 245.0
          },
          "Security Studies - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Security Studies - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Security Studies - # prompt tokens": {
            "description": "min=1160.294, mean=1160.294, max=1160.294, sum=2320.588 (2)",
            "tab": "General information",
            "score": 1160.2938775510204
          },
          "Security Studies - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "security_studies",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_security_studies"
        }
      }
    },
    {
      "evaluation_name": "Sociology - EM",
      "source_data": {
        "dataset_name": "Sociology",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The sociology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.955,
        "details": {
          "description": "min=0.955, mean=0.955, max=0.955, sum=1.91 (2)",
          "tab": "Accuracy",
          "Sociology - Observed inference time (s)": {
            "description": "min=1.251, mean=1.251, max=1.251, sum=2.501 (2)",
            "tab": "Efficiency",
            "score": 1.2506972652169603
          },
          "Sociology - # eval": {
            "description": "min=201, mean=201, max=201, sum=402 (2)",
            "tab": "General information",
            "score": 201.0
          },
          "Sociology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Sociology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Sociology - # prompt tokens": {
            "description": "min=443.891, mean=443.891, max=443.891, sum=887.781 (2)",
            "tab": "General information",
            "score": 443.8905472636816
          },
          "Sociology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "sociology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_sociology"
        }
      }
    },
    {
      "evaluation_name": "Virology - EM",
      "source_data": {
        "dataset_name": "Virology",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The virology subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.596,
        "details": {
          "description": "min=0.596, mean=0.596, max=0.596, sum=1.193 (2)",
          "tab": "Accuracy",
          "Virology - Observed inference time (s)": {
            "description": "min=0.509, mean=0.509, max=0.509, sum=1.019 (2)",
            "tab": "Efficiency",
            "score": 0.5092598558908485
          },
          "Virology - # eval": {
            "description": "min=166, mean=166, max=166, sum=332 (2)",
            "tab": "General information",
            "score": 166.0
          },
          "Virology - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "Virology - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "Virology - # prompt tokens": {
            "description": "min=329.572, mean=329.572, max=329.572, sum=659.145 (2)",
            "tab": "General information",
            "score": 329.5722891566265
          },
          "Virology - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "virology",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_virology"
        }
      }
    },
    {
      "evaluation_name": "World Religions - EM",
      "source_data": {
        "dataset_name": "World Religions",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The world religions subject in the Massive Multitask Language Understanding (MMLU) benchmark.\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.912,
        "details": {
          "description": "min=0.912, mean=0.912, max=0.912, sum=1.825 (2)",
          "tab": "Accuracy",
          "World Religions - Observed inference time (s)": {
            "description": "min=1.251, mean=1.251, max=1.251, sum=2.501 (2)",
            "tab": "Efficiency",
            "score": 1.2507223441586857
          },
          "World Religions - # eval": {
            "description": "min=171, mean=171, max=171, sum=342 (2)",
            "tab": "General information",
            "score": 171.0
          },
          "World Religions - # train": {
            "description": "min=5, mean=5, max=5, sum=10 (2)",
            "tab": "General information",
            "score": 5.0
          },
          "World Religions - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          },
          "World Religions - # prompt tokens": {
            "description": "min=268.918, mean=268.918, max=268.918, sum=537.836 (2)",
            "tab": "General information",
            "score": 268.91812865497076
          },
          "World Religions - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (2)",
            "tab": "General information",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": "world_religions",
          "method": "multiple_choice_joint",
          "eval_split": "test",
          "groups": "mmlu_world_religions"
        }
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_mmlu",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/mmlu/benchmark_output/releases/v1.13.0/groups/mmlu_subjects.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.215,
        "details": {
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}