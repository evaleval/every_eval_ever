{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/aleph-alpha_Luminous-Extended-30B/1770830385.7573261",
  "retrieved_timestamp": "1770830385.7573261",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Luminous Extended 30B",
    "id": "aleph-alpha/Luminous-Extended-30B",
    "developer": "aleph-alpha",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.485,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.5765957446808511
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.42993006993006994
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.45142191142191146
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.629471974916769
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.7191265524598858
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.5657894736842105
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.321,
        "details": {
          "description": "min=0.23, mean=0.321, max=0.49, sum=4.811 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.075, mean=0.135, max=0.225, sum=2.023 (15)",
            "tab": "Calibration",
            "score": 0.1348564339845485
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.1, mean=0.23, max=0.37, sum=3.451 (15)",
            "tab": "Robustness",
            "score": 0.23008187134502922
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.14, mean=0.237, max=0.35, sum=3.549 (15)",
            "tab": "Fairness",
            "score": 0.23658479532163745
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=360.75, mean=471.075, max=618.447, sum=7066.132 (15)",
            "tab": "General information",
            "score": 471.0754736842105
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ - EM",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.767,
        "details": {
          "description": "min=0.752, mean=0.767, max=0.794, sum=2.3 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.11, mean=0.129, max=0.154, sum=0.387 (3)",
            "tab": "Calibration",
            "score": 0.1289354797828563
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.637, mean=0.659, max=0.7, sum=1.976 (3)",
            "tab": "Robustness",
            "score": 0.6586666666666666
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.692, mean=0.711, max=0.733, sum=2.133 (3)",
            "tab": "Fairness",
            "score": 0.711
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=651.658, mean=908.991, max=1252.658, sum=2726.974 (3)",
            "tab": "General information",
            "score": 908.9913333333333
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.665,
        "details": {
          "description": "min=0.637, mean=0.665, max=0.684, sum=1.994 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.043, mean=0.046, max=0.047, sum=0.138 (3)",
            "tab": "Calibration",
            "score": 0.046063826868188405
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.481, mean=0.513, max=0.539, sum=1.54 (3)",
            "tab": "Robustness",
            "score": 0.513450295883327
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.503, mean=0.532, max=0.565, sum=1.597 (3)",
            "tab": "Fairness",
            "score": 0.5321907426131639
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.039, mean=1.621, max=2.037, sum=4.862 (3)",
            "tab": "General information",
            "score": 1.6206572769953052
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1606.952, mean=1647.783, max=1694.642, sum=4943.349 (3)",
            "tab": "General information",
            "score": 1647.783098591549
          },
          "NarrativeQA - # output tokens": {
            "description": "min=6.321, mean=7.042, max=8.175, sum=21.127 (3)",
            "tab": "General information",
            "score": 7.04225352112676
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.4, mean=0.416, max=0.44, sum=1.248 (3)",
            "tab": "Bias",
            "score": 0.4159611992945326
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.333, mean=0.556, max=0.667, sum=1.667 (3)",
            "tab": "Bias",
            "score": 0.5555555555555557
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.186, mean=0.199, max=0.207, sum=0.598 (3)",
            "tab": "Bias",
            "score": 0.19931611685099856
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.014, mean=0.017, max=0.02, sum=0.051 (3)",
            "tab": "Toxicity",
            "score": 0.016901408450704227
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.609,
        "details": {
          "description": "min=0.606, mean=0.609, max=0.611, sum=1.827 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.018, mean=0.022, max=0.024, sum=0.065 (3)",
            "tab": "Calibration",
            "score": 0.02157162838647707
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.08, mean=0.09, max=0.095, sum=0.269 (3)",
            "tab": "Calibration",
            "score": 0.08979897901208977
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.205, mean=0.212, max=0.218, sum=0.635 (3)",
            "tab": "Robustness",
            "score": 0.211552896733343
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.515, mean=0.524, max=0.537, sum=1.572 (3)",
            "tab": "Robustness",
            "score": 0.5239378524073847
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.205, mean=0.214, max=0.22, sum=0.642 (3)",
            "tab": "Fairness",
            "score": 0.21385439000180537
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.548, mean=0.551, max=0.554, sum=1.654 (3)",
            "tab": "Fairness",
            "score": 0.5512241821510145
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.087, mean=111.754, max=116.087, sum=335.261 (3)",
            "tab": "General information",
            "score": 111.75366666666667
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=5.508, mean=6.119, max=6.869, sum=18.356 (3)",
            "tab": "General information",
            "score": 6.118666666666666
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.691, mean=4.711, max=4.726, sum=14.134 (3)",
            "tab": "General information",
            "score": 4.711333333333333
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.038, mean=0.039, max=0.04, sum=0.116 (3)",
            "tab": "General information",
            "score": 0.03866666666666666
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1224.733, mean=1384.565, max=1488.14, sum=4153.695 (3)",
            "tab": "General information",
            "score": 1384.5649999999998
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=8.216, mean=10.3, max=11.913, sum=30.9 (3)",
            "tab": "General information",
            "score": 10.299999999999999
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.379, mean=0.46, max=0.5, sum=1.379 (3)",
            "tab": "Bias",
            "score": 0.4597701149425288
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.414, mean=0.435, max=0.447, sum=1.304 (3)",
            "tab": "Bias",
            "score": 0.43455385345385017
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.15, mean=0.223, max=0.269, sum=0.669 (3)",
            "tab": "Bias",
            "score": 0.2230769230769231
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.4, mean=0.411, max=0.433, sum=1.233 (3)",
            "tab": "Bias",
            "score": 0.41111111111111115
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.421, mean=0.441, max=0.477, sum=1.324 (3)",
            "tab": "Bias",
            "score": 0.44143286168772855
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.022, mean=0.045, max=0.082, sum=0.135 (3)",
            "tab": "Bias",
            "score": 0.04515740195666192
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC - F1",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.349,
        "details": {
          "description": "min=0.34, mean=0.349, max=0.363, sum=1.047 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.081, mean=0.096, max=0.116, sum=0.287 (3)",
            "tab": "Calibration",
            "score": 0.09561324552236967
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.188, mean=0.193, max=0.201, sum=0.578 (3)",
            "tab": "Robustness",
            "score": 0.1926796273359054
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.268, mean=0.277, max=0.295, sum=0.832 (3)",
            "tab": "Fairness",
            "score": 0.2774375608495023
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.84, mean=0.909, max=0.991, sum=2.727 (3)",
            "tab": "General information",
            "score": 0.9089999999999999
          },
          "QuAC - truncated": {
            "description": "min=0.029, mean=0.033, max=0.037, sum=0.098 (3)",
            "tab": "General information",
            "score": 0.03266666666666667
          },
          "QuAC - # prompt tokens": {
            "description": "min=1596.904, mean=1641.256, max=1672.92, sum=4923.768 (3)",
            "tab": "General information",
            "score": 1641.256
          },
          "QuAC - # output tokens": {
            "description": "min=20.299, mean=21.144, max=22.408, sum=63.432 (3)",
            "tab": "General information",
            "score": 21.144000000000002
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.59, mean=0.612, max=0.636, sum=1.837 (3)",
            "tab": "Bias",
            "score": 0.6124061124061125
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.382, mean=0.403, max=0.421, sum=1.208 (3)",
            "tab": "Bias",
            "score": 0.40276421801932005
          },
          "QuAC - Representation (race)": {
            "description": "min=0.202, mean=0.24, max=0.259, sum=0.719 (3)",
            "tab": "Bias",
            "score": 0.23980711859954595
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.194, mean=0.2, max=0.205, sum=0.601 (3)",
            "tab": "Bias",
            "score": 0.20029662396768255
          },
          "QuAC - Toxic fraction": {
            "description": "min=0, mean=0.0, max=0.001, sum=0.001 (3)",
            "tab": "Toxicity",
            "score": 0.0003333333333333333
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "HellaSwag - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.221,
        "details": {
          "description": "min=0.208, mean=0.221, max=0.231, sum=0.662 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.057, mean=0.064, max=0.068, sum=0.192 (3)",
            "tab": "Calibration",
            "score": 0.0641638452052097
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.139, mean=0.151, max=0.161, sum=0.454 (3)",
            "tab": "Robustness",
            "score": 0.15137614678899083
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.144, mean=0.16, max=0.171, sum=0.479 (3)",
            "tab": "Fairness",
            "score": 0.15953109072375127
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=504.073, mean=514.073, max=533.073, sum=1542.22 (3)",
            "tab": "General information",
            "score": 514.0733944954128
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.139,
        "details": {
          "description": "min=0.117, mean=0.139, max=0.15, sum=0.834 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1564.648, mean=1578.648, max=1593.648, sum=9471.888 (6)",
            "tab": "General information",
            "score": 1578.648068669528
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=73.322, mean=83.112, max=88.178, sum=498.674 (6)",
            "tab": "General information",
            "score": 83.11230329041489
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.58, mean=0.608, max=0.637, sum=3.651 (6)",
            "tab": "Bias",
            "score": 0.6084787955510622
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.382, mean=0.391, max=0.398, sum=2.347 (6)",
            "tab": "Bias",
            "score": 0.3911797965697547
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.254, mean=0.274, max=0.288, sum=1.642 (6)",
            "tab": "Bias",
            "score": 0.27361254875467617
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.128, mean=0.151, max=0.191, sum=0.909 (6)",
            "tab": "Bias",
            "score": 0.15142644383010628
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.309, mean=0.481, max=0.569, sum=1.443 (3)",
            "tab": "Summarization metrics",
            "score": 0.4809362133230566
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.202, mean=0.255, max=0.288, sum=0.766 (3)",
            "tab": "Summarization metrics",
            "score": 0.25521962437955664
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.8, mean=0.925, max=0.989, sum=5.552 (6)",
            "tab": "Summarization metrics",
            "score": 0.9253891304300669
          },
          "CNN/DailyMail - Density": {
            "description": "min=34.945, mean=41.619, max=45.552, sum=249.715 (6)",
            "tab": "Summarization metrics",
            "score": 41.61911540769457
          },
          "CNN/DailyMail - Compression": {
            "description": "min=8.478, mean=9.039, max=9.909, sum=54.236 (6)",
            "tab": "Summarization metrics",
            "score": 9.039273431117751
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.124,
        "details": {
          "description": "min=0.122, mean=0.124, max=0.126, sum=0.742 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.998, mean=4.999, max=5, sum=29.992 (6)",
            "tab": "General information",
            "score": 4.998712998712999
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1472.903, mean=1532.912, max=1566.407, sum=9197.471 (6)",
            "tab": "General information",
            "score": 1532.9118404118406
          },
          "XSUM - # output tokens": {
            "description": "min=25.747, mean=25.987, max=26.212, sum=155.923 (6)",
            "tab": "General information",
            "score": 25.987129987129986
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.449, mean=0.45, max=0.451, sum=2.701 (6)",
            "tab": "Bias",
            "score": 0.450224364113253
          },
          "XSUM - Representation (race)": {
            "description": "min=0.532, mean=0.547, max=0.565, sum=3.282 (6)",
            "tab": "Bias",
            "score": 0.5469576096753798
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.212, mean=0.214, max=0.217, sum=1.283 (6)",
            "tab": "Bias",
            "score": 0.2138886962661304
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.002, max=0.004, sum=0.012 (6)",
            "tab": "Toxicity",
            "score": 0.0019305019305019308
          },
          "XSUM - SummaC": {
            "description": "min=-0.233, mean=-0.225, max=-0.212, sum=-0.675 (3)",
            "tab": "Summarization metrics",
            "score": -0.22500232932190178
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.419, mean=0.423, max=0.427, sum=1.269 (3)",
            "tab": "Summarization metrics",
            "score": 0.4230439766625391
          },
          "XSUM - Coverage": {
            "description": "min=0.817, mean=0.818, max=0.819, sum=4.91 (6)",
            "tab": "Summarization metrics",
            "score": 0.8184154242425056
          },
          "XSUM - Density": {
            "description": "min=3.392, mean=3.507, max=3.668, sum=21.042 (6)",
            "tab": "Summarization metrics",
            "score": 3.507010978728374
          },
          "XSUM - Compression": {
            "description": "min=17.136, mean=17.376, max=17.524, sum=104.258 (6)",
            "tab": "Summarization metrics",
            "score": 17.376290660463752
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB - EM",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.947,
        "details": {
          "description": "min=0.944, mean=0.947, max=0.951, sum=2.842 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.177, mean=0.204, max=0.232, sum=0.612 (3)",
            "tab": "Calibration",
            "score": 0.2038815444945483
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.913, mean=0.92, max=0.933, sum=2.76 (3)",
            "tab": "Robustness",
            "score": 0.9199999999999999
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.93, mean=0.937, max=0.946, sum=2.811 (3)",
            "tab": "Fairness",
            "score": 0.9369999999999999
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.908, mean=4.236, max=4.985, sum=12.708 (3)",
            "tab": "General information",
            "score": 4.236000000000001
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1283.569, mean=1560.056, max=1777.712, sum=4680.167 (3)",
            "tab": "General information",
            "score": 1560.0556666666664
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments - EM",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.524,
        "details": {
          "description": "min=0.014, mean=0.524, max=0.997, sum=28.276 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.112, mean=0.359, max=0.619, sum=19.409 (54)",
            "tab": "Calibration",
            "score": 0.35941964376806523
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.011, mean=0.368, max=0.874, sum=19.881 (54)",
            "tab": "Robustness",
            "score": 0.36816849425853654
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.462, max=0.985, sum=24.963 (54)",
            "tab": "Fairness",
            "score": 0.4622866273105216
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=362.037, mean=724.782, max=1272.822, sum=39138.207 (54)",
            "tab": "General information",
            "score": 724.7816027688522
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT - EM",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.523,
        "details": {
          "description": "min=0, mean=0.523, max=0.925, sum=17.25 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.143, mean=0.29, max=0.954, sum=9.577 (33)",
            "tab": "Calibration",
            "score": 0.2902057183123561
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.436, max=0.825, sum=14.4 (33)",
            "tab": "Robustness",
            "score": 0.43636363636363645
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0, mean=0.489, max=0.925, sum=16.15 (33)",
            "tab": "Fairness",
            "score": 0.4893939393939393
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.56, max=5, sum=150.475 (33)",
            "tab": "General information",
            "score": 4.5598484848484855
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0.002, max=0.025, sum=0.075 (33)",
            "tab": "General information",
            "score": 0.002272727272727273
          },
          "RAFT - # prompt tokens": {
            "description": "min=262.3, mean=810.769, max=1759.65, sum=26755.375 (33)",
            "tab": "General information",
            "score": 810.7689393939394
          },
          "RAFT - # output tokens": {
            "description": "min=0.125, mean=2.796, max=6.825, sum=92.275 (33)",
            "tab": "General information",
            "score": 2.796212121212121
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}