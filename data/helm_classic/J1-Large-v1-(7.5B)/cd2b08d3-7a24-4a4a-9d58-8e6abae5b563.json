{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/J1-Large-v1-(7.5B)/1767656643.700665",
  "retrieved_timestamp": "1767656643.700665",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "J1-Large v1 (7.5B)",
    "id": "J1-Large-v1-(7.5B)",
    "developer": "unknown",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.285,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.6383920923698907
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.29777282413544925
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.27467778791471786
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.38930372807017544
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5487461676083087
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.6599416016082683
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.6502297410192147
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.241,
        "details": {
          "description": "min=0.2, mean=0.241, max=0.298, sum=3.617 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.051, mean=0.123, max=0.181, sum=1.842 (15)",
            "tab": "Calibration",
            "score": 0.12277396117394333
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.16, mean=0.2, max=0.272, sum=3.002 (15)",
            "tab": "Robustness",
            "score": 0.20011695906432747
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.16, mean=0.204, max=0.23, sum=3.059 (15)",
            "tab": "Fairness",
            "score": 0.2039415204678363
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.348, mean=0.377, max=0.422, sum=5.648 (15)",
            "tab": "Efficiency",
            "score": 0.3765351217105263
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=308.59, mean=396.74, max=552.719, sum=5951.098 (15)",
            "tab": "General information",
            "score": 396.73985964912276
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.683,
        "details": {
          "description": "min=0.652, mean=0.683, max=0.709, sum=2.05 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.085, mean=0.106, max=0.133, sum=0.319 (3)",
            "tab": "Calibration",
            "score": 0.10621693084730484
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.539, mean=0.567, max=0.603, sum=1.701 (3)",
            "tab": "Robustness",
            "score": 0.5670000000000001
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.591, mean=0.622, max=0.651, sum=1.867 (3)",
            "tab": "Fairness",
            "score": 0.6223333333333333
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.43, mean=0.485, max=0.566, sum=1.455 (3)",
            "tab": "Efficiency",
            "score": 0.48513916883680525
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=506.985, mean=694.652, max=952.985, sum=2083.955 (3)",
            "tab": "General information",
            "score": 694.6516666666666
          },
          "BoolQ - # output tokens": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.623,
        "details": {
          "description": "min=0.612, mean=0.623, max=0.634, sum=1.87 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.042, mean=0.046, max=0.048, sum=0.137 (3)",
            "tab": "Calibration",
            "score": 0.04554705251298522
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.341, mean=0.4, max=0.438, sum=1.201 (3)",
            "tab": "Robustness",
            "score": 0.4003895179156612
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.496, mean=0.513, max=0.524, sum=1.538 (3)",
            "tab": "Fairness",
            "score": 0.5126679432053903
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=0.768, mean=0.797, max=0.829, sum=2.391 (3)",
            "tab": "Efficiency",
            "score": 0.7971074946205007
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=2.166, mean=2.639, max=3.225, sum=7.918 (3)",
            "tab": "General information",
            "score": 2.63943661971831
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1598.614, mean=1692.218, max=1777.299, sum=5076.654 (3)",
            "tab": "General information",
            "score": 1692.2178403755868
          },
          "NarrativeQA - # output tokens": {
            "description": "min=4.797, mean=5.09, max=5.518, sum=15.27 (3)",
            "tab": "General information",
            "score": 5.090140845070422
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1 (2)",
            "tab": "Bias",
            "score": 0.5
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.17, mean=0.203, max=0.223, sum=0.609 (3)",
            "tab": "Bias",
            "score": 0.20304247377415918
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.011, mean=0.013, max=0.014, sum=0.039 (3)",
            "tab": "Toxicity",
            "score": 0.013145539906103287
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.532,
        "details": {
          "description": "min=0.5, mean=0.532, max=0.571, sum=1.597 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.013, mean=0.015, max=0.017, sum=0.046 (3)",
            "tab": "Calibration",
            "score": 0.01549922748171477
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.075, mean=0.086, max=0.093, sum=0.258 (3)",
            "tab": "Calibration",
            "score": 0.08597598507389619
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.092, mean=0.098, max=0.106, sum=0.293 (3)",
            "tab": "Robustness",
            "score": 0.097632746101742
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.361, mean=0.41, max=0.455, sum=1.23 (3)",
            "tab": "Robustness",
            "score": 0.4099829032840138
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.14, mean=0.146, max=0.151, sum=0.439 (3)",
            "tab": "Fairness",
            "score": 0.14648226412007787
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.44, mean=0.47, max=0.508, sum=1.409 (3)",
            "tab": "Fairness",
            "score": 0.4695231845662433
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=0.355, mean=0.372, max=0.396, sum=1.117 (3)",
            "tab": "Efficiency",
            "score": 0.3722484414062495
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=0.66, mean=0.733, max=0.784, sum=2.198 (3)",
            "tab": "Efficiency",
            "score": 0.7326816432291658
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=94.377, mean=99.377, max=102.377, sum=298.131 (3)",
            "tab": "General information",
            "score": 99.377
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=6.868, mean=7.876, max=9.311, sum=23.628 (3)",
            "tab": "General information",
            "score": 7.876
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.568, mean=4.666, max=4.734, sum=13.999 (3)",
            "tab": "General information",
            "score": 4.666333333333333
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.038, mean=0.038, max=0.038, sum=0.114 (3)",
            "tab": "General information",
            "score": 0.038
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1136.933, mean=1418.457, max=1595.508, sum=4255.37 (3)",
            "tab": "General information",
            "score": 1418.4566666666667
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=5.487, mean=5.946, max=6.338, sum=17.838 (3)",
            "tab": "General information",
            "score": 5.946000000000001
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.214, mean=0.405, max=0.5, sum=1.214 (3)",
            "tab": "Bias",
            "score": 0.4047619047619048
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.302, mean=0.362, max=0.45, sum=1.085 (3)",
            "tab": "Bias",
            "score": 0.36169748540882557
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.088, mean=0.216, max=0.371, sum=0.647 (3)",
            "tab": "Bias",
            "score": 0.21556767868437698
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.327, mean=0.394, max=0.457, sum=1.182 (3)",
            "tab": "Bias",
            "score": 0.39383347574877653
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.106, mean=0.109, max=0.113, sum=0.328 (3)",
            "tab": "Bias",
            "score": 0.10941198128319474
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.002, max=0.002, sum=0.005 (3)",
            "tab": "Toxicity",
            "score": 0.0016666666666666668
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.001, sum=0.002 (3)",
            "tab": "Toxicity",
            "score": 0.0006666666666666666
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.328,
        "details": {
          "description": "min=0.322, mean=0.328, max=0.336, sum=0.983 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.016, mean=0.024, max=0.033, sum=0.073 (3)",
            "tab": "Calibration",
            "score": 0.02431531680637249
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.186, mean=0.197, max=0.209, sum=0.591 (3)",
            "tab": "Robustness",
            "score": 0.19699898429353593
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.227, mean=0.241, max=0.256, sum=0.722 (3)",
            "tab": "Fairness",
            "score": 0.24062000532402938
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=1.105, mean=1.16, max=1.191, sum=3.48 (3)",
            "tab": "Efficiency",
            "score": 1.159840737413194
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=1.788, mean=1.829, max=1.88, sum=5.486 (3)",
            "tab": "General information",
            "score": 1.8286666666666667
          },
          "QuAC - truncated": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "General information",
            "score": 0.001
          },
          "QuAC - # prompt tokens": {
            "description": "min=1645.856, mean=1698.711, max=1730.814, sum=5096.134 (3)",
            "tab": "General information",
            "score": 1698.7113333333334
          },
          "QuAC - # output tokens": {
            "description": "min=23.833, mean=27.642, max=30.067, sum=82.927 (3)",
            "tab": "General information",
            "score": 27.64233333333333
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.632, mean=0.647, max=0.667, sum=1.942 (3)",
            "tab": "Bias",
            "score": 0.6472747525379104
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.407, mean=0.428, max=0.446, sum=1.284 (3)",
            "tab": "Bias",
            "score": 0.42785601825865643
          },
          "QuAC - Representation (race)": {
            "description": "min=0.226, mean=0.3, max=0.351, sum=0.9 (3)",
            "tab": "Bias",
            "score": 0.2998485806834953
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.235, mean=0.249, max=0.271, sum=0.748 (3)",
            "tab": "Bias",
            "score": 0.24941347459181362
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.002, mean=0.003, max=0.004, sum=0.008 (3)",
            "tab": "Toxicity",
            "score": 0.0026666666666666666
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.7,
        "details": {
          "description": "min=0.7, mean=0.7, max=0.7, sum=0.7 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.192, mean=0.192, max=0.192, sum=0.192 (1)",
            "tab": "Calibration",
            "score": 0.19173198668049052
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.646, mean=0.646, max=0.646, sum=0.646 (1)",
            "tab": "Robustness",
            "score": 0.646
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.528, mean=0.528, max=0.528, sum=0.528 (1)",
            "tab": "Fairness",
            "score": 0.528
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "min=0.253, mean=0.253, max=0.253, sum=0.253 (1)",
            "tab": "Efficiency",
            "score": 0.25286050781250013
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=62.466, mean=62.466, max=62.466, sum=62.466 (1)",
            "tab": "General information",
            "score": 62.466
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.514,
        "details": {
          "description": "min=0.514, mean=0.514, max=0.514, sum=0.514 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
            "tab": "Calibration",
            "score": 0.24986668171933007
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
            "tab": "Robustness",
            "score": 0.412
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
            "tab": "Fairness",
            "score": 0.444
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "min=0.238, mean=0.238, max=0.238, sum=0.238 (1)",
            "tab": "Efficiency",
            "score": 0.2381039843749996
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=4.348, mean=4.348, max=4.348, sum=4.348 (1)",
            "tab": "General information",
            "score": 4.348
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.197,
        "details": {
          "description": "min=0.19, mean=0.197, max=0.2, sum=0.59 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.105, mean=0.112, max=0.121, sum=0.337 (3)",
            "tab": "Calibration",
            "score": 0.11232689963932652
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.138, mean=0.155, max=0.168, sum=0.465 (3)",
            "tab": "Robustness",
            "score": 0.15494393476044852
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.159, mean=0.174, max=0.182, sum=0.521 (3)",
            "tab": "Fairness",
            "score": 0.17380224260958207
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.351, mean=0.365, max=0.372, sum=1.094 (3)",
            "tab": "Efficiency",
            "score": 0.36458362003058115
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=317.682, mean=355.015, max=375.682, sum=1065.046 (3)",
            "tab": "General information",
            "score": 355.0152905198777
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.292,
        "details": {
          "description": "min=0.266, mean=0.292, max=0.338, sum=0.877 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.089, mean=0.105, max=0.128, sum=0.315 (3)",
            "tab": "Robustness",
            "score": 0.10499510582010585
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.231, mean=0.248, max=0.274, sum=0.743 (3)",
            "tab": "Robustness",
            "score": 0.24769351383898738
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.096, mean=0.117, max=0.143, sum=0.351 (3)",
            "tab": "Fairness",
            "score": 0.11706984126984123
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.258, mean=0.28, max=0.322, sum=0.841 (3)",
            "tab": "Fairness",
            "score": 0.2804651230679189
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "min=0.379, mean=0.393, max=0.406, sum=1.178 (3)",
            "tab": "Efficiency",
            "score": 0.3926667591145831
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "min=0.376, mean=0.389, max=0.402, sum=1.167 (3)",
            "tab": "Efficiency",
            "score": 0.3890438468992247
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=349.303, mean=385.636, max=423.303, sum=1156.909 (3)",
            "tab": "General information",
            "score": 385.63633333333337
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=2.011, mean=2.072, max=2.163, sum=6.217 (3)",
            "tab": "General information",
            "score": 2.0723333333333334
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=337.047, mean=373.38, max=411.047, sum=1120.14 (3)",
            "tab": "General information",
            "score": 373.3798449612403
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=2.093, mean=2.116, max=2.163, sum=6.349 (3)",
            "tab": "General information",
            "score": 2.116279069767442
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.134,
        "details": {
          "description": "min=0.123, mean=0.134, max=0.147, sum=0.802 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=1.832, mean=2.011, max=2.216, sum=12.069 (6)",
            "tab": "Efficiency",
            "score": 2.011487112821144
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1203.032, mean=1213.032, max=1224.032, sum=7278.193 (6)",
            "tab": "General information",
            "score": 1213.0321888412018
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=78.521, mean=89.614, max=102.401, sum=537.682 (6)",
            "tab": "General information",
            "score": 89.61373390557941
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.602, mean=0.632, max=0.648, sum=3.791 (6)",
            "tab": "Bias",
            "score": 0.6318145834093977
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.385, mean=0.391, max=0.396, sum=2.349 (6)",
            "tab": "Bias",
            "score": 0.3914278177516011
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.257, mean=0.302, max=0.354, sum=1.811 (6)",
            "tab": "Bias",
            "score": 0.3019033965877131
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.135, mean=0.142, max=0.152, sum=0.851 (6)",
            "tab": "Bias",
            "score": 0.14183552076259287
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.004, sum=0.009 (6)",
            "tab": "Toxicity",
            "score": 0.001430615164520744
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.488, mean=0.512, max=0.535, sum=1.537 (3)",
            "tab": "Summarization metrics",
            "score": 0.5121705493530246
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "min=4.664, mean=4.716, max=4.749, sum=28.295 (6)",
            "tab": "Summarization metrics",
            "score": 4.715823146970394
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.229, mean=0.248, max=0.272, sum=0.745 (3)",
            "tab": "Summarization metrics",
            "score": 0.2482954175661162
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.971, mean=0.977, max=0.985, sum=5.861 (6)",
            "tab": "Summarization metrics",
            "score": 0.9768840440430324
          },
          "CNN/DailyMail - Density": {
            "description": "min=55.528, mean=71.654, max=97.831, sum=429.924 (6)",
            "tab": "Summarization metrics",
            "score": 71.65405587945487
          },
          "CNN/DailyMail - Compression": {
            "description": "min=5.872, mean=7.632, max=9.373, sum=45.79 (6)",
            "tab": "Summarization metrics",
            "score": 7.631709472598792
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.102,
        "details": {
          "description": "min=0.095, mean=0.102, max=0.107, sum=0.612 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=0.896, mean=0.903, max=0.91, sum=5.418 (6)",
            "tab": "Efficiency",
            "score": 0.9030293349990619
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1099.388, mean=1133.388, max=1172.388, sum=6800.328 (6)",
            "tab": "General information",
            "score": 1133.388030888031
          },
          "XSUM - # output tokens": {
            "description": "min=20.832, mean=21.299, max=21.809, sum=127.792 (6)",
            "tab": "General information",
            "score": 21.2985842985843
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4.0 (6)",
            "tab": "Bias",
            "score": 0.6666666666666669
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.397, mean=0.424, max=0.451, sum=2.547 (6)",
            "tab": "Bias",
            "score": 0.42449478248089356
          },
          "XSUM - Representation (race)": {
            "description": "min=0.387, mean=0.426, max=0.467, sum=2.554 (6)",
            "tab": "Bias",
            "score": 0.4255855855855855
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.151, mean=0.172, max=0.189, sum=1.031 (6)",
            "tab": "Bias",
            "score": 0.1717873516720604
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "XSUM - SummaC": {
            "description": "min=-0.26, mean=-0.239, max=-0.222, sum=-0.716 (3)",
            "tab": "Summarization metrics",
            "score": -0.23866760351278402
          },
          "XSUM - QAFactEval": {
            "description": "min=3.354, mean=3.675, max=4.009, sum=22.047 (6)",
            "tab": "Summarization metrics",
            "score": 3.674546888395078
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.393, mean=0.4, max=0.405, sum=1.2 (3)",
            "tab": "Summarization metrics",
            "score": 0.40004604044843806
          },
          "XSUM - Coverage": {
            "description": "min=0.804, mean=0.808, max=0.813, sum=4.85 (6)",
            "tab": "Summarization metrics",
            "score": 0.8084128334077892
          },
          "XSUM - Density": {
            "description": "min=3.618, mean=3.757, max=3.939, sum=22.541 (6)",
            "tab": "Summarization metrics",
            "score": 3.7567632334705046
          },
          "XSUM - Compression": {
            "description": "min=17.523, mean=18.133, max=18.761, sum=108.8 (6)",
            "tab": "Summarization metrics",
            "score": 18.133322572088453
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.956,
        "details": {
          "description": "min=0.951, mean=0.956, max=0.962, sum=2.869 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.196, mean=0.213, max=0.234, sum=0.639 (3)",
            "tab": "Calibration",
            "score": 0.21314336064172376
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.927, mean=0.932, max=0.936, sum=2.796 (3)",
            "tab": "Robustness",
            "score": 0.932
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.939, mean=0.946, max=0.951, sum=2.839 (3)",
            "tab": "Fairness",
            "score": 0.9463333333333334
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=0.531, mean=0.637, max=0.757, sum=1.911 (3)",
            "tab": "Efficiency",
            "score": 0.6371184251302079
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.915, mean=4.972, max=5, sum=14.915 (3)",
            "tab": "General information",
            "score": 4.971666666666667
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=853.851, mean=1281.577, max=1725.03, sum=3844.732 (3)",
            "tab": "General information",
            "score": 1281.5773333333334
          },
          "IMDB - # output tokens": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.532,
        "details": {
          "description": "min=0, mean=0.532, max=0.996, sum=28.713 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.073, mean=0.377, max=0.573, sum=20.347 (54)",
            "tab": "Calibration",
            "score": 0.37680252478263027
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.444, max=0.984, sum=23.966 (54)",
            "tab": "Robustness",
            "score": 0.4438230435194026
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.447, max=0.962, sum=24.127 (54)",
            "tab": "Fairness",
            "score": 0.4468037461427085
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.338, mean=0.434, max=0.564, sum=23.454 (54)",
            "tab": "Efficiency",
            "score": 0.43432643222557377
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=271.927, mean=532.602, max=942.498, sum=28760.487 (54)",
            "tab": "General information",
            "score": 532.6016121330534
          },
          "CivilComments - # output tokens": {
            "description": "min=2, mean=2, max=2, sum=108 (54)",
            "tab": "General information",
            "score": 2.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.545,
        "details": {
          "description": "min=0.15, mean=0.545, max=0.95, sum=18 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.134, mean=0.269, max=0.513, sum=8.875 (33)",
            "tab": "Calibration",
            "score": 0.2689468403025133
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0.025, mean=0.443, max=0.95, sum=14.625 (33)",
            "tab": "Robustness",
            "score": 0.4431818181818182
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.15, mean=0.511, max=0.95, sum=16.85 (33)",
            "tab": "Fairness",
            "score": 0.5106060606060605
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.312, mean=0.499, max=0.763, sum=16.476 (33)",
            "tab": "Efficiency",
            "score": 0.4992617404513889
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0.95, mean=4.658, max=5, sum=153.7 (33)",
            "tab": "General information",
            "score": 4.657575757575757
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=212.25, mean=712.248, max=1745.25, sum=23504.175 (33)",
            "tab": "General information",
            "score": 712.2477272727273
          },
          "RAFT - # output tokens": {
            "description": "min=1.975, mean=3.499, max=7.025, sum=115.475 (33)",
            "tab": "General information",
            "score": 3.4992424242424245
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}