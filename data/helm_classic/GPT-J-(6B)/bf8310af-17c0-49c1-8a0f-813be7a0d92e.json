{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/GPT-J-(6B)/1767657483.8868392",
  "retrieved_timestamp": "1767657483.8868392",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "GPT-J (6B)",
    "id": "GPT-J-(6B)",
    "developer": "unknown",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.273,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.4640964584689531
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.29051104623963353
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.2899930436637889
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.6008771929824561
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.4572430192172563
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.24521373688040354
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.5489557226399332
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.249,
        "details": {
          "description": "min=0.14, mean=0.249, max=0.3, sum=3.728 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.062, mean=0.115, max=0.149, sum=1.732 (15)",
            "tab": "Calibration",
            "score": 0.11546362297486105
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.11, mean=0.217, max=0.28, sum=3.262 (15)",
            "tab": "Robustness",
            "score": 0.2174502923976608
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.13, mean=0.22, max=0.27, sum=3.294 (15)",
            "tab": "Fairness",
            "score": 0.21961403508771932
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.066, mean=0.07, max=0.072, sum=1.05 (15)",
            "tab": "Efficiency",
            "score": 0.06997480863135229
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=371.38, mean=472.274, max=624.07, sum=7084.111 (15)",
            "tab": "General information",
            "score": 472.2740350877193
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.649,
        "details": {
          "description": "min=0.646, mean=0.649, max=0.65, sum=1.946 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.043, mean=0.062, max=0.086, sum=0.187 (3)",
            "tab": "Calibration",
            "score": 0.062432673938629946
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.608, mean=0.621, max=0.631, sum=1.863 (3)",
            "tab": "Robustness",
            "score": 0.621
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.638, mean=0.639, max=0.64, sum=1.916 (3)",
            "tab": "Fairness",
            "score": 0.6386666666666666
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.354, mean=0.499, max=0.575, sum=1.497 (3)",
            "tab": "Efficiency",
            "score": 0.49915384031836946
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=660.073, mean=908.406, max=1242.073, sum=2725.219 (3)",
            "tab": "General information",
            "score": 908.4063333333334
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.545,
        "details": {
          "description": "min=0.54, mean=0.545, max=0.554, sum=1.634 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.189, mean=0.199, max=0.211, sum=0.596 (3)",
            "tab": "Calibration",
            "score": 0.19883043691040034
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.099, mean=0.135, max=0.156, sum=0.405 (3)",
            "tab": "Robustness",
            "score": 0.1349521611222693
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.417, mean=0.433, max=0.448, sum=1.3 (3)",
            "tab": "Fairness",
            "score": 0.43317656281615613
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=0.988, mean=1.311, max=1.513, sum=3.934 (3)",
            "tab": "Efficiency",
            "score": 1.311420011868712
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.051, mean=1.647, max=2.085, sum=4.941 (3)",
            "tab": "General information",
            "score": 1.6469483568075116
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1601.955, mean=1652.377, max=1705.003, sum=4957.132 (3)",
            "tab": "General information",
            "score": 1652.3774647887324
          },
          "NarrativeQA - # output tokens": {
            "description": "min=42.766, mean=56.052, max=70.845, sum=168.155 (3)",
            "tab": "General information",
            "score": 56.05164319248826
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.416, mean=0.451, max=0.5, sum=1.353 (3)",
            "tab": "Bias",
            "score": 0.4510416666666666
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.204, mean=0.217, max=0.229, sum=0.651 (3)",
            "tab": "Bias",
            "score": 0.21710889248239795
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.017, mean=0.021, max=0.025, sum=0.062 (3)",
            "tab": "Toxicity",
            "score": 0.020657276995305163
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.559,
        "details": {
          "description": "min=0.548, mean=0.559, max=0.57, sum=1.677 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.069, mean=0.075, max=0.079, sum=0.224 (3)",
            "tab": "Calibration",
            "score": 0.07464671252737104
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.346, mean=0.354, max=0.358, sum=1.062 (3)",
            "tab": "Calibration",
            "score": 0.3539383109024162
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.09, mean=0.099, max=0.109, sum=0.298 (3)",
            "tab": "Robustness",
            "score": 0.09933930594531819
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.185, mean=0.228, max=0.265, sum=0.683 (3)",
            "tab": "Robustness",
            "score": 0.22767804828628146
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.112, mean=0.122, max=0.128, sum=0.365 (3)",
            "tab": "Fairness",
            "score": 0.12161534757794057
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.475, mean=0.493, max=0.505, sum=1.479 (3)",
            "tab": "Fairness",
            "score": 0.4930833990161269
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=1.626, mean=1.777, max=1.998, sum=5.331 (3)",
            "tab": "Efficiency",
            "score": 1.77691167926379
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=3.687, mean=3.866, max=4.016, sum=11.599 (3)",
            "tab": "Efficiency",
            "score": 3.8663324384530373
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=110.254, mean=112.254, max=116.254, sum=336.762 (3)",
            "tab": "General information",
            "score": 112.254
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=273.408, mean=282.837, max=296.556, sum=848.512 (3)",
            "tab": "General information",
            "score": 282.83733333333333
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.647, mean=4.691, max=4.724, sum=14.074 (3)",
            "tab": "General information",
            "score": 4.691333333333334
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.036, mean=0.036, max=0.036, sum=0.108 (3)",
            "tab": "General information",
            "score": 0.036
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1231.212, mean=1419.574, max=1523.257, sum=4258.721 (3)",
            "tab": "General information",
            "score": 1419.5736666666664
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=234.154, mean=247.23, max=261.681, sum=741.689 (3)",
            "tab": "General information",
            "score": 247.22966666666665
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.373, mean=0.49, max=0.553, sum=1.47 (3)",
            "tab": "Bias",
            "score": 0.49013920663848926
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.071, mean=0.192, max=0.38, sum=0.576 (3)",
            "tab": "Bias",
            "score": 0.19214285714285717
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.484, mean=0.524, max=0.561, sum=1.571 (3)",
            "tab": "Bias",
            "score": 0.5236086934551658
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.289, mean=0.317, max=0.333, sum=0.95 (3)",
            "tab": "Bias",
            "score": 0.3167977414801371
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.002, sum=0.004 (3)",
            "tab": "Toxicity",
            "score": 0.0013333333333333333
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.33,
        "details": {
          "description": "min=0.322, mean=0.33, max=0.335, sum=0.989 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.104, mean=0.13, max=0.169, sum=0.391 (3)",
            "tab": "Calibration",
            "score": 0.13037730069459044
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.14, mean=0.147, max=0.155, sum=0.44 (3)",
            "tab": "Robustness",
            "score": 0.14672783806116493
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.245, mean=0.249, max=0.258, sum=0.748 (3)",
            "tab": "Fairness",
            "score": 0.2494842989068126
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=1.354, mean=1.389, max=1.411, sum=4.166 (3)",
            "tab": "Efficiency",
            "score": 1.3887290514336688
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.845, mean=0.944, max=1.086, sum=2.833 (3)",
            "tab": "General information",
            "score": 0.9443333333333334
          },
          "QuAC - truncated": {
            "description": "min=0.016, mean=0.016, max=0.016, sum=0.048 (3)",
            "tab": "General information",
            "score": 0.016
          },
          "QuAC - # prompt tokens": {
            "description": "min=1625.523, mean=1644.831, max=1670.605, sum=4934.492 (3)",
            "tab": "General information",
            "score": 1644.8306666666667
          },
          "QuAC - # output tokens": {
            "description": "min=64.208, mean=68.54, max=71.626, sum=205.621 (3)",
            "tab": "General information",
            "score": 68.54033333333334
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.568, mean=0.613, max=0.641, sum=1.838 (3)",
            "tab": "Bias",
            "score": 0.6126959460292795
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.41, mean=0.43, max=0.447, sum=1.29 (3)",
            "tab": "Bias",
            "score": 0.4301368170697724
          },
          "QuAC - Representation (race)": {
            "description": "min=0.232, mean=0.266, max=0.294, sum=0.798 (3)",
            "tab": "Bias",
            "score": 0.2658629278217009
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.211, mean=0.23, max=0.241, sum=0.69 (3)",
            "tab": "Bias",
            "score": 0.2300432286449244
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.001, mean=0.004, max=0.005, sum=0.011 (3)",
            "tab": "Toxicity",
            "score": 0.0036666666666666666
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.663,
        "details": {
          "description": "min=0.663, mean=0.663, max=0.663, sum=0.663 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.233, mean=0.233, max=0.233, sum=0.233 (1)",
            "tab": "Calibration",
            "score": 0.2332919292558098
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.619, mean=0.619, max=0.619, sum=0.619 (1)",
            "tab": "Robustness",
            "score": 0.619
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.486, mean=0.486, max=0.486, sum=0.486 (1)",
            "tab": "Fairness",
            "score": 0.486
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "min=0.03, mean=0.03, max=0.03, sum=0.03 (1)",
            "tab": "Efficiency",
            "score": 0.030294155851006508
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=87.888, mean=87.888, max=87.888, sum=87.888 (1)",
            "tab": "General information",
            "score": 87.888
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.514,
        "details": {
          "description": "min=0.514, mean=0.514, max=0.514, sum=0.514 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.235, mean=0.235, max=0.235, sum=0.235 (1)",
            "tab": "Calibration",
            "score": 0.2353362549897216
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.398, mean=0.398, max=0.398, sum=0.398 (1)",
            "tab": "Robustness",
            "score": 0.398
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.416, mean=0.416, max=0.416, sum=0.416 (1)",
            "tab": "Fairness",
            "score": 0.416
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "min=0.019, mean=0.019, max=0.019, sum=0.019 (1)",
            "tab": "Efficiency",
            "score": 0.019339164675618026
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.27, mean=5.27, max=5.27, sum=5.27 (1)",
            "tab": "General information",
            "score": 5.27
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.199,
        "details": {
          "description": "min=0.187, mean=0.199, max=0.213, sum=0.797 (4)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.056, mean=0.078, max=0.103, sum=0.311 (4)",
            "tab": "Calibration",
            "score": 0.07772735423117484
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.157, mean=0.181, max=0.209, sum=0.725 (4)",
            "tab": "Robustness",
            "score": 0.1811926605504587
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.156, mean=0.18, max=0.209, sum=0.72 (4)",
            "tab": "Fairness",
            "score": 0.18004587155963303
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.018, mean=0.044, max=0.053, sum=0.175 (4)",
            "tab": "Efficiency",
            "score": 0.043782452828866295
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=2616 (4)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=0, mean=3.75, max=5, sum=15 (4)",
            "tab": "General information",
            "score": 3.75
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (4)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=85.121, mean=404.621, max=529.121, sum=1618.483 (4)",
            "tab": "General information",
            "score": 404.62079510703364
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=4 (4)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=2.5, max=3, sum=10 (4)",
            "tab": "General information",
            "score": 2.5
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.345,
        "details": {
          "description": "min=0.315, mean=0.345, max=0.362, sum=1.035 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.094, mean=0.116, max=0.131, sum=0.349 (3)",
            "tab": "Robustness",
            "score": 0.11636587301587299
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.29, mean=0.319, max=0.336, sum=0.957 (3)",
            "tab": "Robustness",
            "score": 0.3190834142643501
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.106, mean=0.129, max=0.144, sum=0.387 (3)",
            "tab": "Fairness",
            "score": 0.12886375661375657
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.303, mean=0.332, max=0.348, sum=0.997 (3)",
            "tab": "Fairness",
            "score": 0.3321982457704417
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "min=0.081, mean=0.084, max=0.088, sum=0.252 (3)",
            "tab": "Efficiency",
            "score": 0.08407480907713127
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "min=0.078, mean=0.081, max=0.083, sum=0.242 (3)",
            "tab": "Efficiency",
            "score": 0.08053553836682271
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=495.232, mean=532.565, max=577.232, sum=1597.696 (3)",
            "tab": "General information",
            "score": 532.5653333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=478.488, mean=515.822, max=560.488, sum=1547.465 (3)",
            "tab": "General information",
            "score": 515.8217054263565
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.131,
        "details": {
          "description": "min=0.127, mean=0.131, max=0.135, sum=0.787 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=1.997, mean=2.076, max=2.172, sum=12.455 (6)",
            "tab": "Efficiency",
            "score": 2.0758840914959578
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1531.586, mean=1549.919, max=1567.586, sum=9299.515 (6)",
            "tab": "General information",
            "score": 1549.9191702432045
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=76.916, mean=83.931, max=91.68, sum=503.584 (6)",
            "tab": "General information",
            "score": 83.93061516452074
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.602, mean=0.63, max=0.655, sum=3.78 (6)",
            "tab": "Bias",
            "score": 0.6299677400199846
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.398, mean=0.402, max=0.41, sum=2.415 (6)",
            "tab": "Bias",
            "score": 0.40247728320483095
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.23, mean=0.293, max=0.359, sum=1.759 (6)",
            "tab": "Bias",
            "score": 0.2931668421996429
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.131, mean=0.146, max=0.169, sum=0.875 (6)",
            "tab": "Bias",
            "score": 0.14576217898261626
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0.002, mean=0.002, max=0.002, sum=0.013 (6)",
            "tab": "Toxicity",
            "score": 0.002145922746781116
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.172, mean=0.208, max=0.236, sum=0.623 (3)",
            "tab": "Summarization metrics",
            "score": 0.20780144742590156
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "min=4.648, mean=4.704, max=4.739, sum=28.226 (6)",
            "tab": "Summarization metrics",
            "score": 4.704313539792442
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.241, mean=0.247, max=0.25, sum=0.74 (3)",
            "tab": "Summarization metrics",
            "score": 0.2466254745716148
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.902, mean=0.948, max=0.97, sum=5.685 (6)",
            "tab": "Summarization metrics",
            "score": 0.9475541325972495
          },
          "CNN/DailyMail - Density": {
            "description": "min=41.364, mean=48.284, max=57.69, sum=289.703 (6)",
            "tab": "Summarization metrics",
            "score": 48.283839374824815
          },
          "CNN/DailyMail - Compression": {
            "description": "min=8.117, mean=9.864, max=11.439, sum=59.186 (6)",
            "tab": "Summarization metrics",
            "score": 9.864391531990323
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.096,
        "details": {
          "description": "min=0.093, mean=0.096, max=0.097, sum=0.573 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=0.73, mean=0.742, max=0.758, sum=4.455 (6)",
            "tab": "Efficiency",
            "score": 0.7424737962465443
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.998, mean=4.999, max=5, sum=29.992 (6)",
            "tab": "General information",
            "score": 4.998712998712999
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1456.402, mean=1510.418, max=1538.921, sum=9062.51 (6)",
            "tab": "General information",
            "score": 1510.4182754182755
          },
          "XSUM - # output tokens": {
            "description": "min=24.919, mean=25.529, max=26.187, sum=153.174 (6)",
            "tab": "General information",
            "score": 25.52895752895753
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.393, mean=0.435, max=0.466, sum=2.612 (6)",
            "tab": "Bias",
            "score": 0.43535525321239604
          },
          "XSUM - Representation (race)": {
            "description": "min=0.467, mean=0.513, max=0.565, sum=3.08 (6)",
            "tab": "Bias",
            "score": 0.5133548156104547
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.141, mean=0.165, max=0.179, sum=0.988 (6)",
            "tab": "Bias",
            "score": 0.1646512031093765
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.002, max=0.004, sum=0.012 (6)",
            "tab": "Toxicity",
            "score": 0.0019305019305019308
          },
          "XSUM - SummaC": {
            "description": "min=-0.229, mean=-0.198, max=-0.176, sum=-0.593 (3)",
            "tab": "Summarization metrics",
            "score": -0.1976111372976741
          },
          "XSUM - QAFactEval": {
            "description": "min=3.59, mean=3.813, max=4.142, sum=22.877 (6)",
            "tab": "Summarization metrics",
            "score": 3.8128682530109397
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.379, mean=0.381, max=0.384, sum=1.142 (3)",
            "tab": "Summarization metrics",
            "score": 0.3808147712365148
          },
          "XSUM - Coverage": {
            "description": "min=0.824, mean=0.829, max=0.831, sum=4.972 (6)",
            "tab": "Summarization metrics",
            "score": 0.8286466360730634
          },
          "XSUM - Density": {
            "description": "min=3.796, mean=4.043, max=4.434, sum=24.256 (6)",
            "tab": "Summarization metrics",
            "score": 4.042629935538992
          },
          "XSUM - Compression": {
            "description": "min=17.57, mean=17.942, max=18.398, sum=107.65 (6)",
            "tab": "Summarization metrics",
            "score": 17.941696288315352
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.939,
        "details": {
          "description": "min=0.932, mean=0.939, max=0.946, sum=2.816 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.285, mean=0.295, max=0.311, sum=0.884 (3)",
            "tab": "Calibration",
            "score": 0.2945110955018834
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.895, mean=0.903, max=0.908, sum=2.709 (3)",
            "tab": "Robustness",
            "score": 0.903
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.92, mean=0.927, max=0.932, sum=2.782 (3)",
            "tab": "Fairness",
            "score": 0.9273333333333333
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=0.63, mean=0.701, max=0.761, sum=2.104 (3)",
            "tab": "Efficiency",
            "score": 0.7011672212481499
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.846, mean=4.933, max=4.986, sum=14.798 (3)",
            "tab": "General information",
            "score": 4.932666666666667
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1152.694, mean=1389.454, max=1744.631, sum=4168.363 (3)",
            "tab": "General information",
            "score": 1389.4543333333331
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.52,
        "details": {
          "description": "min=0.002, mean=0.52, max=1, sum=28.06 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.075, mean=0.409, max=0.626, sum=22.076 (54)",
            "tab": "Calibration",
            "score": 0.40880926893677766
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.418, max=1, sum=22.597 (54)",
            "tab": "Robustness",
            "score": 0.4184575354873046
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.488, max=1, sum=26.356 (54)",
            "tab": "Fairness",
            "score": 0.4880679688031825
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.154, mean=0.307, max=0.494, sum=16.591 (54)",
            "tab": "Efficiency",
            "score": 0.30723795570455475
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=356.537, mean=722.635, max=1267.519, sum=39022.317 (54)",
            "tab": "General information",
            "score": 722.6354931173206
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "min=0.333, mean=0.5, max=0.667, sum=1 (2)",
            "tab": "Bias",
            "score": 0.5
          },
          "CivilComments - Representation (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1 (2)",
            "tab": "Bias",
            "score": 0.5
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0.0, max=0.001, sum=0.001 (54)",
            "tab": "Toxicity",
            "score": 0.000027763895829862844
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.619,
        "details": {
          "description": "min=0.275, mean=0.619, max=0.975, sum=20.425 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.116, mean=0.389, max=0.975, sum=12.832 (33)",
            "tab": "Calibration",
            "score": 0.3888407166022056
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0.1, mean=0.53, max=0.975, sum=17.5 (33)",
            "tab": "Robustness",
            "score": 0.5303030303030303
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.275, mean=0.594, max=0.975, sum=19.6 (33)",
            "tab": "Fairness",
            "score": 0.593939393939394
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.107, mean=0.628, max=1.382, sum=20.733 (33)",
            "tab": "Efficiency",
            "score": 0.6282604447639349
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.556, max=5, sum=150.35 (33)",
            "tab": "General information",
            "score": 4.556060606060607
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=257.35, mean=812.938, max=1773.675, sum=26826.95 (33)",
            "tab": "General information",
            "score": 812.937878787879
          },
          "RAFT - # output tokens": {
            "description": "min=5, mean=14.276, max=30, sum=471.1 (33)",
            "tab": "General information",
            "score": 14.275757575757577
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}