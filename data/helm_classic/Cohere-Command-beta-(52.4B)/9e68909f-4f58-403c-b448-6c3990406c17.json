{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/Cohere-Command-beta-(52.4B)/1767656643.700665",
  "retrieved_timestamp": "1767656643.700665",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Cohere Command beta (52.4B)",
    "id": "Cohere-Command-beta-(52.4B)",
    "developer": "unknown",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.874,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.5963856625666678
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.8502739196287583
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.8657917351465738
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5758163753811841
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.6738178488178488
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.6776315789473684
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.452,
        "details": {
          "description": "min=0.23, mean=0.452, max=0.79, sum=6.786 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.099, mean=0.183, max=0.338, sum=2.742 (15)",
            "tab": "Calibration",
            "score": 0.18282231471159943
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.15, mean=0.387, max=0.73, sum=5.807 (15)",
            "tab": "Robustness",
            "score": 0.38711111111111113
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.19, mean=0.407, max=0.73, sum=6.107 (15)",
            "tab": "Fairness",
            "score": 0.4071111111111111
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=372.75, mean=481.26, max=628.421, sum=7218.903 (15)",
            "tab": "General information",
            "score": 481.2602105263158
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.856,
        "details": {
          "description": "min=0.849, mean=0.856, max=0.86, sum=2.569 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.018, mean=0.023, max=0.026, sum=0.069 (3)",
            "tab": "Calibration",
            "score": 0.02302613493537822
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.806, mean=0.811, max=0.816, sum=2.432 (3)",
            "tab": "Robustness",
            "score": 0.8106666666666666
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.812, mean=0.822, max=0.827, sum=2.465 (3)",
            "tab": "Fairness",
            "score": 0.8216666666666667
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=669.307, mean=925.307, max=1269.307, sum=2775.921 (3)",
            "tab": "General information",
            "score": 925.3070000000001
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.752,
        "details": {
          "description": "min=0.744, mean=0.752, max=0.763, sum=2.255 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.051, mean=0.058, max=0.067, sum=0.173 (3)",
            "tab": "Calibration",
            "score": 0.05761424791814445
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.566, mean=0.57, max=0.578, sum=1.711 (3)",
            "tab": "Robustness",
            "score": 0.5702997988620334
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.647, mean=0.657, max=0.666, sum=1.97 (3)",
            "tab": "Fairness",
            "score": 0.6566736137653061
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0.904, mean=1.508, max=1.941, sum=4.524 (3)",
            "tab": "General information",
            "score": 1.5079812206572771
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1570.772, mean=1600.684, max=1660.485, sum=4802.051 (3)",
            "tab": "General information",
            "score": 1600.6835680751174
          },
          "NarrativeQA - # output tokens": {
            "description": "min=5.679, mean=5.992, max=6.496, sum=17.977 (3)",
            "tab": "General information",
            "score": 5.992488262910798
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.361, mean=0.404, max=0.444, sum=1.213 (3)",
            "tab": "Bias",
            "score": 0.404320987654321
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.174, mean=0.178, max=0.181, sum=0.534 (3)",
            "tab": "Bias",
            "score": 0.1778748183802931
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.011, mean=0.014, max=0.017, sum=0.042 (3)",
            "tab": "Toxicity",
            "score": 0.014084507042253521
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.76,
        "details": {
          "description": "min=0.755, mean=0.76, max=0.763, sum=2.28 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.075, mean=0.084, max=0.091, sum=0.251 (3)",
            "tab": "Calibration",
            "score": 0.08377931898267306
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.044, mean=0.056, max=0.063, sum=0.168 (3)",
            "tab": "Calibration",
            "score": 0.05602757611120105
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.286, mean=0.289, max=0.294, sum=0.867 (3)",
            "tab": "Robustness",
            "score": 0.28891923018489013
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.669, mean=0.679, max=0.685, sum=2.036 (3)",
            "tab": "Robustness",
            "score": 0.6786112890887687
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.29, mean=0.296, max=0.301, sum=0.888 (3)",
            "tab": "Fairness",
            "score": 0.29608566298974776
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.7, mean=0.706, max=0.714, sum=2.117 (3)",
            "tab": "Fairness",
            "score": 0.7056823207366739
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.191, mean=111.191, max=115.191, sum=333.573 (3)",
            "tab": "General information",
            "score": 111.19099999999999
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=4.29, mean=4.325, max=4.367, sum=12.974 (3)",
            "tab": "General information",
            "score": 4.324666666666666
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.485, mean=4.602, max=4.705, sum=13.807 (3)",
            "tab": "General information",
            "score": 4.602333333333333
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.039, mean=0.039, max=0.039, sum=0.117 (3)",
            "tab": "General information",
            "score": 0.039
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1258.15, mean=1471.073, max=1597.431, sum=4413.22 (3)",
            "tab": "General information",
            "score": 1471.073333333333
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=7.153, mean=7.288, max=7.488, sum=21.864 (3)",
            "tab": "General information",
            "score": 7.288
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.487, mean=0.552, max=0.634, sum=1.655 (3)",
            "tab": "Bias",
            "score": 0.5517958743765196
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.063, mean=0.129, max=0.206, sum=0.387 (3)",
            "tab": "Bias",
            "score": 0.12914332399626519
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.479, mean=0.482, max=0.483, sum=1.446 (3)",
            "tab": "Bias",
            "score": 0.48194444444444445
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.577, mean=0.579, max=0.582, sum=1.737 (3)",
            "tab": "Bias",
            "score": 0.5791309646902151
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.025, mean=0.05, max=0.067, sum=0.151 (3)",
            "tab": "Bias",
            "score": 0.05047080979284368
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.432,
        "details": {
          "description": "min=0.429, mean=0.432, max=0.435, sum=1.296 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.043, mean=0.06, max=0.073, sum=0.181 (3)",
            "tab": "Calibration",
            "score": 0.06049762085119498
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.236, mean=0.238, max=0.24, sum=0.715 (3)",
            "tab": "Robustness",
            "score": 0.23825281130135667
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.309, mean=0.316, max=0.322, sum=0.947 (3)",
            "tab": "Fairness",
            "score": 0.31563184414828255
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.748, mean=0.848, max=0.933, sum=2.545 (3)",
            "tab": "General information",
            "score": 0.8483333333333333
          },
          "QuAC - truncated": {
            "description": "min=0.022, mean=0.022, max=0.022, sum=0.066 (3)",
            "tab": "General information",
            "score": 0.022000000000000002
          },
          "QuAC - # prompt tokens": {
            "description": "min=1577.224, mean=1610.503, max=1643.74, sum=4831.508 (3)",
            "tab": "General information",
            "score": 1610.5026666666665
          },
          "QuAC - # output tokens": {
            "description": "min=19.435, mean=19.627, max=19.984, sum=58.881 (3)",
            "tab": "General information",
            "score": 19.627
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.593, mean=0.596, max=0.603, sum=1.788 (3)",
            "tab": "Bias",
            "score": 0.5961199294532628
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.459, mean=0.47, max=0.484, sum=1.409 (3)",
            "tab": "Bias",
            "score": 0.4696816360952984
          },
          "QuAC - Representation (race)": {
            "description": "min=0.299, mean=0.316, max=0.333, sum=0.949 (3)",
            "tab": "Bias",
            "score": 0.316297459154602
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.219, mean=0.232, max=0.245, sum=0.695 (3)",
            "tab": "Bias",
            "score": 0.23168423828159934
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.811,
        "details": {
          "description": "min=0.811, mean=0.811, max=0.811, sum=0.811 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.325, mean=0.325, max=0.325, sum=0.325 (1)",
            "tab": "Calibration",
            "score": 0.3246923611213033
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.774, mean=0.774, max=0.774, sum=0.774 (1)",
            "tab": "Robustness",
            "score": 0.774
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.699, mean=0.699, max=0.699, sum=0.699 (1)",
            "tab": "Fairness",
            "score": 0.699
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.855, mean=88.855, max=88.855, sum=88.855 (1)",
            "tab": "General information",
            "score": 88.855
          },
          "HellaSwag - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.582,
        "details": {
          "description": "min=0.582, mean=0.582, max=0.582, sum=0.582 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.231, mean=0.231, max=0.231, sum=0.231 (1)",
            "tab": "Calibration",
            "score": 0.23111297495969485
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.492, mean=0.492, max=0.492, sum=0.492 (1)",
            "tab": "Robustness",
            "score": 0.492
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.508, mean=0.508, max=0.508, sum=0.508 (1)",
            "tab": "Fairness",
            "score": 0.508
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.358, mean=5.358, max=5.358, sum=5.358 (1)",
            "tab": "General information",
            "score": 5.358
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.269,
        "details": {
          "description": "min=0.265, mean=0.269, max=0.275, sum=0.807 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.272, mean=0.311, max=0.338, sum=0.933 (3)",
            "tab": "Calibration",
            "score": 0.31095945192078733
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.226, mean=0.229, max=0.231, sum=0.688 (3)",
            "tab": "Robustness",
            "score": 0.2293577981651376
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.219, mean=0.222, max=0.225, sum=0.665 (3)",
            "tab": "Fairness",
            "score": 0.2217125382262997
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.315, mean=514.648, max=532.315, sum=1543.945 (3)",
            "tab": "General information",
            "score": 514.6483180428135
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.762,
        "details": {
          "description": "min=0.761, mean=0.762, max=0.765, sum=2.287 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.429, mean=0.434, max=0.438, sum=1.303 (3)",
            "tab": "Robustness",
            "score": 0.43439140211640154
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.726, mean=0.734, max=0.743, sum=2.202 (3)",
            "tab": "Robustness",
            "score": 0.7339375978505934
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.444, mean=0.45, max=0.453, sum=1.35 (3)",
            "tab": "Fairness",
            "score": 0.4498752645502638
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.745, mean=0.748, max=0.752, sum=2.245 (3)",
            "tab": "Fairness",
            "score": 0.7483868294443408
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=497.281, mean=536.614, max=583.281, sum=1609.843 (3)",
            "tab": "General information",
            "score": 536.6143333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=480.163, mean=519.496, max=566.163, sum=1558.488 (3)",
            "tab": "General information",
            "score": 519.4961240310078
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.161,
        "details": {
          "description": "min=0.156, mean=0.161, max=0.167, sum=0.966 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1555.036, mean=1575.036, max=1602.036, sum=9450.219 (6)",
            "tab": "General information",
            "score": 1575.0364806866953
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=72.088, mean=74.406, max=77.451, sum=446.433 (6)",
            "tab": "General information",
            "score": 74.40557939914163
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.587, mean=0.612, max=0.629, sum=3.673 (6)",
            "tab": "Bias",
            "score": 0.6121656731068496
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.391, mean=0.396, max=0.407, sum=2.379 (6)",
            "tab": "Bias",
            "score": 0.39642600089657387
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.238, mean=0.286, max=0.343, sum=1.713 (6)",
            "tab": "Bias",
            "score": 0.28558037967512334
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.088, mean=0.09, max=0.093, sum=0.537 (6)",
            "tab": "Bias",
            "score": 0.08955985269326716
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.366, mean=0.415, max=0.441, sum=1.245 (3)",
            "tab": "Summarization metrics",
            "score": 0.4149051333035736
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.316, mean=0.318, max=0.322, sum=0.955 (3)",
            "tab": "Summarization metrics",
            "score": 0.31834420143428105
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.976, mean=0.979, max=0.982, sum=5.874 (6)",
            "tab": "Summarization metrics",
            "score": 0.9790462109521986
          },
          "CNN/DailyMail - Density": {
            "description": "min=28.96, mean=32.165, max=35.676, sum=192.989 (6)",
            "tab": "Summarization metrics",
            "score": 32.164866076836944
          },
          "CNN/DailyMail - Compression": {
            "description": "min=8.594, mean=9.156, max=9.657, sum=54.938 (6)",
            "tab": "Summarization metrics",
            "score": 9.156293880030324
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.152,
        "details": {
          "description": "min=0.147, mean=0.152, max=0.156, sum=0.913 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.996, mean=4.997, max=5, sum=29.985 (6)",
            "tab": "General information",
            "score": 4.997425997425997
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1484.608, mean=1537.293, max=1572.616, sum=9223.757 (6)",
            "tab": "General information",
            "score": 1537.2927927927929
          },
          "XSUM - # output tokens": {
            "description": "min=24.187, mean=24.351, max=24.541, sum=146.108 (6)",
            "tab": "General information",
            "score": 24.35135135135135
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4.0 (6)",
            "tab": "Bias",
            "score": 0.6666666666666669
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.433, mean=0.457, max=0.476, sum=2.745 (6)",
            "tab": "Bias",
            "score": 0.4574302134646962
          },
          "XSUM - Representation (race)": {
            "description": "min=0.481, mean=0.522, max=0.556, sum=3.13 (6)",
            "tab": "Bias",
            "score": 0.5217473884140551
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.18, mean=0.181, max=0.182, sum=1.086 (6)",
            "tab": "Bias",
            "score": 0.1810207108427353
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "XSUM - SummaC": {
            "description": "min=-0.285, mean=-0.271, max=-0.262, sum=-0.814 (3)",
            "tab": "Summarization metrics",
            "score": -0.27140173856816235
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.455, mean=0.459, max=0.462, sum=1.376 (3)",
            "tab": "Summarization metrics",
            "score": 0.4587225678869484
          },
          "XSUM - Coverage": {
            "description": "min=0.788, mean=0.793, max=0.797, sum=4.758 (6)",
            "tab": "Summarization metrics",
            "score": 0.7930169105851288
          },
          "XSUM - Density": {
            "description": "min=2.417, mean=2.548, max=2.678, sum=15.286 (6)",
            "tab": "Summarization metrics",
            "score": 2.54760656490819
          },
          "XSUM - Compression": {
            "description": "min=16.704, mean=16.937, max=17.065, sum=101.621 (6)",
            "tab": "Summarization metrics",
            "score": 16.93675136805864
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.96,
        "details": {
          "description": "min=0.955, mean=0.96, max=0.965, sum=2.881 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.011, mean=0.015, max=0.02, sum=0.045 (3)",
            "tab": "Calibration",
            "score": 0.015015056118517703
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.929, mean=0.933, max=0.936, sum=2.799 (3)",
            "tab": "Robustness",
            "score": 0.9330000000000002
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.951, mean=0.957, max=0.96, sum=2.871 (3)",
            "tab": "Fairness",
            "score": 0.957
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.89, mean=4.217, max=4.981, sum=12.652 (3)",
            "tab": "General information",
            "score": 4.217333333333333
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1282.318, mean=1557.741, max=1776.111, sum=4673.222 (3)",
            "tab": "General information",
            "score": 1557.7406666666666
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.601,
        "details": {
          "description": "min=0.254, mean=0.601, max=0.86, sum=32.478 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.054, mean=0.161, max=0.416, sum=8.676 (54)",
            "tab": "Calibration",
            "score": 0.16066140880534402
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.205, mean=0.535, max=0.84, sum=28.866 (54)",
            "tab": "Robustness",
            "score": 0.5345588668880686
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.222, mean=0.544, max=0.85, sum=29.397 (54)",
            "tab": "Fairness",
            "score": 0.5443897908426464
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=362.293, mean=732.514, max=1288.441, sum=39555.782 (54)",
            "tab": "General information",
            "score": 732.5144825548033
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.667,
        "details": {
          "description": "min=0.025, mean=0.667, max=0.975, sum=22.0 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.041, mean=0.262, max=0.96, sum=8.637 (33)",
            "tab": "Calibration",
            "score": 0.26172447899775947
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.599, max=0.975, sum=19.775 (33)",
            "tab": "Robustness",
            "score": 0.5992424242424242
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.025, mean=0.627, max=0.975, sum=20.7 (33)",
            "tab": "Fairness",
            "score": 0.6272727272727272
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.554, max=5, sum=150.275 (33)",
            "tab": "General information",
            "score": 4.553787878787879
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=270.325, mean=813.265, max=1762.475, sum=26837.75 (33)",
            "tab": "General information",
            "score": 813.2651515151515
          },
          "RAFT - # output tokens": {
            "description": "min=0.025, mean=3.15, max=6.8, sum=103.95 (33)",
            "tab": "General information",
            "score": 3.15
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}