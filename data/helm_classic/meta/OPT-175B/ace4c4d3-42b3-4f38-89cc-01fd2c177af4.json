{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/meta_OPT-175B/1770830385.7573261",
  "retrieved_timestamp": "1770830385.7573261",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "OPT 175B",
    "id": "meta/OPT-175B",
    "developer": "meta",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.609,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.33807716905928437
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.5191448151403657
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.6221815633384042
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.24121162280701755
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.58013310485115
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.43513523513523517
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.5927318295739348
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.318,
        "details": {
          "description": "min=0.21, mean=0.318, max=0.48, sum=4.775 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.115, mean=0.147, max=0.194, sum=2.207 (15)",
            "tab": "Calibration",
            "score": 0.14714449343481936
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.13, mean=0.27, max=0.45, sum=4.048 (15)",
            "tab": "Robustness",
            "score": 0.2698479532163743
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.167, mean=0.287, max=0.43, sum=4.298 (15)",
            "tab": "Fairness",
            "score": 0.28651461988304094
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.11, mean=0.12, max=0.138, sum=1.793 (15)",
            "tab": "Efficiency",
            "score": 0.1195572826114746
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=371.38, mean=472.274, max=624.07, sum=7084.111 (15)",
            "tab": "General information",
            "score": 472.2740350877193
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ - EM",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.793,
        "details": {
          "description": "min=0.777, mean=0.793, max=0.813, sum=2.379 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.177, mean=0.194, max=0.218, sum=0.581 (3)",
            "tab": "Calibration",
            "score": 0.19360710050007168
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.584, mean=0.623, max=0.662, sum=1.869 (3)",
            "tab": "Robustness",
            "score": 0.623
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.712, mean=0.731, max=0.746, sum=2.193 (3)",
            "tab": "Fairness",
            "score": 0.731
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.71, mean=0.869, max=0.954, sum=2.608 (3)",
            "tab": "Efficiency",
            "score": 0.869335141547284
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=660.073, mean=908.406, max=1242.073, sum=2725.219 (3)",
            "tab": "General information",
            "score": 908.4063333333334
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.671,
        "details": {
          "description": "min=0.657, mean=0.671, max=0.692, sum=2.013 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.25, mean=0.254, max=0.261, sum=0.763 (3)",
            "tab": "Calibration",
            "score": 0.25442494535286947
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.365, mean=0.409, max=0.447, sum=1.227 (3)",
            "tab": "Robustness",
            "score": 0.4090933797146052
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.545, mean=0.573, max=0.6, sum=1.718 (3)",
            "tab": "Fairness",
            "score": 0.5725951072978767
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=2.375, mean=2.783, max=3.573, sum=8.348 (3)",
            "tab": "Efficiency",
            "score": 2.7825779012238017
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.051, mean=1.647, max=2.085, sum=4.941 (3)",
            "tab": "General information",
            "score": 1.6469483568075116
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1601.955, mean=1652.377, max=1705.003, sum=4957.132 (3)",
            "tab": "General information",
            "score": 1652.3774647887324
          },
          "NarrativeQA - # output tokens": {
            "description": "min=27.152, mean=40.781, max=56.166, sum=122.344 (3)",
            "tab": "General information",
            "score": 40.781220657277
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.472, mean=0.491, max=0.5, sum=1.472 (3)",
            "tab": "Bias",
            "score": 0.49074074074074076
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.213, mean=0.232, max=0.257, sum=0.695 (3)",
            "tab": "Bias",
            "score": 0.23182834585691858
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.017, mean=0.019, max=0.023, sum=0.056 (3)",
            "tab": "Toxicity",
            "score": 0.018779342723004692
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.615,
        "details": {
          "description": "min=0.607, mean=0.615, max=0.619, sum=1.845 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.169, mean=0.173, max=0.178, sum=0.52 (3)",
            "tab": "Calibration",
            "score": 0.17321815784980257
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.365, mean=0.372, max=0.38, sum=1.117 (3)",
            "tab": "Calibration",
            "score": 0.3723122842871363
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.202, mean=0.208, max=0.213, sum=0.623 (3)",
            "tab": "Robustness",
            "score": 0.2076699169323979
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.382, mean=0.408, max=0.445, sum=1.224 (3)",
            "tab": "Robustness",
            "score": 0.40794279599736244
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.244, mean=0.246, max=0.248, sum=0.738 (3)",
            "tab": "Fairness",
            "score": 0.2461285688311032
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.557, mean=0.561, max=0.566, sum=1.684 (3)",
            "tab": "Fairness",
            "score": 0.5613201936765554
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=4.226, mean=4.548, max=4.977, sum=13.645 (3)",
            "tab": "Efficiency",
            "score": 4.5482187833781085
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=6.761, mean=7.78, max=8.516, sum=23.341 (3)",
            "tab": "Efficiency",
            "score": 7.78018927021878
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=110.254, mean=112.254, max=116.254, sum=336.762 (3)",
            "tab": "General information",
            "score": 112.254
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=272.695, mean=278.02, max=287.118, sum=834.059 (3)",
            "tab": "General information",
            "score": 278.01966666666664
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.647, mean=4.691, max=4.724, sum=14.074 (3)",
            "tab": "General information",
            "score": 4.691333333333334
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.036, mean=0.036, max=0.036, sum=0.108 (3)",
            "tab": "General information",
            "score": 0.036
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1231.212, mean=1419.574, max=1523.257, sum=4258.721 (3)",
            "tab": "General information",
            "score": 1419.5736666666664
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=168.53, mean=194.671, max=213.115, sum=584.014 (3)",
            "tab": "General information",
            "score": 194.67133333333334
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.279, mean=0.327, max=0.375, sum=0.654 (2)",
            "tab": "Bias",
            "score": 0.32684426229508196
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.48, mean=0.521, max=0.562, sum=1.563 (3)",
            "tab": "Bias",
            "score": 0.5211641167340236
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.029, mean=0.081, max=0.119, sum=0.243 (3)",
            "tab": "Bias",
            "score": 0.0811320308714203
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.433, mean=0.439, max=0.45, sum=1.317 (3)",
            "tab": "Bias",
            "score": 0.4388888888888889
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.423, mean=0.461, max=0.48, sum=1.384 (3)",
            "tab": "Bias",
            "score": 0.4612918002748511
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.318, mean=0.325, max=0.332, sum=0.974 (3)",
            "tab": "Bias",
            "score": 0.324702218997521
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.001, sum=0.002 (3)",
            "tab": "Toxicity",
            "score": 0.0006666666666666666
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.002, max=0.003, sum=0.005 (3)",
            "tab": "Toxicity",
            "score": 0.0016666666666666668
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC - F1",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.36,
        "details": {
          "description": "min=0.347, mean=0.36, max=0.369, sum=1.08 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.128, mean=0.148, max=0.173, sum=0.443 (3)",
            "tab": "Calibration",
            "score": 0.14774672207107284
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.194, mean=0.2, max=0.209, sum=0.6 (3)",
            "tab": "Robustness",
            "score": 0.2000302607507829
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.255, mean=0.266, max=0.274, sum=0.798 (3)",
            "tab": "Fairness",
            "score": 0.26591098840755784
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=3.951, mean=4.049, max=4.154, sum=12.147 (3)",
            "tab": "Efficiency",
            "score": 4.049007016242971
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.845, mean=0.944, max=1.086, sum=2.833 (3)",
            "tab": "General information",
            "score": 0.9443333333333334
          },
          "QuAC - truncated": {
            "description": "min=0.016, mean=0.016, max=0.016, sum=0.048 (3)",
            "tab": "General information",
            "score": 0.016
          },
          "QuAC - # prompt tokens": {
            "description": "min=1625.523, mean=1644.831, max=1670.605, sum=4934.492 (3)",
            "tab": "General information",
            "score": 1644.8306666666667
          },
          "QuAC - # output tokens": {
            "description": "min=75.972, mean=77.836, max=79.528, sum=233.507 (3)",
            "tab": "General information",
            "score": 77.83566666666667
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.561, mean=0.591, max=0.614, sum=1.773 (3)",
            "tab": "Bias",
            "score": 0.5910808767951625
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.376, mean=0.386, max=0.399, sum=1.159 (3)",
            "tab": "Bias",
            "score": 0.38627685600159944
          },
          "QuAC - Representation (race)": {
            "description": "min=0.167, mean=0.243, max=0.304, sum=0.73 (3)",
            "tab": "Bias",
            "score": 0.2433558772540988
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.195, mean=0.207, max=0.218, sum=0.621 (3)",
            "tab": "Bias",
            "score": 0.2069846056271054
          },
          "QuAC - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.003, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.791,
        "details": {
          "description": "min=0.791, mean=0.791, max=0.791, sum=0.791 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.325, mean=0.325, max=0.325, sum=0.325 (1)",
            "tab": "Calibration",
            "score": 0.324637159664446
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.744, mean=0.744, max=0.744, sum=0.744 (1)",
            "tab": "Robustness",
            "score": 0.744
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.66, mean=0.66, max=0.66, sum=0.66 (1)",
            "tab": "Fairness",
            "score": 0.66
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "min=0.71, mean=0.71, max=0.71, sum=0.71 (1)",
            "tab": "Efficiency",
            "score": 0.7096132577732451
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=87.888, mean=87.888, max=87.888, sum=87.888 (1)",
            "tab": "General information",
            "score": 87.888
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.586,
        "details": {
          "description": "min=0.586, mean=0.586, max=0.586, sum=0.586 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.209, mean=0.209, max=0.209, sum=0.209 (1)",
            "tab": "Calibration",
            "score": 0.20889829455743214
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.488, mean=0.488, max=0.488, sum=0.488 (1)",
            "tab": "Robustness",
            "score": 0.488
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Fairness",
            "score": 0.5
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "min=0.038, mean=0.038, max=0.038, sum=0.038 (1)",
            "tab": "Efficiency",
            "score": 0.03760148134353242
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.27, mean=5.27, max=5.27, sum=5.27 (1)",
            "tab": "General information",
            "score": 5.27
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.25,
        "details": {
          "description": "min=0.228, mean=0.25, max=0.269, sum=1.002 (4)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.042, mean=0.054, max=0.061, sum=0.216 (4)",
            "tab": "Calibration",
            "score": 0.05404322346973557
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.167, mean=0.205, max=0.249, sum=0.818 (4)",
            "tab": "Robustness",
            "score": 0.20451070336391436
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.165, mean=0.203, max=0.249, sum=0.812 (4)",
            "tab": "Fairness",
            "score": 0.2029816513761468
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.079, mean=0.141, max=0.246, sum=0.563 (4)",
            "tab": "Efficiency",
            "score": 0.1406602569641055
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=2616 (4)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=0, mean=3.75, max=5, sum=15 (4)",
            "tab": "General information",
            "score": 3.75
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (4)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=85.121, mean=404.621, max=529.121, sum=1618.483 (4)",
            "tab": "General information",
            "score": 404.62079510703364
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=4 (4)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=2.5, max=3, sum=10 (4)",
            "tab": "General information",
            "score": 2.5
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.448,
        "details": {
          "description": "min=0.425, mean=0.448, max=0.467, sum=1.344 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.198, mean=0.235, max=0.263, sum=0.705 (3)",
            "tab": "Robustness",
            "score": 0.23496613756613724
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.386, mean=0.408, max=0.422, sum=1.225 (3)",
            "tab": "Robustness",
            "score": 0.4083455179340017
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.229, mean=0.26, max=0.288, sum=0.779 (3)",
            "tab": "Fairness",
            "score": 0.25959669312169276
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.4, mean=0.419, max=0.428, sum=1.256 (3)",
            "tab": "Fairness",
            "score": 0.41868435186381264
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "min=0.229, mean=0.241, max=0.262, sum=0.724 (3)",
            "tab": "Efficiency",
            "score": 0.24148347487755295
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "min=0.19, mean=0.226, max=0.254, sum=0.678 (3)",
            "tab": "Efficiency",
            "score": 0.2261325473631569
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=495.232, mean=532.565, max=577.232, sum=1597.696 (3)",
            "tab": "General information",
            "score": 532.5653333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=478.488, mean=515.822, max=560.488, sum=1547.465 (3)",
            "tab": "General information",
            "score": 515.8217054263565
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.146,
        "details": {
          "description": "min=0.132, mean=0.146, max=0.156, sum=0.875 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=4.705, mean=4.729, max=4.742, sum=28.373 (6)",
            "tab": "Efficiency",
            "score": 4.728843353285813
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1531.586, mean=1549.919, max=1567.586, sum=9299.515 (6)",
            "tab": "General information",
            "score": 1549.9191702432045
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=72.006, mean=73.533, max=75.564, sum=441.197 (6)",
            "tab": "General information",
            "score": 73.53290414878398
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.584, mean=0.591, max=0.602, sum=3.548 (6)",
            "tab": "Bias",
            "score": 0.5912557147615382
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.389, mean=0.407, max=0.423, sum=2.439 (6)",
            "tab": "Bias",
            "score": 0.406575836707982
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.258, mean=0.294, max=0.328, sum=1.765 (6)",
            "tab": "Bias",
            "score": 0.29422007838910086
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.109, mean=0.123, max=0.15, sum=0.74 (6)",
            "tab": "Bias",
            "score": 0.1233558384477443
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.004 (6)",
            "tab": "Toxicity",
            "score": 0.000715307582260372
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.094, mean=0.202, max=0.259, sum=0.605 (3)",
            "tab": "Summarization metrics",
            "score": 0.20179927196685032
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "min=4.642, mean=4.67, max=4.721, sum=28.022 (6)",
            "tab": "Summarization metrics",
            "score": 4.67041236939807
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.234, mean=0.276, max=0.301, sum=0.827 (3)",
            "tab": "Summarization metrics",
            "score": 0.2755570292220846
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.855, mean=0.933, max=0.973, sum=5.599 (6)",
            "tab": "Summarization metrics",
            "score": 0.9331599358896452
          },
          "CNN/DailyMail - Density": {
            "description": "min=28.251, mean=31.307, max=33.584, sum=187.839 (6)",
            "tab": "Summarization metrics",
            "score": 31.306505459997258
          },
          "CNN/DailyMail - Compression": {
            "description": "min=9.442, mean=9.8, max=10.068, sum=58.802 (6)",
            "tab": "Summarization metrics",
            "score": 9.800322939057557
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "min=1, mean=1, max=1, sum=6 (6)",
            "tab": "Summarization metrics",
            "score": 1.0
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "min=4.333, mean=4.378, max=4.467, sum=26.267 (6)",
            "tab": "Summarization metrics",
            "score": 4.377777777777777
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "min=2.833, mean=3.233, max=3.867, sum=19.4 (6)",
            "tab": "Summarization metrics",
            "score": 3.233333333333333
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.155,
        "details": {
          "description": "min=0.153, mean=0.155, max=0.158, sum=0.929 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=2.509, mean=2.523, max=2.545, sum=15.138 (6)",
            "tab": "Efficiency",
            "score": 2.522969657178858
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.998, mean=4.999, max=5, sum=29.992 (6)",
            "tab": "General information",
            "score": 4.998712998712999
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1456.402, mean=1510.418, max=1538.921, sum=9062.51 (6)",
            "tab": "General information",
            "score": 1510.4182754182755
          },
          "XSUM - # output tokens": {
            "description": "min=26.037, mean=26.229, max=26.481, sum=157.375 (6)",
            "tab": "General information",
            "score": 26.22908622908623
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.444, mean=0.449, max=0.459, sum=2.697 (6)",
            "tab": "Bias",
            "score": 0.44948914431673054
          },
          "XSUM - Representation (race)": {
            "description": "min=0.429, mean=0.453, max=0.481, sum=2.719 (6)",
            "tab": "Bias",
            "score": 0.45310942412391686
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.188, mean=0.218, max=0.235, sum=1.309 (6)",
            "tab": "Bias",
            "score": 0.21820243248814677
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.008 (6)",
            "tab": "Toxicity",
            "score": 0.001287001287001287
          },
          "XSUM - SummaC": {
            "description": "min=-0.271, mean=-0.253, max=-0.224, sum=-0.76 (3)",
            "tab": "Summarization metrics",
            "score": -0.25337265715073337
          },
          "XSUM - QAFactEval": {
            "description": "min=3.343, mean=3.523, max=3.7, sum=21.139 (6)",
            "tab": "Summarization metrics",
            "score": 3.5231601957035803
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.458, mean=0.46, max=0.461, sum=1.38 (3)",
            "tab": "Summarization metrics",
            "score": 0.45990517032509515
          },
          "XSUM - Coverage": {
            "description": "min=0.792, mean=0.793, max=0.795, sum=4.76 (6)",
            "tab": "Summarization metrics",
            "score": 0.7933759020774565
          },
          "XSUM - Density": {
            "description": "min=2.672, mean=2.732, max=2.852, sum=16.393 (6)",
            "tab": "Summarization metrics",
            "score": 2.732196710488823
          },
          "XSUM - Compression": {
            "description": "min=16.442, mean=16.792, max=17.056, sum=100.753 (6)",
            "tab": "Summarization metrics",
            "score": 16.79220871639349
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "min=0.583, mean=0.798, max=0.944, sum=4.789 (6)",
            "tab": "Summarization metrics",
            "score": 0.7981481481481479
          },
          "XSUM - HumanEval-relevance": {
            "description": "min=4.167, mean=4.3, max=4.4, sum=25.8 (6)",
            "tab": "Summarization metrics",
            "score": 4.300000000000001
          },
          "XSUM - HumanEval-coherence": {
            "description": "min=4.867, mean=4.891, max=4.917, sum=29.344 (6)",
            "tab": "Summarization metrics",
            "score": 4.890740740740742
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB - EM",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.947,
        "details": {
          "description": "min=0.932, mean=0.947, max=0.96, sum=2.842 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.164, mean=0.19, max=0.216, sum=0.569 (3)",
            "tab": "Calibration",
            "score": 0.18962950165784687
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.904, mean=0.919, max=0.937, sum=2.756 (3)",
            "tab": "Robustness",
            "score": 0.9186666666666667
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.929, mean=0.944, max=0.958, sum=2.831 (3)",
            "tab": "Fairness",
            "score": 0.9436666666666667
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=1.488, mean=1.575, max=1.732, sum=4.724 (3)",
            "tab": "Efficiency",
            "score": 1.5747312279142403
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.846, mean=4.933, max=4.986, sum=14.798 (3)",
            "tab": "General information",
            "score": 4.932666666666667
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1152.694, mean=1389.454, max=1744.631, sum=4168.363 (3)",
            "tab": "General information",
            "score": 1389.4543333333331
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments - EM",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.505,
        "details": {
          "description": "min=0, mean=0.505, max=1, sum=27.251 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.226, mean=0.462, max=0.633, sum=24.957 (54)",
            "tab": "Calibration",
            "score": 0.46216217374926066
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.184, max=0.769, sum=9.952 (54)",
            "tab": "Robustness",
            "score": 0.18428995439708568
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.491, max=1, sum=26.489 (54)",
            "tab": "Fairness",
            "score": 0.4905409716584098
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.299, mean=0.498, max=0.974, sum=26.871 (54)",
            "tab": "Efficiency",
            "score": 0.4976179389529128
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=356.537, mean=722.635, max=1267.519, sum=39022.317 (54)",
            "tab": "General information",
            "score": 722.6354931173206
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT - EM",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.606,
        "details": {
          "description": "min=0.075, mean=0.606, max=0.975, sum=20 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.1, mean=0.352, max=0.74, sum=11.606 (33)",
            "tab": "Calibration",
            "score": 0.35168585204039804
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.48, max=0.975, sum=15.85 (33)",
            "tab": "Robustness",
            "score": 0.4803030303030303
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.075, mean=0.58, max=0.975, sum=19.125 (33)",
            "tab": "Fairness",
            "score": 0.5795454545454547
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.403, mean=0.962, max=1.712, sum=31.76 (33)",
            "tab": "Efficiency",
            "score": 0.9624239013413396
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.556, max=5, sum=150.35 (33)",
            "tab": "General information",
            "score": 4.556060606060607
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=257.35, mean=812.938, max=1773.675, sum=26826.95 (33)",
            "tab": "General information",
            "score": 812.937878787879
          },
          "RAFT - # output tokens": {
            "description": "min=5, mean=9.057, max=18.95, sum=298.875 (33)",
            "tab": "General information",
            "score": 9.056818181818182
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}