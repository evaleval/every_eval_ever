{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/meta_LLaMA-65B/1770834891.1472661",
  "retrieved_timestamp": "1770834891.1472661",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "LLaMA 65B",
    "id": "meta/LLaMA-65B",
    "developer": "meta",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.908,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": null
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.8851981351981352
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.9235431235431235
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.4059399223461723
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.5910839160839161
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.584,
        "details": {
          "description": "min=0.34, mean=0.584, max=0.89, sum=2.919 (5)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.27, mean=0.504, max=0.81, sum=2.518 (5)",
            "tab": "Robustness",
            "score": 0.5036842105263158
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.34, mean=0.551, max=0.84, sum=2.757 (5)",
            "tab": "Fairness",
            "score": 0.5514385964912281
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=397.65, mean=522.547, max=684.675, sum=2612.735 (5)",
            "tab": "General information",
            "score": 522.5470877192982
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on BoolQ",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.871,
        "details": {
          "description": "min=0.871, mean=0.871, max=0.871, sum=0.871 (1)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.84, mean=0.84, max=0.84, sum=0.84 (1)",
            "tab": "Robustness",
            "score": 0.84
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.847, mean=0.847, max=0.847, sum=0.847 (1)",
            "tab": "Fairness",
            "score": 0.847
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=1439.447, mean=1439.447, max=1439.447, sum=1439.447 (1)",
            "tab": "General information",
            "score": 1439.447
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.755,
        "details": {
          "description": "min=0.755, mean=0.755, max=0.755, sum=0.755 (1)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.567, mean=0.567, max=0.567, sum=0.567 (1)",
            "tab": "Robustness",
            "score": 0.5674436891870642
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.661, mean=0.661, max=0.661, sum=0.661 (1)",
            "tab": "Fairness",
            "score": 0.6614214785759094
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.437, mean=1.437, max=1.437, sum=1.437 (1)",
            "tab": "General information",
            "score": 1.4366197183098592
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1541.115, mean=1541.115, max=1541.115, sum=1541.115 (1)",
            "tab": "General information",
            "score": 1541.1154929577465
          },
          "NarrativeQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NarrativeQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.198, mean=0.198, max=0.198, sum=0.198 (1)",
            "tab": "Bias",
            "score": 0.1981132075471698
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.008, mean=0.008, max=0.008, sum=0.008 (1)",
            "tab": "Toxicity",
            "score": 0.008450704225352112
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (open-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.672,
        "details": {
          "description": "min=0.672, mean=0.672, max=0.672, sum=0.672 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.388, mean=0.388, max=0.388, sum=0.388 (1)",
            "tab": "Robustness",
            "score": 0.3875883665002626
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.624, mean=0.624, max=0.624, sum=0.624 (1)",
            "tab": "Robustness",
            "score": 0.623794662165915
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.375, mean=0.375, max=0.375, sum=0.375 (1)",
            "tab": "Fairness",
            "score": 0.3753249636782112
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.633, mean=0.633, max=0.633, sum=0.633 (1)",
            "tab": "Fairness",
            "score": 0.6326996444457361
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=137.383, mean=137.383, max=137.383, sum=137.383 (1)",
            "tab": "General information",
            "score": 137.383
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=3.722, mean=3.722, max=3.722, sum=3.722 (1)",
            "tab": "General information",
            "score": 3.722
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.049, mean=0.049, max=0.049, sum=0.049 (1)",
            "tab": "General information",
            "score": 0.049
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1407.178, mean=1407.178, max=1407.178, sum=1407.178 (1)",
            "tab": "General information",
            "score": 1407.178
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=0.987, mean=0.987, max=0.987, sum=0.987 (1)",
            "tab": "General information",
            "score": 0.987
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.352, mean=0.352, max=0.352, sum=0.352 (1)",
            "tab": "Bias",
            "score": 0.35238095238095235
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.3, mean=0.3, max=0.3, sum=0.3 (1)",
            "tab": "Bias",
            "score": 0.30000000000000004
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.436, mean=0.436, max=0.436, sum=0.436 (1)",
            "tab": "Bias",
            "score": 0.4358974358974359
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.393, mean=0.393, max=0.393, sum=0.393 (1)",
            "tab": "Bias",
            "score": 0.3928571428571429
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.001 (1)",
            "tab": "Toxicity",
            "score": 0.001
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on QuAC",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.401,
        "details": {
          "description": "min=0.401, mean=0.401, max=0.401, sum=0.401 (1)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.275, mean=0.275, max=0.275, sum=0.275 (1)",
            "tab": "Robustness",
            "score": 0.2748605351114493
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
            "tab": "Fairness",
            "score": 0.33296543407590734
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.507, mean=0.507, max=0.507, sum=0.507 (1)",
            "tab": "General information",
            "score": 0.507
          },
          "QuAC - truncated": {
            "description": "min=0.06, mean=0.06, max=0.06, sum=0.06 (1)",
            "tab": "General information",
            "score": 0.06
          },
          "QuAC - # prompt tokens": {
            "description": "min=1498.657, mean=1498.657, max=1498.657, sum=1498.657 (1)",
            "tab": "General information",
            "score": 1498.657
          },
          "QuAC - # output tokens": {
            "description": "min=0.997, mean=0.997, max=0.997, sum=0.997 (1)",
            "tab": "General information",
            "score": 0.997
          },
          "QuAC - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.621, mean=0.621, max=0.621, sum=0.621 (1)",
            "tab": "Bias",
            "score": 0.6210526315789473
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.394, mean=0.394, max=0.394, sum=0.394 (1)",
            "tab": "Bias",
            "score": 0.3944670750705233
          },
          "QuAC - Representation (race)": {
            "description": "min=0.38, mean=0.38, max=0.38, sum=0.38 (1)",
            "tab": "Bias",
            "score": 0.3804713804713804
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.243, mean=0.243, max=0.243, sum=0.243 (1)",
            "tab": "Bias",
            "score": 0.24335260115606938
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.003, mean=0.003, max=0.003, sum=0.003 (1)",
            "tab": "Toxicity",
            "score": 0.003
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on HellaSwag",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "HellaSwag - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on TruthfulQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.508,
        "details": {
          "description": "min=0.508, mean=0.508, max=0.508, sum=0.508 (1)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.448, mean=0.448, max=0.448, sum=0.448 (1)",
            "tab": "Robustness",
            "score": 0.44801223241590216
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.42, mean=0.42, max=0.42, sum=0.42 (1)",
            "tab": "Fairness",
            "score": 0.42048929663608564
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=654 (1)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=524.602, mean=524.602, max=524.602, sum=524.602 (1)",
            "tab": "General information",
            "score": 524.6024464831804
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC)",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "NDCG@10 on MS MARCO (TREC)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on CNN/DailyMail",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "CNN/DailyMail - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on XSUM",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "XSUM - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on IMDB",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.962,
        "details": {
          "description": "min=0.962, mean=0.962, max=0.962, sum=0.962 (1)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.935, mean=0.935, max=0.935, sum=0.935 (1)",
            "tab": "Robustness",
            "score": 0.935
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.953, mean=0.953, max=0.953, sum=0.953 (1)",
            "tab": "Fairness",
            "score": 0.953
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.781, mean=2.781, max=2.781, sum=2.781 (1)",
            "tab": "General information",
            "score": 2.781
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1751.213, mean=1751.213, max=1751.213, sum=1751.213 (1)",
            "tab": "General information",
            "score": 1751.213
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on CivilComments",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.655,
        "details": {
          "description": "min=0.395, mean=0.655, max=0.863, sum=11.783 (18)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.247, mean=0.566, max=0.853, sum=10.188 (18)",
            "tab": "Robustness",
            "score": 0.565986035612513
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.32, mean=0.574, max=0.8, sum=10.336 (18)",
            "tab": "Fairness",
            "score": 0.57420608635975
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=6688 (18)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=90 (18)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (18)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=404.732, mean=855.241, max=1417.567, sum=15394.339 (18)",
            "tab": "General information",
            "score": 855.2410378605821
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=18 (18)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=1, mean=1, max=1, sum=18 (18)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on RAFT",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.702,
        "details": {
          "description": "min=0.125, mean=0.702, max=0.975, sum=7.725 (11)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.655, max=0.975, sum=7.2 (11)",
            "tab": "Robustness",
            "score": 0.6545454545454545
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.075, mean=0.668, max=0.975, sum=7.35 (11)",
            "tab": "Fairness",
            "score": 0.6681818181818182
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=440 (11)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0.45, mean=4.552, max=5, sum=50.075 (11)",
            "tab": "General information",
            "score": 4.552272727272727
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (11)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=303.675, mean=954.111, max=1882.1, sum=10495.225 (11)",
            "tab": "General information",
            "score": 954.1113636363635
          },
          "RAFT - # output tokens": {
            "description": "min=0.8, mean=0.982, max=1, sum=10.8 (11)",
            "tab": "General information",
            "score": 0.9818181818181819
          },
          "RAFT - # trials": {
            "description": "min=1, mean=1, max=1, sum=11 (11)",
            "tab": "General information",
            "score": 1.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}