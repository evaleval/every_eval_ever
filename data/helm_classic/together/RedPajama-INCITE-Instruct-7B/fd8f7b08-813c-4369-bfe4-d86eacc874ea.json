{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/together_RedPajama-INCITE-Instruct-7B/1768090731.5328572",
  "retrieved_timestamp": "1768090731.5328572",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "RedPajama-INCITE-Instruct 7B",
    "id": "together/RedPajama-INCITE-Instruct-7B",
    "developer": "together",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.524,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.38751156336725257
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.4953146853146853
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.46615384615384614
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.33794748465968927
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.29364801864801865
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.363,
        "details": {
          "description": "min=0.246, mean=0.363, max=0.52, sum=1.816 (5)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.092, mean=0.143, max=0.182, sum=0.715 (5)",
            "tab": "Calibration",
            "score": 0.14292977551638825
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.175, mean=0.291, max=0.46, sum=1.455 (5)",
            "tab": "Robustness",
            "score": 0.2910877192982456
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.167, mean=0.305, max=0.48, sum=1.527 (5)",
            "tab": "Fairness",
            "score": 0.30533333333333335
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=358.76, mean=467.936, max=612.798, sum=2339.678 (5)",
            "tab": "General information",
            "score": 467.935649122807
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.705,
        "details": {
          "description": "min=0.705, mean=0.705, max=0.705, sum=0.705 (1)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.035, mean=0.035, max=0.035, sum=0.035 (1)",
            "tab": "Calibration",
            "score": 0.034644312737608846
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.599, mean=0.599, max=0.599, sum=0.599 (1)",
            "tab": "Robustness",
            "score": 0.599
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.616, mean=0.616, max=0.616, sum=0.616 (1)",
            "tab": "Fairness",
            "score": 0.616
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=1251.897, mean=1251.897, max=1251.897, sum=1251.897 (1)",
            "tab": "General information",
            "score": 1251.897
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.638,
        "details": {
          "description": "min=0.638, mean=0.638, max=0.638, sum=0.638 (1)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.247, mean=0.247, max=0.247, sum=0.247 (1)",
            "tab": "Calibration",
            "score": 0.24703559378209236
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.482, mean=0.482, max=0.482, sum=0.482 (1)",
            "tab": "Robustness",
            "score": 0.4816661888359549
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.506, mean=0.506, max=0.506, sum=0.506 (1)",
            "tab": "Fairness",
            "score": 0.5062845788047843
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.969, mean=1.969, max=1.969, sum=1.969 (1)",
            "tab": "General information",
            "score": 1.9690140845070423
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1691.082, mean=1691.082, max=1691.082, sum=1691.082 (1)",
            "tab": "General information",
            "score": 1691.081690140845
          },
          "NarrativeQA - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=100 (1)",
            "tab": "General information",
            "score": 100.0
          },
          "NarrativeQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NarrativeQA - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.193, mean=0.193, max=0.193, sum=0.193 (1)",
            "tab": "Bias",
            "score": 0.19318181818181815
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.025, mean=0.025, max=0.025, sum=0.025 (1)",
            "tab": "Toxicity",
            "score": 0.02535211267605634
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.659,
        "details": {
          "description": "min=0.659, mean=0.659, max=0.659, sum=0.659 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.142, mean=0.142, max=0.142, sum=0.142 (1)",
            "tab": "Calibration",
            "score": 0.14200000000000002
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.466, mean=0.466, max=0.466, sum=0.466 (1)",
            "tab": "Calibration",
            "score": 0.4659999973351183
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.137, mean=0.137, max=0.137, sum=0.137 (1)",
            "tab": "Robustness",
            "score": 0.13717330495393032
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.547, mean=0.547, max=0.547, sum=0.547 (1)",
            "tab": "Robustness",
            "score": 0.5468327185577326
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.164, mean=0.164, max=0.164, sum=0.164 (1)",
            "tab": "Fairness",
            "score": 0.16419040044922398
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.592, mean=0.592, max=0.592, sum=0.592 (1)",
            "tab": "Fairness",
            "score": 0.5920301139461878
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=117.299, mean=117.299, max=117.299, sum=117.299 (1)",
            "tab": "General information",
            "score": 117.299
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=300 (1)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.704, mean=4.704, max=4.704, sum=4.704 (1)",
            "tab": "General information",
            "score": 4.704
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.037, mean=0.037, max=0.037, sum=0.037 (1)",
            "tab": "General information",
            "score": 0.037
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1495.552, mean=1495.552, max=1495.552, sum=1495.552 (1)",
            "tab": "General information",
            "score": 1495.552
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=300 (1)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.406, mean=0.406, max=0.406, sum=0.406 (1)",
            "tab": "Bias",
            "score": 0.4061624649859944
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "Bias",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.524, mean=0.524, max=0.524, sum=0.524 (1)",
            "tab": "Bias",
            "score": 0.5238095238095237
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.281, mean=0.281, max=0.281, sum=0.281 (1)",
            "tab": "Bias",
            "score": 0.28125
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.001 (1)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.26,
        "details": {
          "description": "min=0.26, mean=0.26, max=0.26, sum=0.26 (1)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.074, mean=0.074, max=0.074, sum=0.074 (1)",
            "tab": "Calibration",
            "score": 0.07389119661461117
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.164, mean=0.164, max=0.164, sum=0.164 (1)",
            "tab": "Robustness",
            "score": 0.16438450644529176
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.181, mean=0.181, max=0.181, sum=0.181 (1)",
            "tab": "Fairness",
            "score": 0.18079535886869938
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.883, mean=0.883, max=0.883, sum=0.883 (1)",
            "tab": "General information",
            "score": 0.883
          },
          "QuAC - truncated": {
            "description": "min=0.021, mean=0.021, max=0.021, sum=0.021 (1)",
            "tab": "General information",
            "score": 0.021
          },
          "QuAC - # prompt tokens": {
            "description": "min=1655.708, mean=1655.708, max=1655.708, sum=1655.708 (1)",
            "tab": "General information",
            "score": 1655.708
          },
          "QuAC - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=100 (1)",
            "tab": "General information",
            "score": 100.0
          },
          "QuAC - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.63, mean=0.63, max=0.63, sum=0.63 (1)",
            "tab": "Bias",
            "score": 0.6296296296296297
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.445, mean=0.445, max=0.445, sum=0.445 (1)",
            "tab": "Bias",
            "score": 0.4446840232318048
          },
          "QuAC - Representation (race)": {
            "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
            "tab": "Bias",
            "score": 0.33333333333333337
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.242, mean=0.242, max=0.242, sum=0.242 (1)",
            "tab": "Bias",
            "score": 0.24226804123711343
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.003, mean=0.003, max=0.003, sum=0.003 (1)",
            "tab": "Toxicity",
            "score": 0.003
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "HellaSwag - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.243,
        "details": {
          "description": "min=0.243, mean=0.243, max=0.243, sum=0.243 (1)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.232, mean=0.232, max=0.232, sum=0.232 (1)",
            "tab": "Calibration",
            "score": 0.23215642305686054
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.197, mean=0.197, max=0.197, sum=0.197 (1)",
            "tab": "Robustness",
            "score": 0.19724770642201836
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.183, mean=0.183, max=0.183, sum=0.183 (1)",
            "tab": "Fairness",
            "score": 0.1834862385321101
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=654 (1)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.352, mean=505.352, max=505.352, sum=505.352 (1)",
            "tab": "General information",
            "score": 505.35168195718654
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "CNN/DailyMail - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "XSUM - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.927,
        "details": {
          "description": "min=0.927, mean=0.927, max=0.927, sum=0.927 (1)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.159, mean=0.159, max=0.159, sum=0.159 (1)",
            "tab": "Calibration",
            "score": 0.15862422483580252
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.82, mean=0.82, max=0.82, sum=0.82 (1)",
            "tab": "Robustness",
            "score": 0.82
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.907, mean=0.907, max=0.907, sum=0.907 (1)",
            "tab": "Fairness",
            "score": 0.907
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.911, mean=2.911, max=2.911, sum=2.911 (1)",
            "tab": "General information",
            "score": 2.911
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1619.568, mean=1619.568, max=1619.568, sum=1619.568 (1)",
            "tab": "General information",
            "score": 1619.568
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.664,
        "details": {
          "description": "min=0.487, mean=0.664, max=0.77, sum=11.961 (18)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.035, mean=0.102, max=0.234, sum=1.831 (18)",
            "tab": "Calibration",
            "score": 0.10174488153691034
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.277, mean=0.527, max=0.77, sum=9.491 (18)",
            "tab": "Robustness",
            "score": 0.5272697486345442
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.25, mean=0.54, max=0.743, sum=9.724 (18)",
            "tab": "Fairness",
            "score": 0.5401968527212513
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=6688 (18)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=90 (18)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (18)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=360.976, mean=771.654, max=1282.4, sum=13889.772 (18)",
            "tab": "General information",
            "score": 771.6539847352628
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=90 (18)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=1, mean=1, max=1, sum=18 (18)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.695,
        "details": {
          "description": "min=0.175, mean=0.695, max=0.925, sum=7.65 (11)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.175, mean=0.695, max=0.925, sum=7.647 (11)",
            "tab": "Calibration",
            "score": 0.69518288885631
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0.175, mean=0.605, max=0.9, sum=6.65 (11)",
            "tab": "Robustness",
            "score": 0.6045454545454546
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.175, mean=0.67, max=0.875, sum=7.375 (11)",
            "tab": "Fairness",
            "score": 0.6704545454545454
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=440 (11)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0.7, mean=4.605, max=5, sum=50.65 (11)",
            "tab": "General information",
            "score": 4.6045454545454545
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (11)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=280.35, mean=869.691, max=1756.575, sum=9566.6 (11)",
            "tab": "General information",
            "score": 869.6909090909089
          },
          "RAFT - # output tokens": {
            "description": "min=30, mean=30, max=30, sum=330 (11)",
            "tab": "General information",
            "score": 30.0
          },
          "RAFT - # trials": {
            "description": "min=1, mean=1, max=1, sum=11 (11)",
            "tab": "General information",
            "score": 1.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    }
  ]
}