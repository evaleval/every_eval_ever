{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/google_UL2-20B/1768090731.5328572",
  "retrieved_timestamp": "1768090731.5328572",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "UL2 20B",
    "id": "google/UL2-20B",
    "developer": "google",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.167,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.464477335800185
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.2572027972027972
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.1858974358974359
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.5056944444444444
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5601766236691538
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.2902378485711819
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.11842105263157894
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.291,
        "details": {
          "description": "min=0.2, mean=0.291, max=0.39, sum=4.368 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.084, mean=0.134, max=0.202, sum=2.004 (15)",
            "tab": "Calibration",
            "score": 0.13362255376880447
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.2, mean=0.272, max=0.37, sum=4.079 (15)",
            "tab": "Robustness",
            "score": 0.2719415204678362
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.19, mean=0.273, max=0.36, sum=4.102 (15)",
            "tab": "Fairness",
            "score": 0.2734502923976609
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.178, mean=0.182, max=0.184, sum=2.725 (15)",
            "tab": "Efficiency",
            "score": 0.18164482078684702
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=2.465, mean=4.316, max=5, sum=64.743 (15)",
            "tab": "General information",
            "score": 4.316222222222222
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=385.228, mean=423.395, max=467.79, sum=6350.919 (15)",
            "tab": "General information",
            "score": 423.39457309941525
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.746,
        "details": {
          "description": "min=0.717, mean=0.746, max=0.762, sum=2.237 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.416, mean=0.46, max=0.512, sum=1.379 (3)",
            "tab": "Calibration",
            "score": 0.45980755585445926
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.638, mean=0.646, max=0.651, sum=1.938 (3)",
            "tab": "Robustness",
            "score": 0.646
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.672, mean=0.698, max=0.714, sum=2.095 (3)",
            "tab": "Fairness",
            "score": 0.6983333333333334
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.292, mean=0.313, max=0.341, sum=0.938 (3)",
            "tab": "Efficiency",
            "score": 0.3127442524572212
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=0.953, mean=1.57, max=1.978, sum=4.709 (3)",
            "tab": "General information",
            "score": 1.5696666666666668
          },
          "BoolQ - truncated": {
            "description": "min=0.004, mean=0.004, max=0.004, sum=0.012 (3)",
            "tab": "General information",
            "score": 0.004
          },
          "BoolQ - # prompt tokens": {
            "description": "min=386.826, mean=402.285, max=424.449, sum=1206.854 (3)",
            "tab": "General information",
            "score": 402.2846666666667
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "min=0.167, mean=0.23, max=0.357, sum=0.69 (3)",
            "tab": "Bias",
            "score": 0.23015873015873015
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.083,
        "details": {
          "description": "min=0.083, mean=0.083, max=0.083, sum=0.248 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.0, mean=0.0, max=0.0, sum=0.0 (3)",
            "tab": "Calibration",
            "score": 4.840114578300129e-6
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.059, mean=0.059, max=0.059, sum=0.178 (3)",
            "tab": "Robustness",
            "score": 0.05920683866208649
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.053, mean=0.053, max=0.053, sum=0.159 (3)",
            "tab": "Fairness",
            "score": 0.05305645886768214
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=1.182, mean=1.182, max=1.182, sum=3.546 (3)",
            "tab": "Efficiency",
            "score": 1.1820060481894892
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0.834, mean=0.834, max=0.834, sum=2.501 (3)",
            "tab": "General information",
            "score": 0.8338028169014086
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=492.876, mean=492.876, max=492.876, sum=1478.628 (3)",
            "tab": "General information",
            "score": 492.87605633802815
          },
          "NarrativeQA - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=300 (3)",
            "tab": "General information",
            "score": 100.0
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.337, mean=0.337, max=0.337, sum=1.01 (3)",
            "tab": "Bias",
            "score": 0.3368016513369257
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.342, mean=0.342, max=0.342, sum=1.026 (3)",
            "tab": "Bias",
            "score": 0.3419913419913419
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.154, mean=0.154, max=0.154, sum=0.462 (3)",
            "tab": "Bias",
            "score": 0.15399534522885955
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.017, mean=0.017, max=0.017, sum=0.051 (3)",
            "tab": "Toxicity",
            "score": 0.016901408450704224
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.349,
        "details": {
          "description": "min=0.195, mean=0.349, max=0.432, sum=1.048 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.088, mean=0.092, max=0.095, sum=0.276 (3)",
            "tab": "Calibration",
            "score": 0.09200000000000001
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.028, mean=0.179, max=0.258, sum=0.537 (3)",
            "tab": "Calibration",
            "score": 0.17899999902043598
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.139, mean=0.141, max=0.143, sum=0.423 (3)",
            "tab": "Robustness",
            "score": 0.1409495030072503
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.154, mean=0.291, max=0.365, sum=0.872 (3)",
            "tab": "Robustness",
            "score": 0.2906387285430619
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.159, mean=0.162, max=0.167, sum=0.486 (3)",
            "tab": "Fairness",
            "score": 0.16184307849771043
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.153, mean=0.303, max=0.389, sum=0.908 (3)",
            "tab": "Fairness",
            "score": 0.30281096844711025
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=1.912, mean=1.994, max=2.142, sum=5.981 (3)",
            "tab": "Efficiency",
            "score": 1.993551874854462
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=2.941, mean=3.093, max=3.306, sum=9.279 (3)",
            "tab": "Efficiency",
            "score": 3.0931644739895567
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=113.556, mean=117.556, max=122.556, sum=352.668 (3)",
            "tab": "General information",
            "score": 117.556
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=900 (3)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=0.083, mean=0.918, max=1.789, sum=2.755 (3)",
            "tab": "General information",
            "score": 0.9183333333333333
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.097, mean=0.355, max=0.852, sum=1.064 (3)",
            "tab": "General information",
            "score": 0.3546666666666667
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=231.47, mean=303.619, max=343.479, sum=910.857 (3)",
            "tab": "General information",
            "score": 303.61899999999997
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=900 (3)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.333, mean=0.387, max=0.44, sum=1.162 (3)",
            "tab": "Bias",
            "score": 0.3874074074074074
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.444, mean=0.519, max=0.562, sum=1.558 (3)",
            "tab": "Bias",
            "score": 0.5194689485314483
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.079, mean=0.183, max=0.239, sum=0.549 (3)",
            "tab": "Bias",
            "score": 0.1829490113242974
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.41, mean=0.449, max=0.5, sum=1.346 (3)",
            "tab": "Bias",
            "score": 0.44858553791887124
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.451, mean=0.538, max=0.595, sum=1.615 (3)",
            "tab": "Bias",
            "score": 0.5381999649472214
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.069, mean=0.111, max=0.136, sum=0.332 (3)",
            "tab": "Bias",
            "score": 0.11064384639781977
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.144,
        "details": {
          "description": "min=0.144, mean=0.144, max=0.144, sum=0.433 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.0, mean=0.0, max=0.0, sum=0.0 (3)",
            "tab": "Calibration",
            "score": 0.00013015946539738277
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.111, mean=0.111, max=0.111, sum=0.333 (3)",
            "tab": "Robustness",
            "score": 0.11096938073772407
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.107, mean=0.107, max=0.107, sum=0.32 (3)",
            "tab": "Fairness",
            "score": 0.10672699918485114
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=1.226, mean=1.226, max=1.226, sum=3.679 (3)",
            "tab": "Efficiency",
            "score": 1.2264695519389521
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "QuAC - truncated": {
            "description": "min=0.999, mean=0.999, max=0.999, sum=2.997 (3)",
            "tab": "General information",
            "score": 0.999
          },
          "QuAC - # prompt tokens": {
            "description": "min=510.938, mean=510.938, max=510.938, sum=1532.814 (3)",
            "tab": "General information",
            "score": 510.93799999999993
          },
          "QuAC - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=300 (3)",
            "tab": "General information",
            "score": 100.0
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.614, mean=0.614, max=0.614, sum=1.843 (3)",
            "tab": "Bias",
            "score": 0.6143486267149368
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.402, mean=0.402, max=0.402, sum=1.207 (3)",
            "tab": "Bias",
            "score": 0.40228575253954807
          },
          "QuAC - Representation (race)": {
            "description": "min=0.317, mean=0.317, max=0.317, sum=0.951 (3)",
            "tab": "Bias",
            "score": 0.3169129720853858
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.253, mean=0.253, max=0.253, sum=0.758 (3)",
            "tab": "Bias",
            "score": 0.2525635309852876
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.006, mean=0.006, max=0.006, sum=0.018 (3)",
            "tab": "Toxicity",
            "score": 0.006000000000000001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "HellaSwag - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.193,
        "details": {
          "description": "min=0.162, mean=0.193, max=0.232, sum=0.772 (4)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.096, mean=0.125, max=0.139, sum=0.498 (4)",
            "tab": "Calibration",
            "score": 0.12460869505528777
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.162, mean=0.178, max=0.209, sum=0.711 (4)",
            "tab": "Robustness",
            "score": 0.17775229357798167
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.15, mean=0.162, max=0.176, sum=0.647 (4)",
            "tab": "Fairness",
            "score": 0.16169724770642202
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.122, mean=0.168, max=0.183, sum=0.671 (4)",
            "tab": "Efficiency",
            "score": 0.16779271445154526
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=2616 (4)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=0, mean=3.513, max=4.838, sum=14.05 (4)",
            "tab": "General information",
            "score": 3.5126146788990824
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (4)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=89.896, mean=372.668, max=473.333, sum=1490.671 (4)",
            "tab": "General information",
            "score": 372.66781345565744
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=4 (4)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=2.5, max=3, sum=10 (4)",
            "tab": "General information",
            "score": 2.5
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.03,
        "details": {
          "description": "min=0.03, mean=0.03, max=0.03, sum=0.182 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=1.108, mean=1.108, max=1.109, sum=6.651 (6)",
            "tab": "Efficiency",
            "score": 1.1084291968542619
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=0.06, mean=0.061, max=0.062, sum=0.365 (6)",
            "tab": "General information",
            "score": 0.060801144492131615
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0.933, mean=0.935, max=0.936, sum=5.609 (6)",
            "tab": "General information",
            "score": 0.9349070100143061
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=500.788, mean=500.829, max=500.912, sum=3004.974 (6)",
            "tab": "General information",
            "score": 500.8290414878398
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=128, mean=128, max=128, sum=768 (6)",
            "tab": "General information",
            "score": 128.0
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.402, mean=0.402, max=0.402, sum=2.411 (6)",
            "tab": "Bias",
            "score": 0.4018787714810442
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.361, mean=0.361, max=0.361, sum=2.163 (6)",
            "tab": "Bias",
            "score": 0.3605442176870748
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.188, mean=0.188, max=0.188, sum=1.129 (6)",
            "tab": "Bias",
            "score": 0.1882129277566539
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0.009, mean=0.009, max=0.009, sum=0.052 (6)",
            "tab": "Toxicity",
            "score": 0.008583690987124463
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=-0.27, mean=-0.27, max=-0.27, sum=-0.81 (3)",
            "tab": "Summarization metrics",
            "score": -0.2698551726198464
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=-0.122, mean=-0.121, max=-0.12, sum=-0.362 (3)",
            "tab": "Summarization metrics",
            "score": -0.12078049146748136
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.72, mean=0.72, max=0.72, sum=4.319 (6)",
            "tab": "Summarization metrics",
            "score": 0.7197585278365729
          },
          "CNN/DailyMail - Density": {
            "description": "min=5.044, mean=5.044, max=5.044, sum=30.265 (6)",
            "tab": "Summarization metrics",
            "score": 5.044183333839311
          },
          "CNN/DailyMail - Compression": {
            "description": "min=7.173, mean=7.186, max=7.2, sum=43.118 (6)",
            "tab": "Summarization metrics",
            "score": 7.186281356409094
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.058,
        "details": {
          "description": "min=0.049, mean=0.058, max=0.066, sum=0.345 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=0.771, mean=0.774, max=0.781, sum=4.646 (6)",
            "tab": "Efficiency",
            "score": 0.7743015579914415
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=0.234, mean=0.293, max=0.361, sum=1.761 (6)",
            "tab": "General information",
            "score": 0.29343629343629346
          },
          "XSUM - truncated": {
            "description": "min=0.614, mean=0.677, max=0.736, sum=4.062 (6)",
            "tab": "General information",
            "score": 0.676962676962677
          },
          "XSUM - # prompt tokens": {
            "description": "min=433.917, mean=437.97, max=442.292, sum=2627.819 (6)",
            "tab": "General information",
            "score": 437.96975546975546
          },
          "XSUM - # output tokens": {
            "description": "min=64, mean=64, max=64, sum=384 (6)",
            "tab": "General information",
            "score": 64.0
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.45, mean=0.455, max=0.463, sum=2.729 (6)",
            "tab": "Bias",
            "score": 0.45478395061728394
          },
          "XSUM - Representation (race)": {
            "description": "min=0.489, mean=0.524, max=0.556, sum=3.145 (6)",
            "tab": "Bias",
            "score": 0.5241150528821762
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.236, mean=0.251, max=0.262, sum=1.508 (6)",
            "tab": "Bias",
            "score": 0.251389993488347
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.004 (6)",
            "tab": "Toxicity",
            "score": 0.0006435006435006435
          },
          "XSUM - SummaC": {
            "description": "min=-0.28, mean=-0.275, max=-0.272, sum=-0.826 (3)",
            "tab": "Summarization metrics",
            "score": -0.2753430534988641
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.028, mean=0.072, max=0.121, sum=0.215 (3)",
            "tab": "Summarization metrics",
            "score": 0.07156637071699196
          },
          "XSUM - Coverage": {
            "description": "min=0.617, mean=0.643, max=0.671, sum=3.856 (6)",
            "tab": "Summarization metrics",
            "score": 0.6426528869383965
          },
          "XSUM - Density": {
            "description": "min=3.058, mean=3.208, max=3.428, sum=19.25 (6)",
            "tab": "Summarization metrics",
            "score": 3.2083925287601787
          },
          "XSUM - Compression": {
            "description": "min=7.31, mean=7.853, max=8.427, sum=47.12 (6)",
            "tab": "Summarization metrics",
            "score": 7.853257861418139
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.337,
        "details": {
          "description": "min=0.13, mean=0.337, max=0.556, sum=1.01 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.037, mean=0.225, max=0.41, sum=0.675 (3)",
            "tab": "Calibration",
            "score": 0.22500123786419848
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.091, mean=0.276, max=0.485, sum=0.827 (3)",
            "tab": "Robustness",
            "score": 0.27566666666666667
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.092, mean=0.271, max=0.484, sum=0.814 (3)",
            "tab": "Fairness",
            "score": 0.2713333333333333
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=0.214, mean=0.215, max=0.217, sum=0.645 (3)",
            "tab": "Efficiency",
            "score": 0.21490736543138858
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=0.309, mean=0.449, max=0.689, sum=1.347 (3)",
            "tab": "General information",
            "score": 0.449
          },
          "IMDB - truncated": {
            "description": "min=0.175, mean=0.176, max=0.176, sum=0.527 (3)",
            "tab": "General information",
            "score": 0.17566666666666664
          },
          "IMDB - # prompt tokens": {
            "description": "min=388.254, mean=407.098, max=435.686, sum=1221.293 (3)",
            "tab": "General information",
            "score": 407.0976666666666
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.521,
        "details": {
          "description": "min=0, mean=0.521, max=1, sum=28.146 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.123, mean=0.404, max=0.585, sum=21.802 (54)",
            "tab": "Calibration",
            "score": 0.40373338964571226
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.45, max=0.983, sum=24.293 (54)",
            "tab": "Robustness",
            "score": 0.4498711194026963
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.423, max=0.975, sum=22.816 (54)",
            "tab": "Fairness",
            "score": 0.4225225679997762
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.21, mean=0.264, max=0.45, sum=14.236 (54)",
            "tab": "Efficiency",
            "score": 0.2636334561494892
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=1.01, mean=2.608, max=4.878, sum=140.857 (54)",
            "tab": "General information",
            "score": 2.608459470057463
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0.003, max=0.032, sum=0.138 (54)",
            "tab": "General information",
            "score": 0.0025500084787325617
          },
          "CivilComments - # prompt tokens": {
            "description": "min=335.768, mean=416.896, max=479.235, sum=22512.361 (54)",
            "tab": "General information",
            "score": 416.89557696196465
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.404,
        "details": {
          "description": "min=0, mean=0.404, max=0.95, sum=13.325 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.0, mean=0.401, max=0.95, sum=13.228 (33)",
            "tab": "Calibration",
            "score": 0.40084433515818857
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.349, max=0.95, sum=11.525 (33)",
            "tab": "Robustness",
            "score": 0.3492424242424242
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0, mean=0.375, max=0.95, sum=12.375 (33)",
            "tab": "Fairness",
            "score": 0.375
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.316, mean=0.434, max=0.454, sum=14.32 (33)",
            "tab": "Efficiency",
            "score": 0.43394225670679076
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=2.433, max=5, sum=80.3 (33)",
            "tab": "General information",
            "score": 2.433333333333333
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0.394, max=1, sum=13 (33)",
            "tab": "General information",
            "score": 0.3939393939393939
          },
          "RAFT - # prompt tokens": {
            "description": "min=267.4, mean=423.537, max=511, sum=13976.725 (33)",
            "tab": "General information",
            "score": 423.53712121212124
          },
          "RAFT - # output tokens": {
            "description": "min=30, mean=30, max=30, sum=990 (33)",
            "tab": "General information",
            "score": 30.0
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "min=0.079, mean=0.079, max=0.079, sum=0.237 (3)",
            "tab": "Bias",
            "score": 0.07894736842105265
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}