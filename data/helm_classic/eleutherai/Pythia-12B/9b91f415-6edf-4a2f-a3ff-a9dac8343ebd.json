{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/eleutherai_Pythia-12B/1770834891.1472661",
  "retrieved_timestamp": "1770834891.1472661",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Pythia 12B",
    "id": "eleutherai/Pythia-12B",
    "developer": "eleutherai",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.257,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.37428307123034227
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.27195804195804196
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.22631701631701634
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.4331466568182155
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.38444055944055944
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.274,
        "details": {
          "description": "min=0.2, mean=0.274, max=0.3, sum=1.368 (5)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.092, mean=0.111, max=0.166, sum=0.557 (5)",
            "tab": "Calibration",
            "score": 0.11132961223278444
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.17, mean=0.22, max=0.28, sum=1.102 (5)",
            "tab": "Robustness",
            "score": 0.22035087719298244
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.16, mean=0.212, max=0.29, sum=1.061 (5)",
            "tab": "Fairness",
            "score": 0.2121052631578947
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=358.76, mean=467.936, max=612.798, sum=2339.678 (5)",
            "tab": "General information",
            "score": 467.935649122807
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on BoolQ",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.662,
        "details": {
          "description": "min=0.662, mean=0.662, max=0.662, sum=0.662 (1)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.14, mean=0.14, max=0.14, sum=0.14 (1)",
            "tab": "Calibration",
            "score": 0.13986557582802048
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.51, mean=0.51, max=0.51, sum=0.51 (1)",
            "tab": "Robustness",
            "score": 0.51
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.547, mean=0.547, max=0.547, sum=0.547 (1)",
            "tab": "Fairness",
            "score": 0.547
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=1251.897, mean=1251.897, max=1251.897, sum=1251.897 (1)",
            "tab": "General information",
            "score": 1251.897
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.596,
        "details": {
          "description": "min=0.596, mean=0.596, max=0.596, sum=0.596 (1)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.239, mean=0.239, max=0.239, sum=0.239 (1)",
            "tab": "Calibration",
            "score": 0.2394289121866973
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.42, mean=0.42, max=0.42, sum=0.42 (1)",
            "tab": "Robustness",
            "score": 0.42022169799567144
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.449, mean=0.449, max=0.449, sum=0.449 (1)",
            "tab": "Fairness",
            "score": 0.44869513696457247
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.969, mean=1.969, max=1.969, sum=1.969 (1)",
            "tab": "General information",
            "score": 1.9690140845070423
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1691.082, mean=1691.082, max=1691.082, sum=1691.082 (1)",
            "tab": "General information",
            "score": 1691.081690140845
          },
          "NarrativeQA - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=100 (1)",
            "tab": "General information",
            "score": 100.0
          },
          "NarrativeQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.215, mean=0.215, max=0.215, sum=0.215 (1)",
            "tab": "Bias",
            "score": 0.2152777777777778
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.023, mean=0.023, max=0.023, sum=0.023 (1)",
            "tab": "Toxicity",
            "score": 0.022535211267605635
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (open-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.581,
        "details": {
          "description": "min=0.581, mean=0.581, max=0.581, sum=0.581 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.094, mean=0.094, max=0.094, sum=0.094 (1)",
            "tab": "Calibration",
            "score": 0.09399996958029097
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.39, mean=0.39, max=0.39, sum=0.39 (1)",
            "tab": "Calibration",
            "score": 0.3899944090149843
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.108, mean=0.108, max=0.108, sum=0.108 (1)",
            "tab": "Robustness",
            "score": 0.10849928114746796
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.47, mean=0.47, max=0.47, sum=0.47 (1)",
            "tab": "Robustness",
            "score": 0.46990137932247006
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.131, mean=0.131, max=0.131, sum=0.131 (1)",
            "tab": "Fairness",
            "score": 0.13109020655004933
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.523, mean=0.523, max=0.523, sum=0.523 (1)",
            "tab": "Fairness",
            "score": 0.5229768252994325
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=117.299, mean=117.299, max=117.299, sum=117.299 (1)",
            "tab": "General information",
            "score": 117.299
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=300 (1)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.704, mean=4.704, max=4.704, sum=4.704 (1)",
            "tab": "General information",
            "score": 4.704
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.037, mean=0.037, max=0.037, sum=0.037 (1)",
            "tab": "General information",
            "score": 0.037
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1495.552, mean=1495.552, max=1495.552, sum=1495.552 (1)",
            "tab": "General information",
            "score": 1495.552
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=300 (1)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.407, mean=0.407, max=0.407, sum=0.407 (1)",
            "tab": "Bias",
            "score": 0.40682414698162733
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.122, mean=0.122, max=0.122, sum=0.122 (1)",
            "tab": "Bias",
            "score": 0.1216216216216216
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.405, mean=0.405, max=0.405, sum=0.405 (1)",
            "tab": "Bias",
            "score": 0.4047619047619048
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.467, mean=0.467, max=0.467, sum=0.467 (1)",
            "tab": "Bias",
            "score": 0.4666666666666667
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.276, mean=0.276, max=0.276, sum=0.276 (1)",
            "tab": "Bias",
            "score": 0.27551020408163257
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.002, mean=0.002, max=0.002, sum=0.002 (1)",
            "tab": "Toxicity",
            "score": 0.002
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on QuAC",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.313,
        "details": {
          "description": "min=0.313, mean=0.313, max=0.313, sum=0.313 (1)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.138, mean=0.138, max=0.138, sum=0.138 (1)",
            "tab": "Calibration",
            "score": 0.1383150544527575
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.171, mean=0.171, max=0.171, sum=0.171 (1)",
            "tab": "Robustness",
            "score": 0.17120890749036072
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.227, mean=0.227, max=0.227, sum=0.227 (1)",
            "tab": "Fairness",
            "score": 0.22738715021444486
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.883, mean=0.883, max=0.883, sum=0.883 (1)",
            "tab": "General information",
            "score": 0.883
          },
          "QuAC - truncated": {
            "description": "min=0.021, mean=0.021, max=0.021, sum=0.021 (1)",
            "tab": "General information",
            "score": 0.021
          },
          "QuAC - # prompt tokens": {
            "description": "min=1655.708, mean=1655.708, max=1655.708, sum=1655.708 (1)",
            "tab": "General information",
            "score": 1655.708
          },
          "QuAC - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=100 (1)",
            "tab": "General information",
            "score": 100.0
          },
          "QuAC - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.641, mean=0.641, max=0.641, sum=0.641 (1)",
            "tab": "Bias",
            "score": 0.6406926406926409
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.415, mean=0.415, max=0.415, sum=0.415 (1)",
            "tab": "Bias",
            "score": 0.4150793650793651
          },
          "QuAC - Representation (race)": {
            "description": "min=0.314, mean=0.314, max=0.314, sum=0.314 (1)",
            "tab": "Bias",
            "score": 0.3137254901960784
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.26, mean=0.26, max=0.26, sum=0.26 (1)",
            "tab": "Bias",
            "score": 0.25965665236051505
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.002, mean=0.002, max=0.002, sum=0.002 (1)",
            "tab": "Toxicity",
            "score": 0.002
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on HellaSwag",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "HellaSwag - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on TruthfulQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.177,
        "details": {
          "description": "min=0.177, mean=0.177, max=0.177, sum=0.177 (1)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.094, mean=0.094, max=0.094, sum=0.094 (1)",
            "tab": "Calibration",
            "score": 0.09363268995646454
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.138, mean=0.138, max=0.138, sum=0.138 (1)",
            "tab": "Robustness",
            "score": 0.13761467889908258
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.154, mean=0.154, max=0.154, sum=0.154 (1)",
            "tab": "Fairness",
            "score": 0.154434250764526
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=654 (1)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.352, mean=505.352, max=505.352, sum=505.352 (1)",
            "tab": "General information",
            "score": 505.35168195718654
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC)",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "NDCG@10 on MS MARCO (TREC)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on CNN/DailyMail",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "CNN/DailyMail - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on XSUM",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "XSUM - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on IMDB",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.931,
        "details": {
          "description": "min=0.931, mean=0.931, max=0.931, sum=0.931 (1)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.342, mean=0.342, max=0.342, sum=0.342 (1)",
            "tab": "Calibration",
            "score": 0.34150363639115
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.854, mean=0.854, max=0.854, sum=0.854 (1)",
            "tab": "Robustness",
            "score": 0.854
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.916, mean=0.916, max=0.916, sum=0.916 (1)",
            "tab": "Fairness",
            "score": 0.916
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.911, mean=2.911, max=2.911, sum=2.911 (1)",
            "tab": "General information",
            "score": 2.911
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1619.568, mean=1619.568, max=1619.568, sum=1619.568 (1)",
            "tab": "General information",
            "score": 1619.568
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on CivilComments",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.531,
        "details": {
          "description": "min=0.03, mean=0.531, max=0.988, sum=9.561 (18)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.138, mean=0.297, max=0.479, sum=5.337 (18)",
            "tab": "Calibration",
            "score": 0.2965193799633309
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.02, mean=0.418, max=0.973, sum=7.526 (18)",
            "tab": "Robustness",
            "score": 0.41812542395705293
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.01, mean=0.448, max=0.985, sum=8.071 (18)",
            "tab": "Fairness",
            "score": 0.44837567354282437
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=6688 (18)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=90 (18)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (18)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=360.976, mean=771.654, max=1282.4, sum=13889.772 (18)",
            "tab": "General information",
            "score": 771.6539847352628
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=90 (18)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=1, mean=1, max=1, sum=18 (18)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on RAFT",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.514,
        "details": {
          "description": "min=0.175, mean=0.514, max=0.975, sum=5.65 (11)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.175, mean=0.514, max=0.975, sum=5.649 (11)",
            "tab": "Calibration",
            "score": 0.5135614568346981
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.45, max=0.975, sum=4.95 (11)",
            "tab": "Robustness",
            "score": 0.45
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.15, mean=0.489, max=0.975, sum=5.375 (11)",
            "tab": "Fairness",
            "score": 0.48863636363636365
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=440 (11)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0.7, mean=4.605, max=5, sum=50.65 (11)",
            "tab": "General information",
            "score": 4.6045454545454545
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (11)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=280.35, mean=869.691, max=1756.575, sum=9566.6 (11)",
            "tab": "General information",
            "score": 869.6909090909089
          },
          "RAFT - # output tokens": {
            "description": "min=30, mean=30, max=30, sum=330 (11)",
            "tab": "General information",
            "score": 30.0
          },
          "RAFT - # trials": {
            "description": "min=1, mean=1, max=1, sum=11 (11)",
            "tab": "General information",
            "score": 1.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}