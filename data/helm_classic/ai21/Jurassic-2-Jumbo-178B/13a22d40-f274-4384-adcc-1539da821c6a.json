{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/ai21_Jurassic-2-Jumbo-178B/1770834891.1472661",
  "retrieved_timestamp": "1770834891.1472661",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Jurassic-2 Jumbo 178B",
    "id": "ai21/Jurassic-2-Jumbo-178B",
    "developer": "ai21",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.824,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.6597594819611471
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.7910296229539834
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.8360206534288848
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5968189835436076
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.5064102564102564
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.6447368421052632
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.48,
        "details": {
          "description": "min=0.23, mean=0.48, max=0.83, sum=7.207 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.056, mean=0.137, max=0.248, sum=2.059 (15)",
            "tab": "Calibration",
            "score": 0.13723997934779486
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.17, mean=0.417, max=0.75, sum=6.251 (15)",
            "tab": "Robustness",
            "score": 0.41671345029239765
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.21, mean=0.45, max=0.78, sum=6.75 (15)",
            "tab": "Fairness",
            "score": 0.44997660818713453
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=308.59, mean=396.74, max=552.719, sum=5951.098 (15)",
            "tab": "General information",
            "score": 396.73985964912276
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on BoolQ",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.829,
        "details": {
          "description": "min=0.818, mean=0.829, max=0.838, sum=2.487 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.163, mean=0.175, max=0.198, sum=0.526 (3)",
            "tab": "Calibration",
            "score": 0.17545319159294462
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.72, mean=0.729, max=0.736, sum=2.188 (3)",
            "tab": "Robustness",
            "score": 0.7293333333333333
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.78, mean=0.792, max=0.798, sum=2.375 (3)",
            "tab": "Fairness",
            "score": 0.7916666666666666
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=506.985, mean=694.652, max=952.985, sum=2083.955 (3)",
            "tab": "General information",
            "score": 694.6516666666666
          },
          "BoolQ - # output tokens": {
            "description": "min=2, mean=2.002, max=2.003, sum=6.005 (3)",
            "tab": "General information",
            "score": 2.0016666666666665
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.733,
        "details": {
          "description": "min=0.715, mean=0.733, max=0.757, sum=2.2 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.068, mean=0.073, max=0.076, sum=0.219 (3)",
            "tab": "Calibration",
            "score": 0.07310994320832209
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.627, mean=0.66, max=0.69, sum=1.98 (3)",
            "tab": "Robustness",
            "score": 0.6601600341725052
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.63, mean=0.658, max=0.69, sum=1.973 (3)",
            "tab": "Fairness",
            "score": 0.6577011654908803
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=2534.434, mean=2818.1, max=3027.434, sum=8454.301 (3)",
            "tab": "General information",
            "score": 2818.1004694835683
          },
          "NarrativeQA - # output tokens": {
            "description": "min=4.879, mean=6.406, max=7.755, sum=19.217 (3)",
            "tab": "General information",
            "score": 6.405633802816901
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.385, mean=0.43, max=0.5, sum=1.29 (3)",
            "tab": "Bias",
            "score": 0.4298611111111111
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.333, mean=0.5, max=0.667, sum=1 (2)",
            "tab": "Bias",
            "score": 0.5
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.171, mean=0.183, max=0.192, sum=0.55 (3)",
            "tab": "Bias",
            "score": 0.18345814920903128
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.014, mean=0.017, max=0.02, sum=0.051 (3)",
            "tab": "Toxicity",
            "score": 0.016901408450704227
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (open-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.669,
        "details": {
          "description": "min=0.65, mean=0.669, max=0.681, sum=2.007 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.018, mean=0.018, max=0.019, sum=0.054 (3)",
            "tab": "Calibration",
            "score": 0.018133452831606698
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.071, mean=0.073, max=0.076, sum=0.22 (3)",
            "tab": "Calibration",
            "score": 0.07345259187429393
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.31, mean=0.315, max=0.318, sum=0.945 (3)",
            "tab": "Robustness",
            "score": 0.3150688575152197
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.576, mean=0.599, max=0.616, sum=1.796 (3)",
            "tab": "Robustness",
            "score": 0.5985032886794094
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.326, mean=0.327, max=0.328, sum=0.982 (3)",
            "tab": "Fairness",
            "score": 0.32739768950953246
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.601, mean=0.62, max=0.633, sum=1.86 (3)",
            "tab": "Fairness",
            "score": 0.6201543217700605
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=94.377, mean=99.377, max=102.377, sum=298.131 (3)",
            "tab": "General information",
            "score": 99.377
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=5.127, mean=5.365, max=5.79, sum=16.095 (3)",
            "tab": "General information",
            "score": 5.364999999999999
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.928, mean=4.93, max=4.932, sum=14.791 (3)",
            "tab": "General information",
            "score": 4.9303333333333335
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.012, mean=0.012, max=0.012, sum=0.036 (3)",
            "tab": "General information",
            "score": 0.012000000000000002
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1254.565, mean=1571.171, max=1771.274, sum=4713.512 (3)",
            "tab": "General information",
            "score": 1571.1706666666669
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=4.785, mean=5.113, max=5.399, sum=15.338 (3)",
            "tab": "General information",
            "score": 5.112666666666667
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.352, mean=0.376, max=0.405, sum=1.127 (3)",
            "tab": "Bias",
            "score": 0.3756261756261756
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.033, mean=0.095, max=0.136, sum=0.285 (3)",
            "tab": "Bias",
            "score": 0.09502719502719503
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.3, mean=0.413, max=0.5, sum=1.238 (3)",
            "tab": "Bias",
            "score": 0.41250000000000003
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.514, mean=0.541, max=0.561, sum=1.624 (3)",
            "tab": "Bias",
            "score": 0.5414311179017061
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.06, mean=0.107, max=0.132, sum=0.321 (3)",
            "tab": "Bias",
            "score": 0.10706952566601687
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.001, sum=0.002 (3)",
            "tab": "Toxicity",
            "score": 0.0006666666666666666
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on QuAC",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.435,
        "details": {
          "description": "min=0.426, mean=0.435, max=0.446, sum=1.305 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.032, mean=0.035, max=0.037, sum=0.104 (3)",
            "tab": "Calibration",
            "score": 0.03466023181877799
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.31, mean=0.314, max=0.316, sum=0.941 (3)",
            "tab": "Robustness",
            "score": 0.3135172870245195
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.333, mean=0.34, max=0.348, sum=1.02 (3)",
            "tab": "Fairness",
            "score": 0.34006270092560414
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=4.999, mean=5.0, max=5, sum=14.999 (3)",
            "tab": "General information",
            "score": 4.999666666666666
          },
          "QuAC - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "QuAC - # prompt tokens": {
            "description": "min=3587.32, mean=4018.779, max=4568.698, sum=12056.338 (3)",
            "tab": "General information",
            "score": 4018.7793333333334
          },
          "QuAC - # output tokens": {
            "description": "min=21.621, mean=22.178, max=22.826, sum=66.533 (3)",
            "tab": "General information",
            "score": 22.177666666666664
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.615, mean=0.642, max=0.667, sum=1.925 (3)",
            "tab": "Bias",
            "score": 0.6416361416361417
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.425, mean=0.454, max=0.476, sum=1.363 (3)",
            "tab": "Bias",
            "score": 0.45448951168627727
          },
          "QuAC - Representation (race)": {
            "description": "min=0.342, mean=0.359, max=0.375, sum=1.078 (3)",
            "tab": "Bias",
            "score": 0.35949126363389555
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.22, mean=0.232, max=0.241, sum=0.696 (3)",
            "tab": "Bias",
            "score": 0.23190752816365634
          },
          "QuAC - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on HellaSwag",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.788,
        "details": {
          "description": "min=0.788, mean=0.788, max=0.788, sum=0.788 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.754, mean=0.754, max=0.754, sum=0.754 (1)",
            "tab": "Robustness",
            "score": 0.754
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.655, mean=0.655, max=0.655, sum=0.655 (1)",
            "tab": "Fairness",
            "score": 0.655
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=62.466, mean=62.466, max=62.466, sum=62.466 (1)",
            "tab": "General information",
            "score": 62.466
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.558,
        "details": {
          "description": "min=0.558, mean=0.558, max=0.558, sum=0.558 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.47, mean=0.47, max=0.47, sum=0.47 (1)",
            "tab": "Robustness",
            "score": 0.47
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.488, mean=0.488, max=0.488, sum=0.488 (1)",
            "tab": "Fairness",
            "score": 0.488
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=4.348, mean=4.348, max=4.348, sum=4.348 (1)",
            "tab": "General information",
            "score": 4.348
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on TruthfulQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.437,
        "details": {
          "description": "min=0.367, mean=0.437, max=0.485, sum=1.312 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.049, mean=0.068, max=0.095, sum=0.203 (3)",
            "tab": "Calibration",
            "score": 0.06751578986419772
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.326, mean=0.39, max=0.43, sum=1.17 (3)",
            "tab": "Robustness",
            "score": 0.38990825688073394
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.289, mean=0.354, max=0.398, sum=1.063 (3)",
            "tab": "Fairness",
            "score": 0.35423037716615696
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=317.682, mean=355.015, max=375.682, sum=1065.046 (3)",
            "tab": "General information",
            "score": 355.0152905198777
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC)",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "NDCG@10 on MS MARCO (TREC)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.661,
        "details": {
          "description": "min=0.62, mean=0.661, max=0.706, sum=1.982 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.333, mean=0.337, max=0.343, sum=1.012 (3)",
            "tab": "Robustness",
            "score": 0.3372691798941794
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.569, mean=0.607, max=0.639, sum=1.821 (3)",
            "tab": "Robustness",
            "score": 0.6069545244562901
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.339, mean=0.342, max=0.346, sum=1.027 (3)",
            "tab": "Fairness",
            "score": 0.34235396825396786
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.578, mean=0.62, max=0.66, sum=1.861 (3)",
            "tab": "Fairness",
            "score": 0.6202649047028815
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=349.303, mean=385.636, max=423.303, sum=1156.909 (3)",
            "tab": "General information",
            "score": 385.63633333333337
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=2, mean=2.001, max=2.003, sum=6.003 (3)",
            "tab": "General information",
            "score": 2.001
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=337.047, mean=373.38, max=411.047, sum=1120.14 (3)",
            "tab": "General information",
            "score": 373.3798449612403
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on CNN/DailyMail",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.149,
        "details": {
          "description": "min=0.142, mean=0.149, max=0.157, sum=0.892 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1203.032, mean=1213.032, max=1224.032, sum=7278.193 (6)",
            "tab": "General information",
            "score": 1213.0321888412018
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=47.208, mean=49.239, max=51.633, sum=295.433 (6)",
            "tab": "General information",
            "score": 49.238912732474965
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.593, mean=0.608, max=0.618, sum=3.649 (6)",
            "tab": "Bias",
            "score": 0.6082305358040653
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.396, mean=0.411, max=0.434, sum=2.467 (6)",
            "tab": "Bias",
            "score": 0.4111171483483329
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.177, mean=0.254, max=0.301, sum=1.526 (6)",
            "tab": "Bias",
            "score": 0.25438070908615346
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.064, mean=0.083, max=0.119, sum=0.497 (6)",
            "tab": "Bias",
            "score": 0.08290586755395449
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.009 (6)",
            "tab": "Toxicity",
            "score": 0.001430615164520744
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.442, mean=0.489, max=0.543, sum=1.468 (3)",
            "tab": "Summarization metrics",
            "score": 0.48944984939262354
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.299, mean=0.313, max=0.33, sum=0.94 (3)",
            "tab": "Summarization metrics",
            "score": 0.31320318480412634
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.952, mean=0.957, max=0.964, sum=5.745 (6)",
            "tab": "Summarization metrics",
            "score": 0.9574608785885589
          },
          "CNN/DailyMail - Density": {
            "description": "min=12.535, mean=15.317, max=20.424, sum=91.904 (6)",
            "tab": "Summarization metrics",
            "score": 15.31737957113954
          },
          "CNN/DailyMail - Compression": {
            "description": "min=11.81, mean=12.304, max=13.072, sum=73.827 (6)",
            "tab": "Summarization metrics",
            "score": 12.30449736723726
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on XSUM",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.182,
        "details": {
          "description": "min=0.177, mean=0.182, max=0.186, sum=1.09 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1099.388, mean=1133.388, max=1172.388, sum=6800.328 (6)",
            "tab": "General information",
            "score": 1133.388030888031
          },
          "XSUM - # output tokens": {
            "description": "min=21.909, mean=22.142, max=22.392, sum=132.853 (6)",
            "tab": "General information",
            "score": 22.142213642213644
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.451, mean=0.466, max=0.478, sum=2.796 (6)",
            "tab": "Bias",
            "score": 0.4660306771417882
          },
          "XSUM - Representation (race)": {
            "description": "min=0.362, mean=0.399, max=0.429, sum=2.397 (6)",
            "tab": "Bias",
            "score": 0.39943255885284873
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.189, mean=0.205, max=0.224, sum=1.232 (6)",
            "tab": "Bias",
            "score": 0.20538608377971754
          },
          "XSUM - Toxic fraction": {
            "description": "min=0.002, mean=0.003, max=0.004, sum=0.019 (6)",
            "tab": "Toxicity",
            "score": 0.0032175032175032173
          },
          "XSUM - SummaC": {
            "description": "min=-0.325, mean=-0.32, max=-0.314, sum=-0.96 (3)",
            "tab": "Summarization metrics",
            "score": -0.31997175372142944
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.484, mean=0.489, max=0.493, sum=1.468 (3)",
            "tab": "Summarization metrics",
            "score": 0.4894925021585029
          },
          "XSUM - Coverage": {
            "description": "min=0.75, mean=0.755, max=0.761, sum=4.53 (6)",
            "tab": "Summarization metrics",
            "score": 0.7549647155240389
          },
          "XSUM - Density": {
            "description": "min=1.852, mean=2.145, max=2.331, sum=12.869 (6)",
            "tab": "Summarization metrics",
            "score": 2.144865535443147
          },
          "XSUM - Compression": {
            "description": "min=16.369, mean=16.589, max=16.81, sum=99.535 (6)",
            "tab": "Summarization metrics",
            "score": 16.58922760069323
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on IMDB",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.938,
        "details": {
          "description": "min=0.936, mean=0.938, max=0.943, sum=2.815 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.157, mean=0.182, max=0.199, sum=0.546 (3)",
            "tab": "Calibration",
            "score": 0.18203122522171636
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.878, mean=0.896, max=0.916, sum=2.688 (3)",
            "tab": "Robustness",
            "score": 0.896
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.928, mean=0.933, max=0.937, sum=2.799 (3)",
            "tab": "Fairness",
            "score": 0.9329999999999999
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=853.851, mean=1288.518, max=1745.851, sum=3865.553 (3)",
            "tab": "General information",
            "score": 1288.5176666666669
          },
          "IMDB - # output tokens": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on CivilComments",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.57,
        "details": {
          "description": "min=0.011, mean=0.57, max=1, sum=30.805 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.07, mean=0.314, max=0.578, sum=16.962 (54)",
            "tab": "Calibration",
            "score": 0.31411210820302815
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.009, mean=0.449, max=0.979, sum=24.224 (54)",
            "tab": "Robustness",
            "score": 0.4485846578472439
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.005, mean=0.507, max=0.995, sum=27.37 (54)",
            "tab": "Fairness",
            "score": 0.5068507198702314
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=271.927, mean=532.602, max=942.498, sum=28760.487 (54)",
            "tab": "General information",
            "score": 532.6016121330534
          },
          "CivilComments - # output tokens": {
            "description": "min=2, mean=2, max=2, sum=108 (54)",
            "tab": "General information",
            "score": 2.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on RAFT",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.746,
        "details": {
          "description": "min=0.225, mean=0.746, max=0.975, sum=24.625 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.126, mean=0.218, max=0.683, sum=7.184 (33)",
            "tab": "Calibration",
            "score": 0.2177038585857703
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0.225, mean=0.69, max=0.95, sum=22.775 (33)",
            "tab": "Robustness",
            "score": 0.6901515151515151
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.225, mean=0.711, max=0.975, sum=23.45 (33)",
            "tab": "Fairness",
            "score": 0.7106060606060605
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=5, mean=5, max=5, sum=165 (33)",
            "tab": "General information",
            "score": 5.0
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=212.25, mean=944.157, max=4506.05, sum=31157.175 (33)",
            "tab": "General information",
            "score": 944.1568181818182
          },
          "RAFT - # output tokens": {
            "description": "min=2, mean=3.597, max=7.275, sum=118.7 (33)",
            "tab": "General information",
            "score": 3.5969696969696967
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}