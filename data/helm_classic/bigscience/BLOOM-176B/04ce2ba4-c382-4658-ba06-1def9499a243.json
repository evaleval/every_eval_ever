{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/bigscience_BLOOM-176B/1770830385.7573261",
  "retrieved_timestamp": "1770830385.7573261",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "BLOOM 176B",
    "id": "bigscience/BLOOM-176B",
    "developer": "bigscience",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.446,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.3480016788296159
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.5409357605686861
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.5507003378527294
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.26823464912280703
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5459762982621468
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.5959534292867626
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.29074770258980787
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.299,
        "details": {
          "description": "min=0.19, mean=0.299, max=0.42, sum=4.481 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.115, mean=0.137, max=0.173, sum=2.054 (15)",
            "tab": "Calibration",
            "score": 0.13690038983912287
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.167, mean=0.25, max=0.38, sum=3.754 (15)",
            "tab": "Robustness",
            "score": 0.25025730994152046
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.175, mean=0.274, max=0.38, sum=4.104 (15)",
            "tab": "Fairness",
            "score": 0.27360233918128657
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.135, mean=0.233, max=0.418, sum=3.493 (15)",
            "tab": "Efficiency",
            "score": 0.23288457024982262
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=333.02, mean=436.99, max=574.658, sum=6554.844 (15)",
            "tab": "General information",
            "score": 436.9895789473684
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ - EM",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.704,
        "details": {
          "description": "min=0.659, mean=0.704, max=0.728, sum=2.112 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.153, mean=0.209, max=0.247, sum=0.626 (3)",
            "tab": "Calibration",
            "score": 0.2086643852555177
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.595, mean=0.642, max=0.674, sum=1.926 (3)",
            "tab": "Robustness",
            "score": 0.642
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.601, mean=0.656, max=0.693, sum=1.968 (3)",
            "tab": "Fairness",
            "score": 0.656
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.665, mean=0.853, max=1.05, sum=2.558 (3)",
            "tab": "Efficiency",
            "score": 0.852823399183769
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=636.774, mean=897.107, max=1242.774, sum=2691.322 (3)",
            "tab": "General information",
            "score": 897.1073333333333
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.662,
        "details": {
          "description": "min=0.631, mean=0.662, max=0.695, sum=1.986 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.231, mean=0.237, max=0.242, sum=0.712 (3)",
            "tab": "Calibration",
            "score": 0.2374266630696186
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.468, mean=0.53, max=0.574, sum=1.591 (3)",
            "tab": "Robustness",
            "score": 0.5303029858435905
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.535, mean=0.577, max=0.613, sum=1.73 (3)",
            "tab": "Fairness",
            "score": 0.5767895596204061
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=2.081, mean=2.598, max=3.427, sum=7.794 (3)",
            "tab": "Efficiency",
            "score": 2.5979962524114084
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.042, mean=1.621, max=2.048, sum=4.862 (3)",
            "tab": "General information",
            "score": 1.6206572769953052
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1604.899, mean=1649.598, max=1699.146, sum=4948.794 (3)",
            "tab": "General information",
            "score": 1649.5981220657277
          },
          "NarrativeQA - # output tokens": {
            "description": "min=18.468, mean=33.276, max=50.499, sum=99.828 (3)",
            "tab": "General information",
            "score": 33.27605633802816
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.333, mean=0.355, max=0.389, sum=1.065 (3)",
            "tab": "Bias",
            "score": 0.354945620223398
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.118, mean=0.165, max=0.241, sum=0.494 (3)",
            "tab": "Bias",
            "score": 0.16472050143449737
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.011, mean=0.012, max=0.014, sum=0.037 (3)",
            "tab": "Toxicity",
            "score": 0.012206572769953052
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.621,
        "details": {
          "description": "min=0.61, mean=0.621, max=0.628, sum=1.864 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.11, mean=0.116, max=0.118, sum=0.347 (3)",
            "tab": "Calibration",
            "score": 0.11564225453050514
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.338, mean=0.347, max=0.36, sum=1.041 (3)",
            "tab": "Calibration",
            "score": 0.3469801265406112
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.18, mean=0.185, max=0.19, sum=0.556 (3)",
            "tab": "Robustness",
            "score": 0.18537100322417385
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.547, mean=0.558, max=0.569, sum=1.675 (3)",
            "tab": "Robustness",
            "score": 0.5582069622847597
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.183, mean=0.187, max=0.189, sum=0.56 (3)",
            "tab": "Fairness",
            "score": 0.18669047090402127
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.56, mean=0.575, max=0.585, sum=1.724 (3)",
            "tab": "Fairness",
            "score": 0.5745618824682682
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=0.931, mean=1.115, max=1.261, sum=3.346 (3)",
            "tab": "Efficiency",
            "score": 1.115412127906084
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=2.213, mean=2.547, max=2.912, sum=7.64 (3)",
            "tab": "Efficiency",
            "score": 2.546660231937965
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=92.12, mean=96.12, max=102.12, sum=288.36 (3)",
            "tab": "General information",
            "score": 96.12
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=34.82, mean=48.109, max=57.074, sum=144.327 (3)",
            "tab": "General information",
            "score": 48.109
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.73, mean=4.743, max=4.751, sum=14.229 (3)",
            "tab": "General information",
            "score": 4.743000000000001
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.035, mean=0.035, max=0.035, sum=0.105 (3)",
            "tab": "General information",
            "score": 0.035
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1193.69, mean=1313.422, max=1423.457, sum=3940.267 (3)",
            "tab": "General information",
            "score": 1313.4223333333334
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=31.304, mean=38.803, max=46.481, sum=116.409 (3)",
            "tab": "General information",
            "score": 38.803000000000004
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1 (2)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.303, mean=0.418, max=0.519, sum=1.254 (3)",
            "tab": "Bias",
            "score": 0.4180133480204756
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.056, mean=0.09, max=0.143, sum=0.27 (3)",
            "tab": "Bias",
            "score": 0.08994708994708996
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.352, mean=0.426, max=0.5, sum=0.852 (2)",
            "tab": "Bias",
            "score": 0.42619047619047623
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.454, mean=0.499, max=0.546, sum=1.498 (3)",
            "tab": "Bias",
            "score": 0.499333679443982
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.11, mean=0.135, max=0.177, sum=0.404 (3)",
            "tab": "Bias",
            "score": 0.13470779383719764
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.002, sum=0.004 (3)",
            "tab": "Toxicity",
            "score": 0.0013333333333333333
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.002, max=0.003, sum=0.006 (3)",
            "tab": "Toxicity",
            "score": 0.002
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC - F1",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.361,
        "details": {
          "description": "min=0.342, mean=0.361, max=0.375, sum=1.082 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.103, mean=0.122, max=0.142, sum=0.367 (3)",
            "tab": "Calibration",
            "score": 0.1222163558834574
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.229, mean=0.234, max=0.24, sum=0.701 (3)",
            "tab": "Robustness",
            "score": 0.23376457225319638
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.265, mean=0.273, max=0.289, sum=0.82 (3)",
            "tab": "Fairness",
            "score": 0.27335853114408787
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=5.124, mean=5.306, max=5.436, sum=15.919 (3)",
            "tab": "Efficiency",
            "score": 5.3062709801205585
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.855, mean=0.944, max=1.07, sum=2.832 (3)",
            "tab": "General information",
            "score": 0.944
          },
          "QuAC - truncated": {
            "description": "min=0.017, mean=0.017, max=0.017, sum=0.051 (3)",
            "tab": "General information",
            "score": 0.017
          },
          "QuAC - # prompt tokens": {
            "description": "min=1614.308, mean=1639.494, max=1673.303, sum=4918.482 (3)",
            "tab": "General information",
            "score": 1639.494
          },
          "QuAC - # output tokens": {
            "description": "min=86.351, mean=90.164, max=93.357, sum=270.491 (3)",
            "tab": "General information",
            "score": 90.16366666666666
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.604, mean=0.631, max=0.647, sum=1.894 (3)",
            "tab": "Bias",
            "score": 0.6313294548588666
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.388, mean=0.396, max=0.408, sum=1.189 (3)",
            "tab": "Bias",
            "score": 0.3963840842187811
          },
          "QuAC - Representation (race)": {
            "description": "min=0.35, mean=0.365, max=0.381, sum=1.094 (3)",
            "tab": "Bias",
            "score": 0.3645250034421991
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.235, mean=0.244, max=0.26, sum=0.732 (3)",
            "tab": "Bias",
            "score": 0.2440549375970967
          },
          "QuAC - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.744,
        "details": {
          "description": "min=0.744, mean=0.744, max=0.744, sum=0.744 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.293, mean=0.293, max=0.293, sum=0.293 (1)",
            "tab": "Calibration",
            "score": 0.2926428762465171
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.699, mean=0.699, max=0.699, sum=0.699 (1)",
            "tab": "Robustness",
            "score": 0.699
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.585, mean=0.585, max=0.585, sum=0.585 (1)",
            "tab": "Fairness",
            "score": 0.585
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "min=0.075, mean=0.075, max=0.075, sum=0.075 (1)",
            "tab": "Efficiency",
            "score": 0.07493321968615055
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.875, mean=88.875, max=88.875, sum=88.875 (1)",
            "tab": "General information",
            "score": 88.875
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.534,
        "details": {
          "description": "min=0.534, mean=0.534, max=0.534, sum=0.534 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.248, mean=0.248, max=0.248, sum=0.248 (1)",
            "tab": "Calibration",
            "score": 0.24842661648577113
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
            "tab": "Robustness",
            "score": 0.438
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.482, mean=0.482, max=0.482, sum=0.482 (1)",
            "tab": "Fairness",
            "score": 0.482
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "min=0.032, mean=0.032, max=0.032, sum=0.032 (1)",
            "tab": "Efficiency",
            "score": 0.03224579076468945
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.444, mean=5.444, max=5.444, sum=5.444 (1)",
            "tab": "General information",
            "score": 5.444
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.205,
        "details": {
          "description": "min=0.197, mean=0.205, max=0.211, sum=0.82 (4)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.053, mean=0.096, max=0.128, sum=0.385 (4)",
            "tab": "Calibration",
            "score": 0.09624512475777981
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.168, mean=0.183, max=0.206, sum=0.734 (4)",
            "tab": "Robustness",
            "score": 0.1834862385321101
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.164, mean=0.186, max=0.206, sum=0.745 (4)",
            "tab": "Fairness",
            "score": 0.18616207951070335
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.084, mean=0.143, max=0.226, sum=0.573 (4)",
            "tab": "Efficiency",
            "score": 0.14325443854568073
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=2616 (4)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=0, mean=3.75, max=5, sum=15 (4)",
            "tab": "General information",
            "score": 3.75
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (4)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=79.361, mean=370.611, max=481.361, sum=1482.443 (4)",
            "tab": "General information",
            "score": 370.6108562691131
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=4 (4)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=2.5, max=3, sum=10 (4)",
            "tab": "General information",
            "score": 2.5
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.386,
        "details": {
          "description": "min=0.364, mean=0.386, max=0.429, sum=1.158 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.158, mean=0.19, max=0.218, sum=0.57 (3)",
            "tab": "Robustness",
            "score": 0.18996269841269822
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.304, mean=0.333, max=0.385, sum=0.998 (3)",
            "tab": "Robustness",
            "score": 0.33254039819149694
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.189, mean=0.211, max=0.231, sum=0.633 (3)",
            "tab": "Fairness",
            "score": 0.2110978835978834
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.345, mean=0.371, max=0.418, sum=1.114 (3)",
            "tab": "Fairness",
            "score": 0.37148573288404924
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "min=0.246, mean=0.257, max=0.27, sum=0.77 (3)",
            "tab": "Efficiency",
            "score": 0.25680491607178446
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "min=0.227, mean=0.246, max=0.271, sum=0.739 (3)",
            "tab": "Efficiency",
            "score": 0.24635170979166832
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=484.472, mean=524.472, max=570.472, sum=1573.416 (3)",
            "tab": "General information",
            "score": 524.472
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=466.814, mean=506.814, max=552.814, sum=1520.442 (3)",
            "tab": "General information",
            "score": 506.81395348837214
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.08,
        "details": {
          "description": "min=0.052, mean=0.08, max=0.118, sum=0.478 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=5.515, mean=5.584, max=5.648, sum=33.506 (6)",
            "tab": "Efficiency",
            "score": 5.5842744588340345
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1520.33, mean=1541.33, max=1578.33, sum=9247.983 (6)",
            "tab": "General information",
            "score": 1541.3304721030042
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=104.867, mean=117.435, max=124.011, sum=704.609 (6)",
            "tab": "General information",
            "score": 117.4349070100143
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.641, mean=0.658, max=0.667, sum=3.949 (6)",
            "tab": "Bias",
            "score": 0.6581699346405229
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.372, mean=0.385, max=0.405, sum=2.311 (6)",
            "tab": "Bias",
            "score": 0.3851952735514946
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.291, mean=0.314, max=0.352, sum=1.882 (6)",
            "tab": "Bias",
            "score": 0.31373280163525924
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.119, mean=0.145, max=0.16, sum=0.872 (6)",
            "tab": "Bias",
            "score": 0.14536660393941517
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.009 (6)",
            "tab": "Toxicity",
            "score": 0.001430615164520744
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=-0.129, mean=-0.02, max=0.115, sum=-0.059 (3)",
            "tab": "Summarization metrics",
            "score": -0.01977462275373982
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "min=4.63, mean=4.665, max=4.719, sum=27.988 (6)",
            "tab": "Summarization metrics",
            "score": 4.66471171081461
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.005, mean=0.08, max=0.184, sum=0.24 (3)",
            "tab": "Summarization metrics",
            "score": 0.08008308750782954
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.618, mean=0.71, max=0.826, sum=4.26 (6)",
            "tab": "Summarization metrics",
            "score": 0.7099913231813372
          },
          "CNN/DailyMail - Density": {
            "description": "min=20.964, mean=32.013, max=45.756, sum=192.081 (6)",
            "tab": "Summarization metrics",
            "score": 32.0134921906249
          },
          "CNN/DailyMail - Compression": {
            "description": "min=4.623, mean=5.252, max=6.434, sum=31.514 (6)",
            "tab": "Summarization metrics",
            "score": 5.2523388558949184
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.03,
        "details": {
          "description": "min=0.022, mean=0.03, max=0.038, sum=0.179 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=3.874, mean=3.9, max=3.923, sum=23.4 (6)",
            "tab": "Efficiency",
            "score": 3.899962288877679
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1456.338, mean=1501.338, max=1528.338, sum=9008.027 (6)",
            "tab": "General information",
            "score": 1501.3378378378377
          },
          "XSUM - # output tokens": {
            "description": "min=50.606, mean=54.066, max=57.05, sum=324.394 (6)",
            "tab": "General information",
            "score": 54.06563706563707
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.45, mean=0.467, max=0.5, sum=2.802 (6)",
            "tab": "Bias",
            "score": 0.46699346405228753
          },
          "XSUM - Representation (race)": {
            "description": "min=0.238, mean=0.309, max=0.356, sum=1.856 (6)",
            "tab": "Bias",
            "score": 0.3092501368363437
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.109, mean=0.172, max=0.212, sum=1.032 (6)",
            "tab": "Bias",
            "score": 0.17201180425265794
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "XSUM - SummaC": {
            "description": "min=-0.365, mean=-0.35, max=-0.335, sum=-1.049 (3)",
            "tab": "Summarization metrics",
            "score": -0.3496571157539257
          },
          "XSUM - QAFactEval": {
            "description": "min=4.196, mean=4.778, max=5.107, sum=28.667 (6)",
            "tab": "Summarization metrics",
            "score": 4.77785601273731
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.025, mean=0.059, max=0.095, sum=0.177 (3)",
            "tab": "Summarization metrics",
            "score": 0.05904374779925766
          },
          "XSUM - Coverage": {
            "description": "min=0.48, mean=0.515, max=0.553, sum=3.091 (6)",
            "tab": "Summarization metrics",
            "score": 0.5151319646119767
          },
          "XSUM - Density": {
            "description": "min=1.41, mean=1.764, max=2.014, sum=10.585 (6)",
            "tab": "Summarization metrics",
            "score": 1.764128575895107
          },
          "XSUM - Compression": {
            "description": "min=7.741, mean=8.934, max=10.222, sum=53.603 (6)",
            "tab": "Summarization metrics",
            "score": 8.933804533381347
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB - EM",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.945,
        "details": {
          "description": "min=0.936, mean=0.945, max=0.95, sum=2.836 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.305, mean=0.343, max=0.41, sum=1.029 (3)",
            "tab": "Calibration",
            "score": 0.3430318396761201
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.907, mean=0.92, max=0.927, sum=2.761 (3)",
            "tab": "Robustness",
            "score": 0.9203333333333333
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.927, mean=0.938, max=0.946, sum=2.814 (3)",
            "tab": "Fairness",
            "score": 0.9380000000000001
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=3.425, mean=3.536, max=3.659, sum=10.608 (3)",
            "tab": "Efficiency",
            "score": 3.5360445948161456
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.876, mean=4.943, max=4.987, sum=14.83 (3)",
            "tab": "General information",
            "score": 4.943333333333333
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1129.265, mean=1375.21, max=1727.698, sum=4125.631 (3)",
            "tab": "General information",
            "score": 1375.2103333333334
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments - EM",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.62,
        "details": {
          "description": "min=0.293, mean=0.62, max=0.92, sum=33.467 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.069, mean=0.262, max=0.456, sum=14.142 (54)",
            "tab": "Calibration",
            "score": 0.26189371110201226
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.088, mean=0.467, max=0.827, sum=25.192 (54)",
            "tab": "Robustness",
            "score": 0.46652660062188434
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.252, mean=0.546, max=0.91, sum=29.488 (54)",
            "tab": "Fairness",
            "score": 0.5460670492526992
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.316, mean=0.533, max=1.372, sum=28.76 (54)",
            "tab": "Efficiency",
            "score": 0.5325854907984409
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=327.671, mean=683.498, max=1208.636, sum=36908.883 (54)",
            "tab": "General information",
            "score": 683.497824649871
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT - EM",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.592,
        "details": {
          "description": "min=0.25, mean=0.592, max=0.975, sum=19.525 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.246, mean=0.44, max=0.775, sum=14.508 (33)",
            "tab": "Calibration",
            "score": 0.4396262000869267
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0.175, mean=0.527, max=0.95, sum=17.375 (33)",
            "tab": "Robustness",
            "score": 0.5265151515151515
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.2, mean=0.563, max=0.975, sum=18.575 (33)",
            "tab": "Fairness",
            "score": 0.5628787878787879
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.258, mean=1.866, max=3.777, sum=61.574 (33)",
            "tab": "Efficiency",
            "score": 1.86588385979184
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0.05, mean=4.567, max=5, sum=150.725 (33)",
            "tab": "General information",
            "score": 4.567424242424242
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=234.025, mean=779.203, max=1729.4, sum=25713.7 (33)",
            "tab": "General information",
            "score": 779.2030303030305
          },
          "RAFT - # output tokens": {
            "description": "min=5, mean=7.127, max=13.7, sum=235.2 (33)",
            "tab": "General information",
            "score": 7.127272727272727
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}