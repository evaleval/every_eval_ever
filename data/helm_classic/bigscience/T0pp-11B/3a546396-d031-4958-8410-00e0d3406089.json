{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/bigscience_T0pp-11B/1770830385.7573261",
  "retrieved_timestamp": "1770830385.7573261",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "T0pp 11B",
    "id": "bigscience/T0pp-11B",
    "developer": "bigscience",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.197,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.7577474560592045
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.2275932400932401
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.20273892773892774
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.42000000000000004
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.6045183982683983
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.3965229215229215
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.407,
        "details": {
          "description": "min=0.25, mean=0.407, max=0.67, sum=6.098 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.074, mean=0.168, max=0.3, sum=2.515 (15)",
            "tab": "Calibration",
            "score": 0.16765379656947835
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.25, mean=0.378, max=0.62, sum=5.675 (15)",
            "tab": "Robustness",
            "score": 0.37832748538011696
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.25, mean=0.382, max=0.63, sum=5.731 (15)",
            "tab": "Fairness",
            "score": 0.3820701754385965
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.141, mean=0.145, max=0.149, sum=2.18 (15)",
            "tab": "Efficiency",
            "score": 0.1453571324242486
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=386.05, mean=492.01, max=639.561, sum=7380.154 (15)",
            "tab": "General information",
            "score": 492.0102807017544
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ - EM",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.208, mean=0.322, max=0.435, sum=0.967 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Calibration",
            "score": 0.32218942300251074
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0, mean=0, max=0, sum=0 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Robustness",
            "score": 0.0
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0, mean=0, max=0, sum=0 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Fairness",
            "score": 0.0
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.366, mean=0.374, max=0.385, sum=1.121 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Efficiency",
            "score": 0.3736038734018803
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=2.027, mean=3.972, max=4.988, sum=11.915 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 3.971666666666667
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=479.758, mean=702.438, max=905.932, sum=2107.314 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 702.4380000000001
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "min=0, mean=0.25, max=0.5, sum=0.5 (2)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.25
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.151,
        "details": {
          "description": "min=0.139, mean=0.151, max=0.158, sum=0.454 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.0, mean=0.0, max=0.0, sum=0.0 (3)",
            "tab": "Calibration",
            "score": 0.000042543589701120735
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.087, mean=0.099, max=0.105, sum=0.296 (3)",
            "tab": "Robustness",
            "score": 0.09874765137769782
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.074, mean=0.086, max=0.093, sum=0.258 (3)",
            "tab": "Fairness",
            "score": 0.0858526263629113
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=0.848, mean=0.945, max=1.053, sum=2.834 (3)",
            "tab": "Efficiency",
            "score": 0.9445703822729286
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0, mean=0.187, max=0.33, sum=0.561 (3)",
            "tab": "General information",
            "score": 0.18685446009389672
          },
          "NarrativeQA - truncated": {
            "description": "min=0.369, mean=0.372, max=0.377, sum=1.115 (3)",
            "tab": "General information",
            "score": 0.37183098591549296
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=807.577, mean=877.742, max=916.668, sum=2633.225 (3)",
            "tab": "General information",
            "score": 877.7417840375587
          },
          "NarrativeQA - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=300 (3)",
            "tab": "General information",
            "score": 100.0
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.332, mean=0.339, max=0.343, sum=1.017 (3)",
            "tab": "Bias",
            "score": 0.3389834657156105
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.093, mean=0.105, max=0.113, sum=0.314 (3)",
            "tab": "Bias",
            "score": 0.1046501526237907
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.023, mean=0.023, max=0.025, sum=0.07 (3)",
            "tab": "Toxicity",
            "score": 0.02347417840375587
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.19,
        "details": {
          "description": "min=0.171, mean=0.19, max=0.203, sum=0.569 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.0, mean=0.0, max=0.0, sum=0.0 (3)",
            "tab": "Calibration",
            "score": 3.521055021161368e-9
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.0, mean=0.0, max=0.0, sum=0.0 (3)",
            "tab": "Calibration",
            "score": 0.00009644610962286308
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.03, mean=0.031, max=0.032, sum=0.092 (3)",
            "tab": "Robustness",
            "score": 0.030683511825215847
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.101, mean=0.122, max=0.135, sum=0.367 (3)",
            "tab": "Robustness",
            "score": 0.12220564653363493
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.027, mean=0.028, max=0.03, sum=0.084 (3)",
            "tab": "Fairness",
            "score": 0.028132918197666456
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.119, mean=0.136, max=0.151, sum=0.407 (3)",
            "tab": "Fairness",
            "score": 0.13562055302845238
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=1.309, mean=1.457, max=1.621, sum=4.371 (3)",
            "tab": "Efficiency",
            "score": 1.4571279249547553
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=2.864, mean=2.895, max=2.953, sum=8.685 (3)",
            "tab": "Efficiency",
            "score": 2.8950855693236632
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.556, mean=113.556, max=118.556, sum=340.668 (3)",
            "tab": "General information",
            "score": 113.556
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=900 (3)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=3.164, mean=3.396, max=3.709, sum=10.189 (3)",
            "tab": "General information",
            "score": 3.396333333333333
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.052, mean=0.057, max=0.066, sum=0.172 (3)",
            "tab": "General information",
            "score": 0.057333333333333326
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=850.863, mean=903.877, max=958.904, sum=2711.631 (3)",
            "tab": "General information",
            "score": 903.8770000000001
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=300, mean=300, max=300, sum=900 (3)",
            "tab": "General information",
            "score": 300.0
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.385, mean=0.462, max=0.5, sum=1.385 (3)",
            "tab": "Bias",
            "score": 0.46155024509803927
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.552, mean=0.613, max=0.657, sum=1.84 (3)",
            "tab": "Bias",
            "score": 0.6131917464492584
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.028, mean=0.177, max=0.252, sum=0.53 (3)",
            "tab": "Bias",
            "score": 0.17673498741459906
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.209, mean=0.329, max=0.473, sum=0.987 (3)",
            "tab": "Bias",
            "score": 0.32890264223378113
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.289, mean=0.388, max=0.456, sum=1.164 (3)",
            "tab": "Bias",
            "score": 0.38814814814814813
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.394, mean=0.462, max=0.563, sum=1.386 (3)",
            "tab": "Bias",
            "score": 0.4620750643944221
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.044, mean=0.091, max=0.176, sum=0.273 (3)",
            "tab": "Bias",
            "score": 0.09087407629591253
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.002, sum=0.004 (3)",
            "tab": "Toxicity",
            "score": 0.0013333333333333333
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0.0, max=0.001, sum=0.001 (3)",
            "tab": "Toxicity",
            "score": 0.0003333333333333333
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC - F1",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.121,
        "details": {
          "description": "min=0.121, mean=0.121, max=0.121, sum=0.362 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.002 (3)",
            "tab": "Calibration",
            "score": 0.0005015010499976317
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.071, mean=0.071, max=0.071, sum=0.212 (3)",
            "tab": "Robustness",
            "score": 0.07065126152546952
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.067, mean=0.067, max=0.067, sum=0.201 (3)",
            "tab": "Fairness",
            "score": 0.06691720655918869
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=1.239, mean=1.239, max=1.239, sum=3.716 (3)",
            "tab": "Efficiency",
            "score": 1.2385025575706792
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "QuAC - truncated": {
            "description": "min=0.985, mean=0.985, max=0.985, sum=2.955 (3)",
            "tab": "General information",
            "score": 0.985
          },
          "QuAC - # prompt tokens": {
            "description": "min=823.365, mean=823.365, max=823.365, sum=2470.095 (3)",
            "tab": "General information",
            "score": 823.3650000000001
          },
          "QuAC - # output tokens": {
            "description": "min=100, mean=100, max=100, sum=300 (3)",
            "tab": "General information",
            "score": 100.0
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.428, mean=0.428, max=0.428, sum=1.284 (3)",
            "tab": "Bias",
            "score": 0.42797040922040913
          },
          "QuAC - Representation (race)": {
            "description": "min=0.436, mean=0.436, max=0.436, sum=1.308 (3)",
            "tab": "Bias",
            "score": 0.4358974358974359
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.291, mean=0.291, max=0.291, sum=0.872 (3)",
            "tab": "Bias",
            "score": 0.2905073649754501
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "HellaSwag - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.377,
        "details": {
          "description": "min=0.347, mean=0.377, max=0.411, sum=1.508 (4)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.1, mean=0.154, max=0.234, sum=0.617 (4)",
            "tab": "Calibration",
            "score": 0.15413479575183991
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.33, mean=0.365, max=0.411, sum=1.46 (4)",
            "tab": "Robustness",
            "score": 0.3650611620795107
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.307, mean=0.35, max=0.411, sum=1.399 (4)",
            "tab": "Fairness",
            "score": 0.34977064220183485
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.133, mean=0.142, max=0.145, sum=0.567 (4)",
            "tab": "Efficiency",
            "score": 0.14173421436146078
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=2616 (4)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=0, mean=3.75, max=5, sum=15 (4)",
            "tab": "General information",
            "score": 3.75
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (4)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=85.896, mean=391.646, max=515.896, sum=1566.584 (4)",
            "tab": "General information",
            "score": 391.6460244648318
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=4 (4)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=2.5, max=3, sum=10 (4)",
            "tab": "General information",
            "score": 2.5
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.122,
        "details": {
          "description": "min=0.121, mean=0.122, max=0.122, sum=0.73 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=1.057, mean=1.066, max=1.081, sum=6.393 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Efficiency",
            "score": 1.0655231237061773
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=1.303, mean=1.335, max=1.378, sum=8.013 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 1.3354792560801145
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0.004, mean=0.004, max=0.004, sum=0.026 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 0.004291845493562232
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=885.292, mean=886.838, max=888.921, sum=5321.026 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 886.8376251788268
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=128, mean=128, max=128, sum=768 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 128.0
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.562, mean=0.594, max=0.631, sum=3.562 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.5936999598322023
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.391, mean=0.403, max=0.421, sum=2.417 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.4028700462262689
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.27, mean=0.277, max=0.282, sum=1.662 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.2769263317991031
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.047, mean=0.093, max=0.138, sum=0.559 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.09311410441258088
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.009 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Toxicity",
            "score": 0.001430615164520744
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=-0.052, mean=-0.044, max=-0.031, sum=-0.132 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": -0.04384894228805586
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.151, mean=0.155, max=0.163, sum=0.465 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 0.1550916195946839
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.836, mean=0.841, max=0.845, sum=5.047 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 0.841192270385719
          },
          "CNN/DailyMail - Density": {
            "description": "min=8.147, mean=8.588, max=8.816, sum=51.53 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 8.588383920302716
          },
          "CNN/DailyMail - Compression": {
            "description": "min=8.169, mean=8.274, max=8.416, sum=49.643 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 8.27387938295926
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.09,
        "details": {
          "description": "min=0.07, mean=0.09, max=0.103, sum=0.539 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=0.523, mean=0.554, max=0.571, sum=3.326 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Efficiency",
            "score": 0.5543883131537052
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=1.967, mean=2.068, max=2.214, sum=12.405 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 2.0675675675675675
          },
          "XSUM - truncated": {
            "description": "min=0.002, mean=0.01, max=0.019, sum=0.058 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 0.009652509652509652
          },
          "XSUM - # prompt tokens": {
            "description": "min=889.981, mean=907.769, max=929.006, sum=5446.614 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 907.7689832689833
          },
          "XSUM - # output tokens": {
            "description": "min=64, mean=64, max=64, sum=384 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 64.0
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.43, mean=0.444, max=0.463, sum=2.663 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.4438297255067441
          },
          "XSUM - Representation (race)": {
            "description": "min=0.286, mean=0.457, max=0.617, sum=2.74 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.45673778645470176
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.215, mean=0.27, max=0.328, sum=1.62 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Bias",
            "score": 0.2699471127776433
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.004 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Toxicity",
            "score": 0.0006435006435006435
          },
          "XSUM - SummaC": {
            "description": "min=-0.331, mean=-0.3, max=-0.268, sum=-0.901 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": -0.3004745337800477
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.083, mean=0.097, max=0.111, sum=0.292 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 0.09723521885401472
          },
          "XSUM - Coverage": {
            "description": "min=0.543, mean=0.579, max=0.605, sum=3.474 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 0.5789418979978066
          },
          "XSUM - Density": {
            "description": "min=1.492, mean=1.684, max=1.861, sum=10.105 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 1.6841663389066148
          },
          "XSUM - Compression": {
            "description": "min=10.341, mean=11.178, max=11.672, sum=67.065 (6)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Summarization metrics",
            "score": 11.17756803869132
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB - EM",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.207,
        "details": {
          "description": "min=0.181, mean=0.207, max=0.26, sum=0.622 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.207, mean=0.291, max=0.36, sum=0.872 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Calibration",
            "score": 0.29061500207311436
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.13, mean=0.17, max=0.227, sum=0.511 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Robustness",
            "score": 0.17033333333333334
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.129, mean=0.168, max=0.22, sum=0.505 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Fairness",
            "score": 0.16833333333333333
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=0.37, mean=0.393, max=0.436, sum=1.18 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "Efficiency",
            "score": 0.39343433208828427
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=1.981, mean=2.44, max=3.074, sum=7.321 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 2.4403333333333332
          },
          "IMDB - truncated": {
            "description": "min=0.03, mean=0.03, max=0.03, sum=0.09 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 0.03
          },
          "IMDB - # prompt tokens": {
            "description": "min=905.879, mean=910.174, max=913.752, sum=2730.521 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 910.1736666666666
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)\n☠ T0++ is explicitly trained on these datasets, i.e. data from the same distribution as the test set. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments - EM",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.234,
        "details": {
          "description": "min=0, mean=0.234, max=0.985, sum=12.634 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.067, mean=0.308, max=0.574, sum=16.631 (54)",
            "tab": "Calibration",
            "score": 0.30797595023001567
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.087, max=0.824, sum=4.704 (54)",
            "tab": "Robustness",
            "score": 0.0871064519307774
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.165, max=0.947, sum=8.894 (54)",
            "tab": "Fairness",
            "score": 0.16470832145418626
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.328, mean=0.391, max=0.487, sum=21.126 (54)",
            "tab": "Efficiency",
            "score": 0.3912135341654548
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=2.991, mean=4.861, max=5, sum=262.497 (54)",
            "tab": "General information",
            "score": 4.861055391438897
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=385.732, mean=744.109, max=936.562, sum=40181.894 (54)",
            "tab": "General information",
            "score": 744.1091399163704
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "min=0.3, mean=0.459, max=0.5, sum=5.503 (12)",
            "tab": "Bias",
            "score": 0.4585978835978836
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0.0, max=0.008, sum=0.025 (54)",
            "tab": "Toxicity",
            "score": 0.0004596436870303355
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT - EM",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.118,
        "details": {
          "description": "min=0, mean=0.118, max=0.775, sum=3.9 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.0, mean=0.086, max=0.573, sum=2.84 (33)",
            "tab": "Calibration",
            "score": 0.08607203532710274
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.085, max=0.775, sum=2.8 (33)",
            "tab": "Robustness",
            "score": 0.08484848484848484
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0, mean=0.106, max=0.75, sum=3.5 (33)",
            "tab": "Fairness",
            "score": 0.10606060606060606
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.329, mean=0.586, max=0.74, sum=19.352 (33)",
            "tab": "Efficiency",
            "score": 0.586429068475456
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=3.913, max=5, sum=129.125 (33)",
            "tab": "General information",
            "score": 3.912878787878788
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0.09, max=0.925, sum=2.975 (33)",
            "tab": "General information",
            "score": 0.09015151515151516
          },
          "RAFT - # prompt tokens": {
            "description": "min=263.4, mean=650.012, max=949.7, sum=21450.4 (33)",
            "tab": "General information",
            "score": 650.0121212121212
          },
          "RAFT - # output tokens": {
            "description": "min=30, mean=30, max=30, sum=990 (33)",
            "tab": "General information",
            "score": 30.0
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "min=0.125, mean=0.125, max=0.125, sum=0.375 (3)",
            "tab": "Bias",
            "score": 0.12500000000000003
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}