{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/GPT-NeoX-(20B)/1767656643.700665",
  "retrieved_timestamp": "1767656643.700665",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "GPT-NeoX (20B)",
    "id": "GPT-NeoX-(20B)",
    "developer": "unknown",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.351,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.4215761012322838
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.3361523348731358
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.3311530516202374
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.5141337719298246
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.46836548983528487
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.36547434047434046
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.4456349206349206
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.276,
        "details": {
          "description": "min=0.21, mean=0.276, max=0.351, sum=4.146 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.094, mean=0.122, max=0.145, sum=1.831 (15)",
            "tab": "Calibration",
            "score": 0.12205035764205192
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.149, mean=0.189, max=0.24, sum=2.833 (15)",
            "tab": "Robustness",
            "score": 0.1888421052631579
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.175, mean=0.215, max=0.26, sum=3.228 (15)",
            "tab": "Fairness",
            "score": 0.21518128654970764
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.093, mean=0.133, max=0.275, sum=1.995 (15)",
            "tab": "Efficiency",
            "score": 0.1330090104470642
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=358.76, mean=467.936, max=612.798, sum=7019.035 (15)",
            "tab": "General information",
            "score": 467.935649122807
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.683,
        "details": {
          "description": "min=0.659, mean=0.683, max=0.714, sum=2.048 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.168, mean=0.195, max=0.238, sum=0.585 (3)",
            "tab": "Calibration",
            "score": 0.19500535688345313
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.548, mean=0.551, max=0.556, sum=1.653 (3)",
            "tab": "Robustness",
            "score": 0.551
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.594, mean=0.609, max=0.629, sum=1.827 (3)",
            "tab": "Fairness",
            "score": 0.609
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.515, mean=0.773, max=1.206, sum=2.318 (3)",
            "tab": "Efficiency",
            "score": 0.772616056262233
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=656.897, mean=913.897, max=1251.897, sum=2741.691 (3)",
            "tab": "General information",
            "score": 913.8969999999999
          },
          "BoolQ - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.599,
        "details": {
          "description": "min=0.558, mean=0.599, max=0.623, sum=1.797 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.2, mean=0.224, max=0.244, sum=0.672 (3)",
            "tab": "Calibration",
            "score": 0.2239646545151891
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.378, mean=0.421, max=0.443, sum=1.263 (3)",
            "tab": "Robustness",
            "score": 0.4211068794456416
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.419, mean=0.461, max=0.485, sum=1.382 (3)",
            "tab": "Fairness",
            "score": 0.46066534756418576
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=0.904, mean=1.468, max=1.998, sum=4.404 (3)",
            "tab": "Efficiency",
            "score": 1.4680144681286658
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0.989, mean=1.568, max=1.969, sum=4.704 (3)",
            "tab": "General information",
            "score": 1.568075117370892
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1607.893, mean=1641.033, max=1691.082, sum=4923.099 (3)",
            "tab": "General information",
            "score": 1641.0328638497651
          },
          "NarrativeQA - # output tokens": {
            "description": "min=24.282, mean=40.047, max=54.028, sum=120.141 (3)",
            "tab": "General information",
            "score": 40.04694835680751
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.396, mean=0.449, max=0.5, sum=1.346 (3)",
            "tab": "Bias",
            "score": 0.44861111111111107
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.159, mean=0.186, max=0.206, sum=0.557 (3)",
            "tab": "Bias",
            "score": 0.18579713036394171
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.017, mean=0.022, max=0.025, sum=0.065 (3)",
            "tab": "Toxicity",
            "score": 0.0215962441314554
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.596,
        "details": {
          "description": "min=0.581, mean=0.596, max=0.608, sum=1.788 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.099, mean=0.103, max=0.106, sum=0.309 (3)",
            "tab": "Calibration",
            "score": 0.10315653555419742
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.371, mean=0.373, max=0.375, sum=1.118 (3)",
            "tab": "Calibration",
            "score": 0.37278118995003706
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.125, mean=0.133, max=0.14, sum=0.398 (3)",
            "tab": "Robustness",
            "score": 0.1325934362402064
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.429, mean=0.452, max=0.48, sum=1.357 (3)",
            "tab": "Robustness",
            "score": 0.4524359199313521
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.147, mean=0.154, max=0.158, sum=0.461 (3)",
            "tab": "Fairness",
            "score": 0.15381312093617092
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.509, mean=0.525, max=0.537, sum=1.574 (3)",
            "tab": "Fairness",
            "score": 0.524698076718683
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=0.381, mean=0.482, max=0.655, sum=1.447 (3)",
            "tab": "Efficiency",
            "score": 0.4823250982166127
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=1.913, mean=2.137, max=2.288, sum=6.411 (3)",
            "tab": "Efficiency",
            "score": 2.1369374864319965
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=110.299, mean=112.966, max=117.299, sum=338.897 (3)",
            "tab": "General information",
            "score": 112.96566666666668
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=77.379, mean=90.195, max=107.541, sum=270.584 (3)",
            "tab": "General information",
            "score": 90.19466666666666
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.685, mean=4.704, max=4.723, sum=14.112 (3)",
            "tab": "General information",
            "score": 4.704
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.037, mean=0.037, max=0.037, sum=0.111 (3)",
            "tab": "General information",
            "score": 0.037
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1247.862, mean=1394.229, max=1495.552, sum=4182.688 (3)",
            "tab": "General information",
            "score": 1394.2293333333334
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=73.671, mean=87.693, max=98.984, sum=263.078 (3)",
            "tab": "General information",
            "score": 87.69266666666665
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1 (2)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.309, mean=0.362, max=0.444, sum=1.086 (3)",
            "tab": "Bias",
            "score": 0.3621399176954732
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.233, mean=0.318, max=0.382, sum=0.954 (3)",
            "tab": "Bias",
            "score": 0.31784137078254726
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.538, mean=0.57, max=0.59, sum=1.709 (3)",
            "tab": "Bias",
            "score": 0.5695499220251695
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0, mean=0.094, max=0.241, sum=0.283 (3)",
            "tab": "Bias",
            "score": 0.09428104575163399
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.002, max=0.003, sum=0.006 (3)",
            "tab": "Toxicity",
            "score": 0.002
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.326,
        "details": {
          "description": "min=0.32, mean=0.326, max=0.335, sum=0.979 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.105, mean=0.115, max=0.129, sum=0.345 (3)",
            "tab": "Calibration",
            "score": 0.11494333135422596
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.176, mean=0.191, max=0.202, sum=0.574 (3)",
            "tab": "Robustness",
            "score": 0.19141062427574787
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.224, mean=0.232, max=0.243, sum=0.695 (3)",
            "tab": "Fairness",
            "score": 0.23177797124335245
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=1.906, mean=2.025, max=2.127, sum=6.075 (3)",
            "tab": "Efficiency",
            "score": 2.024874148220674
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.804, mean=0.889, max=0.979, sum=2.666 (3)",
            "tab": "General information",
            "score": 0.8886666666666666
          },
          "QuAC - truncated": {
            "description": "min=0.021, mean=0.021, max=0.021, sum=0.063 (3)",
            "tab": "General information",
            "score": 0.021
          },
          "QuAC - # prompt tokens": {
            "description": "min=1602.026, mean=1640.361, max=1663.349, sum=4921.083 (3)",
            "tab": "General information",
            "score": 1640.3609999999999
          },
          "QuAC - # output tokens": {
            "description": "min=73.99, mean=77.489, max=80.665, sum=232.466 (3)",
            "tab": "General information",
            "score": 77.48866666666667
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.606, mean=0.626, max=0.639, sum=1.877 (3)",
            "tab": "Bias",
            "score": 0.6257674787086551
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.436, mean=0.448, max=0.455, sum=1.344 (3)",
            "tab": "Bias",
            "score": 0.4481503328194676
          },
          "QuAC - Representation (race)": {
            "description": "min=0.319, mean=0.334, max=0.354, sum=1.003 (3)",
            "tab": "Bias",
            "score": 0.3344046827039365
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.258, mean=0.268, max=0.282, sum=0.804 (3)",
            "tab": "Bias",
            "score": 0.26793463346025864
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.718,
        "details": {
          "description": "min=0.718, mean=0.718, max=0.718, sum=0.718 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.277, mean=0.277, max=0.277, sum=0.277 (1)",
            "tab": "Calibration",
            "score": 0.2773372160584027
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.661, mean=0.661, max=0.661, sum=0.661 (1)",
            "tab": "Robustness",
            "score": 0.661
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.552, mean=0.552, max=0.552, sum=0.552 (1)",
            "tab": "Fairness",
            "score": 0.552
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "min=0.025, mean=0.025, max=0.025, sum=0.025 (1)",
            "tab": "Efficiency",
            "score": 0.025470768198370932
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.806, mean=88.806, max=88.806, sum=88.806 (1)",
            "tab": "General information",
            "score": 88.806
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.524,
        "details": {
          "description": "min=0.524, mean=0.524, max=0.524, sum=0.524 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.232, mean=0.232, max=0.232, sum=0.232 (1)",
            "tab": "Calibration",
            "score": 0.23249621701719156
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.414, mean=0.414, max=0.414, sum=0.414 (1)",
            "tab": "Robustness",
            "score": 0.414
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
            "tab": "Fairness",
            "score": 0.438
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "min=0.024, mean=0.024, max=0.024, sum=0.024 (1)",
            "tab": "Efficiency",
            "score": 0.023963596328905958
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.346, mean=5.346, max=5.346, sum=5.346 (1)",
            "tab": "General information",
            "score": 5.346
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.216,
        "details": {
          "description": "min=0.205, mean=0.216, max=0.225, sum=0.864 (4)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.051, mean=0.058, max=0.068, sum=0.232 (4)",
            "tab": "Calibration",
            "score": 0.057891800582365614
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.144, mean=0.175, max=0.225, sum=0.7 (4)",
            "tab": "Robustness",
            "score": 0.17507645259938837
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.161, mean=0.179, max=0.225, sum=0.714 (4)",
            "tab": "Fairness",
            "score": 0.17851681957186544
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.029, mean=0.084, max=0.133, sum=0.335 (4)",
            "tab": "Efficiency",
            "score": 0.08375055263898766
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=2616 (4)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=0, mean=3.75, max=5, sum=15 (4)",
            "tab": "General information",
            "score": 3.75
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (4)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=86.352, mean=406.102, max=532.352, sum=1624.407 (4)",
            "tab": "General information",
            "score": 406.10168195718654
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=4 (4)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=2.5, max=3, sum=10 (4)",
            "tab": "General information",
            "score": 2.5
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.398,
        "details": {
          "description": "min=0.37, mean=0.398, max=0.436, sum=1.195 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.082, mean=0.096, max=0.107, sum=0.288 (3)",
            "tab": "Robustness",
            "score": 0.09600105820105831
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.338, mean=0.351, max=0.365, sum=1.053 (3)",
            "tab": "Robustness",
            "score": 0.3510422646487042
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.137, mean=0.148, max=0.163, sum=0.445 (3)",
            "tab": "Fairness",
            "score": 0.1483276455026454
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.347, mean=0.381, max=0.416, sum=1.144 (3)",
            "tab": "Fairness",
            "score": 0.38125183165300675
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "min=0.111, mean=0.118, max=0.128, sum=0.355 (3)",
            "tab": "Efficiency",
            "score": 0.11821914517316674
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "min=0.105, mean=0.116, max=0.127, sum=0.349 (3)",
            "tab": "Efficiency",
            "score": 0.11621723726407733
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=499.575, mean=537.908, max=583.575, sum=1613.725 (3)",
            "tab": "General information",
            "score": 537.9083333333334
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=481.14, mean=519.473, max=565.14, sum=1558.419 (3)",
            "tab": "General information",
            "score": 519.4728682170543
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.123,
        "details": {
          "description": "min=0.108, mean=0.123, max=0.138, sum=0.738 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=2.104, mean=2.133, max=2.168, sum=12.798 (6)",
            "tab": "Efficiency",
            "score": 2.133056901521097
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1561.275, mean=1582.608, max=1612.275, sum=9495.648 (6)",
            "tab": "General information",
            "score": 1582.6080114449214
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=80.197, mean=80.409, max=80.588, sum=482.455 (6)",
            "tab": "General information",
            "score": 80.40915593705294
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.612, mean=0.616, max=0.62, sum=3.697 (6)",
            "tab": "Bias",
            "score": 0.6162431158667614
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.386, mean=0.41, max=0.431, sum=2.46 (6)",
            "tab": "Bias",
            "score": 0.4099353286102709
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.182, mean=0.289, max=0.35, sum=1.732 (6)",
            "tab": "Bias",
            "score": 0.288716873622534
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.127, mean=0.149, max=0.168, sum=0.896 (6)",
            "tab": "Bias",
            "score": 0.14933277507884896
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.009 (6)",
            "tab": "Toxicity",
            "score": 0.001430615164520744
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=-0.009, mean=0.165, max=0.255, sum=0.494 (3)",
            "tab": "Summarization metrics",
            "score": 0.16465107490254738
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "min=4.591, mean=4.69, max=4.763, sum=28.138 (6)",
            "tab": "Summarization metrics",
            "score": 4.689614935266213
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.175, mean=0.226, max=0.262, sum=0.677 (3)",
            "tab": "Summarization metrics",
            "score": 0.2255769362361307
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.786, mean=0.91, max=0.973, sum=5.46 (6)",
            "tab": "Summarization metrics",
            "score": 0.910005755446767
          },
          "CNN/DailyMail - Density": {
            "description": "min=35.834, mean=37.149, max=38.818, sum=222.893 (6)",
            "tab": "Summarization metrics",
            "score": 37.14890205441478
          },
          "CNN/DailyMail - Compression": {
            "description": "min=9.164, mean=9.676, max=9.978, sum=58.057 (6)",
            "tab": "Summarization metrics",
            "score": 9.676104726319009
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.102,
        "details": {
          "description": "min=0.098, mean=0.102, max=0.105, sum=0.61 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=1.104, mean=1.116, max=1.135, sum=6.698 (6)",
            "tab": "Efficiency",
            "score": 1.1163698516910754
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.996, mean=4.997, max=5, sum=29.985 (6)",
            "tab": "General information",
            "score": 4.997425997425997
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1487.131, mean=1545.148, max=1574.17, sum=9270.888 (6)",
            "tab": "General information",
            "score": 1545.148005148005
          },
          "XSUM - # output tokens": {
            "description": "min=24.871, mean=25.402, max=26.143, sum=152.413 (6)",
            "tab": "General information",
            "score": 25.402187902187904
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.449, mean=0.449, max=0.449, sum=2.694 (6)",
            "tab": "Bias",
            "score": 0.4490600226000671
          },
          "XSUM - Representation (race)": {
            "description": "min=0.483, mean=0.526, max=0.565, sum=3.158 (6)",
            "tab": "Bias",
            "score": 0.5263835263835264
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.132, mean=0.162, max=0.184, sum=0.972 (6)",
            "tab": "Bias",
            "score": 0.16191706040214252
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.002, max=0.004, sum=0.012 (6)",
            "tab": "Toxicity",
            "score": 0.0019305019305019308
          },
          "XSUM - SummaC": {
            "description": "min=-0.22, mean=-0.208, max=-0.2, sum=-0.625 (3)",
            "tab": "Summarization metrics",
            "score": -0.2082928215061222
          },
          "XSUM - QAFactEval": {
            "description": "min=3.048, mean=3.303, max=3.621, sum=19.818 (6)",
            "tab": "Summarization metrics",
            "score": 3.302964744932122
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.385, mean=0.391, max=0.395, sum=1.174 (3)",
            "tab": "Summarization metrics",
            "score": 0.39129907447599627
          },
          "XSUM - Coverage": {
            "description": "min=0.822, mean=0.825, max=0.83, sum=4.948 (6)",
            "tab": "Summarization metrics",
            "score": 0.8247285888112758
          },
          "XSUM - Density": {
            "description": "min=3.228, mean=3.371, max=3.613, sum=20.226 (6)",
            "tab": "Summarization metrics",
            "score": 3.3710531876366
          },
          "XSUM - Compression": {
            "description": "min=17.631, mean=18.238, max=18.621, sum=109.428 (6)",
            "tab": "Summarization metrics",
            "score": 18.23798025069092
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.948,
        "details": {
          "description": "min=0.946, mean=0.948, max=0.95, sum=2.844 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.189, mean=0.23, max=0.269, sum=0.69 (3)",
            "tab": "Calibration",
            "score": 0.22988586030197733
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.906, mean=0.912, max=0.921, sum=2.736 (3)",
            "tab": "Robustness",
            "score": 0.9119999999999999
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.925, mean=0.928, max=0.933, sum=2.785 (3)",
            "tab": "Fairness",
            "score": 0.9283333333333333
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=0.748, mean=0.862, max=1.078, sum=2.586 (3)",
            "tab": "Efficiency",
            "score": 0.862092325799332
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.842, mean=4.93, max=4.981, sum=14.789 (3)",
            "tab": "General information",
            "score": 4.929666666666667
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1162.003, mean=1398.09, max=1750.717, sum=4194.271 (3)",
            "tab": "General information",
            "score": 1398.0903333333333
          },
          "IMDB - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.516,
        "details": {
          "description": "min=0, mean=0.516, max=1, sum=27.878 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.26, mean=0.444, max=0.593, sum=23.994 (54)",
            "tab": "Calibration",
            "score": 0.4443373993811643
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.48, max=1, sum=25.9 (54)",
            "tab": "Robustness",
            "score": 0.4796354739742704
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.491, max=1, sum=26.497 (54)",
            "tab": "Fairness",
            "score": 0.4906931444587031
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.253, mean=0.408, max=0.906, sum=22.04 (54)",
            "tab": "Efficiency",
            "score": 0.4081493504712871
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=360.976, mean=726.728, max=1282.4, sum=39243.315 (54)",
            "tab": "General information",
            "score": 726.7280588093369
          },
          "CivilComments - # output tokens": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.505,
        "details": {
          "description": "min=0.025, mean=0.505, max=0.975, sum=16.65 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.067, mean=0.324, max=0.975, sum=10.705 (33)",
            "tab": "Calibration",
            "score": 0.3243919141625793
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.399, max=0.975, sum=13.175 (33)",
            "tab": "Robustness",
            "score": 0.39924242424242423
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.025, mean=0.475, max=0.975, sum=15.675 (33)",
            "tab": "Fairness",
            "score": 0.47500000000000003
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.16, mean=1.156, max=2.589, sum=38.155 (33)",
            "tab": "Efficiency",
            "score": 1.1562087950381366
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.56, max=5, sum=150.475 (33)",
            "tab": "General information",
            "score": 4.5598484848484855
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=269.35, mean=807.97, max=1764, sum=26663.0 (33)",
            "tab": "General information",
            "score": 807.9696969696969
          },
          "RAFT - # output tokens": {
            "description": "min=5, mean=13.945, max=30, sum=460.2 (33)",
            "tab": "General information",
            "score": 13.945454545454545
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}