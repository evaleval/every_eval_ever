{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/Cohere-Command-beta-(6.1B)/1767656643.700665",
  "retrieved_timestamp": "1767656643.700665",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Cohere Command beta (6.1B)",
    "id": "Cohere-Command-beta-(6.1B)",
    "developer": "unknown",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.675,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.5291111339523303
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.6159776448986682
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.66227113635345
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.43551719208606965
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.6688037271370605
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.5789473684210527
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.406,
        "details": {
          "description": "min=0.26, mean=0.406, max=0.63, sum=6.095 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.103, mean=0.155, max=0.243, sum=2.327 (15)",
            "tab": "Calibration",
            "score": 0.1551609000421963
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.2, mean=0.334, max=0.54, sum=5.009 (15)",
            "tab": "Robustness",
            "score": 0.33394152046783626
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.2, mean=0.366, max=0.55, sum=5.495 (15)",
            "tab": "Fairness",
            "score": 0.36630409356725147
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=372.75, mean=481.26, max=628.421, sum=7218.903 (15)",
            "tab": "General information",
            "score": 481.2602105263158
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.798,
        "details": {
          "description": "min=0.791, mean=0.798, max=0.809, sum=2.394 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.048, mean=0.059, max=0.069, sum=0.178 (3)",
            "tab": "Calibration",
            "score": 0.0594622129465324
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.715, mean=0.725, max=0.743, sum=2.176 (3)",
            "tab": "Robustness",
            "score": 0.7253333333333334
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.74, mean=0.748, max=0.76, sum=2.244 (3)",
            "tab": "Fairness",
            "score": 0.7479999999999999
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=669.307, mean=925.307, max=1269.307, sum=2775.921 (3)",
            "tab": "General information",
            "score": 925.3070000000001
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.709,
        "details": {
          "description": "min=0.707, mean=0.709, max=0.712, sum=2.128 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.075, mean=0.076, max=0.077, sum=0.228 (3)",
            "tab": "Calibration",
            "score": 0.07599807506781359
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.515, mean=0.529, max=0.539, sum=1.586 (3)",
            "tab": "Robustness",
            "score": 0.5285770759196127
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.592, mean=0.595, max=0.6, sum=1.785 (3)",
            "tab": "Fairness",
            "score": 0.5949605221040284
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0.904, mean=1.508, max=1.941, sum=4.524 (3)",
            "tab": "General information",
            "score": 1.5079812206572771
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1570.772, mean=1600.684, max=1660.485, sum=4802.051 (3)",
            "tab": "General information",
            "score": 1600.6835680751174
          },
          "NarrativeQA - # output tokens": {
            "description": "min=5.301, mean=5.807, max=6.217, sum=17.42 (3)",
            "tab": "General information",
            "score": 5.8065727699530525
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.463, mean=0.488, max=0.5, sum=1.463 (3)",
            "tab": "Bias",
            "score": 0.48765432098765427
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.126, mean=0.144, max=0.169, sum=0.432 (3)",
            "tab": "Bias",
            "score": 0.14398558425056623
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.008, mean=0.01, max=0.014, sum=0.031 (3)",
            "tab": "Toxicity",
            "score": 0.010328638497652582
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.717,
        "details": {
          "description": "min=0.714, mean=0.717, max=0.724, sum=2.152 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.04, mean=0.042, max=0.046, sum=0.127 (3)",
            "tab": "Calibration",
            "score": 0.04227945276969597
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.045, mean=0.057, max=0.074, sum=0.172 (3)",
            "tab": "Calibration",
            "score": 0.057325907163997956
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.156, mean=0.163, max=0.171, sum=0.489 (3)",
            "tab": "Robustness",
            "score": 0.163031767310864
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.596, mean=0.605, max=0.616, sum=1.815 (3)",
            "tab": "Robustness",
            "score": 0.6050162193677248
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.165, mean=0.167, max=0.167, sum=0.5 (3)",
            "tab": "Fairness",
            "score": 0.16652011745655915
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.647, mean=0.654, max=0.66, sum=1.962 (3)",
            "tab": "Fairness",
            "score": 0.6540942012407344
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.191, mean=111.191, max=115.191, sum=333.573 (3)",
            "tab": "General information",
            "score": 111.19099999999999
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=4.428, mean=4.687, max=4.995, sum=14.06 (3)",
            "tab": "General information",
            "score": 4.6866666666666665
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.485, mean=4.602, max=4.705, sum=13.807 (3)",
            "tab": "General information",
            "score": 4.602333333333333
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.039, mean=0.039, max=0.039, sum=0.117 (3)",
            "tab": "General information",
            "score": 0.039
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1258.15, mean=1471.073, max=1597.431, sum=4413.22 (3)",
            "tab": "General information",
            "score": 1471.073333333333
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=7.147, mean=7.377, max=7.586, sum=22.131 (3)",
            "tab": "General information",
            "score": 7.377
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.421, mean=0.465, max=0.506, sum=1.394 (3)",
            "tab": "Bias",
            "score": 0.46474105132386057
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.1, mean=0.183, max=0.3, sum=0.55 (3)",
            "tab": "Bias",
            "score": 0.18333333333333335
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.473, mean=0.487, max=0.509, sum=1.46 (3)",
            "tab": "Bias",
            "score": 0.48677896291115386
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.348, mean=0.356, max=0.363, sum=1.068 (3)",
            "tab": "Bias",
            "score": 0.3560153609831029
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0.0, max=0.001, sum=0.001 (3)",
            "tab": "Toxicity",
            "score": 0.0003333333333333333
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0.0, max=0.001, sum=0.001 (3)",
            "tab": "Toxicity",
            "score": 0.0003333333333333333
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.375,
        "details": {
          "description": "min=0.371, mean=0.375, max=0.379, sum=1.125 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.054, mean=0.062, max=0.067, sum=0.186 (3)",
            "tab": "Calibration",
            "score": 0.06185077042352865
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.159, mean=0.17, max=0.178, sum=0.511 (3)",
            "tab": "Robustness",
            "score": 0.17034790269142241
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.268, mean=0.273, max=0.279, sum=0.819 (3)",
            "tab": "Fairness",
            "score": 0.2730533859766594
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.748, mean=0.848, max=0.933, sum=2.545 (3)",
            "tab": "General information",
            "score": 0.8483333333333333
          },
          "QuAC - truncated": {
            "description": "min=0.022, mean=0.022, max=0.022, sum=0.066 (3)",
            "tab": "General information",
            "score": 0.022000000000000002
          },
          "QuAC - # prompt tokens": {
            "description": "min=1577.224, mean=1610.503, max=1643.74, sum=4831.508 (3)",
            "tab": "General information",
            "score": 1610.5026666666665
          },
          "QuAC - # output tokens": {
            "description": "min=16.185, mean=17.394, max=18.299, sum=52.182 (3)",
            "tab": "General information",
            "score": 17.394
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.469, mean=0.471, max=0.475, sum=1.414 (3)",
            "tab": "Bias",
            "score": 0.47144607843137254
          },
          "QuAC - Representation (race)": {
            "description": "min=0.312, mean=0.356, max=0.423, sum=1.069 (3)",
            "tab": "Bias",
            "score": 0.35619490458200137
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.236, mean=0.248, max=0.259, sum=0.743 (3)",
            "tab": "Bias",
            "score": 0.2476420794142787
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.002, mean=0.002, max=0.002, sum=0.006 (3)",
            "tab": "Toxicity",
            "score": 0.002
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.752,
        "details": {
          "description": "min=0.752, mean=0.752, max=0.752, sum=0.752 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.293, mean=0.293, max=0.293, sum=0.293 (1)",
            "tab": "Calibration",
            "score": 0.2926835489814197
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.696, mean=0.696, max=0.696, sum=0.696 (1)",
            "tab": "Robustness",
            "score": 0.696
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.608, mean=0.608, max=0.608, sum=0.608 (1)",
            "tab": "Fairness",
            "score": 0.608
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.855, mean=88.855, max=88.855, sum=88.855 (1)",
            "tab": "General information",
            "score": 88.855
          },
          "HellaSwag - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.55,
        "details": {
          "description": "min=0.55, mean=0.55, max=0.55, sum=0.55 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
            "tab": "Calibration",
            "score": 0.2504061981122775
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.448, mean=0.448, max=0.448, sum=0.448 (1)",
            "tab": "Robustness",
            "score": 0.448
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.468, mean=0.468, max=0.468, sum=0.468 (1)",
            "tab": "Fairness",
            "score": 0.468
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.358, mean=5.358, max=5.358, sum=5.358 (1)",
            "tab": "General information",
            "score": 5.358
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.203,
        "details": {
          "description": "min=0.197, mean=0.203, max=0.213, sum=0.61 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.275, mean=0.3, max=0.332, sum=0.901 (3)",
            "tab": "Calibration",
            "score": 0.3001833323753285
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.168, mean=0.171, max=0.174, sum=0.512 (3)",
            "tab": "Robustness",
            "score": 0.17074413863404692
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.154, mean=0.163, max=0.167, sum=0.488 (3)",
            "tab": "Fairness",
            "score": 0.16258919469928643
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.315, mean=514.648, max=532.315, sum=1543.945 (3)",
            "tab": "General information",
            "score": 514.6483180428135
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.709,
        "details": {
          "description": "min=0.702, mean=0.709, max=0.717, sum=2.128 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.372, mean=0.387, max=0.401, sum=1.161 (3)",
            "tab": "Robustness",
            "score": 0.386937698412698
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.68, mean=0.685, max=0.689, sum=2.054 (3)",
            "tab": "Robustness",
            "score": 0.6845367765287401
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.402, mean=0.411, max=0.42, sum=1.232 (3)",
            "tab": "Fairness",
            "score": 0.4107572751322747
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.68, mean=0.69, max=0.696, sum=2.069 (3)",
            "tab": "Fairness",
            "score": 0.6896233668786421
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=497.281, mean=536.614, max=583.281, sum=1609.843 (3)",
            "tab": "General information",
            "score": 536.6143333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=480.163, mean=519.496, max=566.163, sum=1558.488 (3)",
            "tab": "General information",
            "score": 519.4961240310078
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.153,
        "details": {
          "description": "min=0.15, mean=0.153, max=0.158, sum=0.919 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1555.036, mean=1575.036, max=1602.036, sum=9450.219 (6)",
            "tab": "General information",
            "score": 1575.0364806866953
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=69.622, mean=73.723, max=77.732, sum=442.339 (6)",
            "tab": "General information",
            "score": 73.72317596566523
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.594, mean=0.603, max=0.609, sum=3.618 (6)",
            "tab": "Bias",
            "score": 0.6029930306246096
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.384, mean=0.408, max=0.421, sum=2.449 (6)",
            "tab": "Bias",
            "score": 0.40820094830714143
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.245, mean=0.259, max=0.269, sum=1.553 (6)",
            "tab": "Bias",
            "score": 0.2588148950314076
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.116, mean=0.121, max=0.127, sum=0.724 (6)",
            "tab": "Bias",
            "score": 0.1206019792299876
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.318, mean=0.331, max=0.342, sum=0.992 (3)",
            "tab": "Summarization metrics",
            "score": 0.3306993242099164
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.289, mean=0.296, max=0.305, sum=0.888 (3)",
            "tab": "Summarization metrics",
            "score": 0.29605955170271475
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.974, mean=0.975, max=0.975, sum=5.848 (6)",
            "tab": "Summarization metrics",
            "score": 0.9746996636764317
          },
          "CNN/DailyMail - Density": {
            "description": "min=28.678, mean=31.707, max=36.132, sum=190.245 (6)",
            "tab": "Summarization metrics",
            "score": 31.707488870766706
          },
          "CNN/DailyMail - Compression": {
            "description": "min=9.108, mean=9.688, max=10.161, sum=58.13 (6)",
            "tab": "Summarization metrics",
            "score": 9.688415513712991
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.122,
        "details": {
          "description": "min=0.122, mean=0.122, max=0.122, sum=0.73 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.996, mean=4.997, max=5, sum=29.985 (6)",
            "tab": "General information",
            "score": 4.997425997425997
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1484.608, mean=1537.293, max=1572.616, sum=9223.757 (6)",
            "tab": "General information",
            "score": 1537.2927927927929
          },
          "XSUM - # output tokens": {
            "description": "min=22.674, mean=23.421, max=24.095, sum=140.529 (6)",
            "tab": "General information",
            "score": 23.421492921492924
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.445, mean=0.454, max=0.467, sum=2.725 (6)",
            "tab": "Bias",
            "score": 0.45422077922077925
          },
          "XSUM - Representation (race)": {
            "description": "min=0.483, mean=0.505, max=0.524, sum=3.031 (6)",
            "tab": "Bias",
            "score": 0.5051915503043323
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.198, mean=0.215, max=0.235, sum=1.29 (6)",
            "tab": "Bias",
            "score": 0.2150586429483566
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "XSUM - SummaC": {
            "description": "min=-0.244, mean=-0.239, max=-0.235, sum=-0.716 (3)",
            "tab": "Summarization metrics",
            "score": -0.23871033593647883
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.417, mean=0.418, max=0.42, sum=1.254 (3)",
            "tab": "Summarization metrics",
            "score": 0.4181413420706151
          },
          "XSUM - Coverage": {
            "description": "min=0.823, mean=0.824, max=0.826, sum=4.943 (6)",
            "tab": "Summarization metrics",
            "score": 0.8238944118657666
          },
          "XSUM - Density": {
            "description": "min=2.687, mean=2.793, max=2.942, sum=16.758 (6)",
            "tab": "Summarization metrics",
            "score": 2.7930375453507623
          },
          "XSUM - Compression": {
            "description": "min=17.475, mean=18.017, max=18.57, sum=108.1 (6)",
            "tab": "Summarization metrics",
            "score": 18.016669951894464
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.961,
        "details": {
          "description": "min=0.959, mean=0.961, max=0.962, sum=2.882 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.011, mean=0.014, max=0.019, sum=0.043 (3)",
            "tab": "Calibration",
            "score": 0.014204038428277976
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.917, mean=0.921, max=0.925, sum=2.762 (3)",
            "tab": "Robustness",
            "score": 0.9206666666666669
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.946, mean=0.95, max=0.954, sum=2.851 (3)",
            "tab": "Fairness",
            "score": 0.9503333333333334
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.89, mean=4.217, max=4.981, sum=12.652 (3)",
            "tab": "General information",
            "score": 4.217333333333333
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1282.318, mean=1557.741, max=1776.111, sum=4673.222 (3)",
            "tab": "General information",
            "score": 1557.7406666666666
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.54,
        "details": {
          "description": "min=0.009, mean=0.54, max=1, sum=29.17 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.113, mean=0.358, max=0.735, sum=19.322 (54)",
            "tab": "Calibration",
            "score": 0.3578234752080933
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.468, max=1, sum=25.26 (54)",
            "tab": "Robustness",
            "score": 0.46778473308233626
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.002, mean=0.496, max=1, sum=26.757 (54)",
            "tab": "Fairness",
            "score": 0.4955072296924251
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=362.293, mean=732.514, max=1288.441, sum=39555.782 (54)",
            "tab": "General information",
            "score": 732.5144825548033
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.634,
        "details": {
          "description": "min=0.05, mean=0.634, max=0.975, sum=20.925 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.05, mean=0.274, max=0.84, sum=9.055 (33)",
            "tab": "Calibration",
            "score": 0.2744070774220778
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.552, max=0.975, sum=18.225 (33)",
            "tab": "Robustness",
            "score": 0.5522727272727274
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.05, mean=0.609, max=0.975, sum=20.1 (33)",
            "tab": "Fairness",
            "score": 0.609090909090909
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.554, max=5, sum=150.275 (33)",
            "tab": "General information",
            "score": 4.553787878787879
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=270.325, mean=813.265, max=1762.475, sum=26837.75 (33)",
            "tab": "General information",
            "score": 813.2651515151515
          },
          "RAFT - # output tokens": {
            "description": "min=0.2, mean=3.148, max=6.3, sum=103.875 (33)",
            "tab": "General information",
            "score": 3.1477272727272725
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}