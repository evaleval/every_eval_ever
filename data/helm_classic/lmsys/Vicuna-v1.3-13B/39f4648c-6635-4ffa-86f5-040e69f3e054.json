{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/lmsys_Vicuna-v1.3-13B/1768090731.5328572",
  "retrieved_timestamp": "1768090731.5328572",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Vicuna v1.3 13B",
    "id": "lmsys/Vicuna-v1.3-13B",
    "developer": "lmsys",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.706,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.27488436632747454
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.7320745920745921
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.7154545454545455
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5333173629091996
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.5758158508158508
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.462,
        "details": {
          "description": "min=0.298, mean=0.462, max=0.72, sum=2.308 (5)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.156, mean=0.194, max=0.246, sum=0.972 (5)",
            "tab": "Calibration",
            "score": 0.19445587267296924
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.237, mean=0.413, max=0.69, sum=2.067 (5)",
            "tab": "Robustness",
            "score": 0.4133684210526316
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.228, mean=0.424, max=0.7, sum=2.118 (5)",
            "tab": "Fairness",
            "score": 0.4236140350877193
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=397.65, mean=522.547, max=684.675, sum=2612.735 (5)",
            "tab": "General information",
            "score": 522.5470877192982
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.808,
        "details": {
          "description": "min=0.808, mean=0.808, max=0.808, sum=0.808 (1)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.159, mean=0.159, max=0.159, sum=0.159 (1)",
            "tab": "Calibration",
            "score": 0.15912327464389103
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.757, mean=0.757, max=0.757, sum=0.757 (1)",
            "tab": "Robustness",
            "score": 0.757
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.748, mean=0.748, max=0.748, sum=0.748 (1)",
            "tab": "Fairness",
            "score": 0.748
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=1439.447, mean=1439.447, max=1439.447, sum=1439.447 (1)",
            "tab": "General information",
            "score": 1439.447
          },
          "BoolQ - # output tokens": {
            "description": "min=4.996, mean=4.996, max=4.996, sum=4.996 (1)",
            "tab": "General information",
            "score": 4.996
          },
          "BoolQ - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.691,
        "details": {
          "description": "min=0.691, mean=0.691, max=0.691, sum=0.691 (1)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.257, mean=0.257, max=0.257, sum=0.257 (1)",
            "tab": "Calibration",
            "score": 0.25677737638719905
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.525, mean=0.525, max=0.525, sum=0.525 (1)",
            "tab": "Robustness",
            "score": 0.5253621693457193
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.607, mean=0.607, max=0.607, sum=0.607 (1)",
            "tab": "Fairness",
            "score": 0.6066076692752655
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.437, mean=1.437, max=1.437, sum=1.437 (1)",
            "tab": "General information",
            "score": 1.4366197183098592
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1541.115, mean=1541.115, max=1541.115, sum=1541.115 (1)",
            "tab": "General information",
            "score": 1541.1154929577465
          },
          "NarrativeQA - # output tokens": {
            "description": "min=67.575, mean=67.575, max=67.575, sum=67.575 (1)",
            "tab": "General information",
            "score": 67.57464788732395
          },
          "NarrativeQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.417, mean=0.417, max=0.417, sum=0.417 (1)",
            "tab": "Bias",
            "score": 0.41666666666666663
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.181, mean=0.181, max=0.181, sum=0.181 (1)",
            "tab": "Bias",
            "score": 0.1806282722513089
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.008, mean=0.008, max=0.008, sum=0.008 (1)",
            "tab": "Toxicity",
            "score": 0.008450704225352112
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.686,
        "details": {
          "description": "min=0.686, mean=0.686, max=0.686, sum=0.686 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.202, mean=0.202, max=0.202, sum=0.202 (1)",
            "tab": "Calibration",
            "score": 0.20199999735253094
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.43, mean=0.43, max=0.43, sum=0.43 (1)",
            "tab": "Calibration",
            "score": 0.4297157164166979
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.273, mean=0.273, max=0.273, sum=0.273 (1)",
            "tab": "Robustness",
            "score": 0.2732835109469542
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.621, mean=0.621, max=0.621, sum=0.621 (1)",
            "tab": "Robustness",
            "score": 0.6205537766211775
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.266, mean=0.266, max=0.266, sum=0.266 (1)",
            "tab": "Fairness",
            "score": 0.26608326669652704
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.63, mean=0.63, max=0.63, sum=0.63 (1)",
            "tab": "Fairness",
            "score": 0.6295785534387982
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=137.383, mean=137.383, max=137.383, sum=137.383 (1)",
            "tab": "General information",
            "score": 137.383
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=299.508, mean=299.508, max=299.508, sum=299.508 (1)",
            "tab": "General information",
            "score": 299.508
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=3.722, mean=3.722, max=3.722, sum=3.722 (1)",
            "tab": "General information",
            "score": 3.722
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.049, mean=0.049, max=0.049, sum=0.049 (1)",
            "tab": "General information",
            "score": 0.049
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1407.178, mean=1407.178, max=1407.178, sum=1407.178 (1)",
            "tab": "General information",
            "score": 1407.178
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=266.895, mean=266.895, max=266.895, sum=266.895 (1)",
            "tab": "General information",
            "score": 266.895
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.364, mean=0.364, max=0.364, sum=0.364 (1)",
            "tab": "Bias",
            "score": 0.363914373088685
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.132, mean=0.132, max=0.132, sum=0.132 (1)",
            "tab": "Bias",
            "score": 0.13157894736842105
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.484, mean=0.484, max=0.484, sum=0.484 (1)",
            "tab": "Bias",
            "score": 0.4838709677419355
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.293, mean=0.293, max=0.293, sum=0.293 (1)",
            "tab": "Bias",
            "score": 0.29310344827586204
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.001 (1)",
            "tab": "Toxicity",
            "score": 0.001
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.001 (1)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.403,
        "details": {
          "description": "min=0.403, mean=0.403, max=0.403, sum=0.403 (1)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.103, mean=0.103, max=0.103, sum=0.103 (1)",
            "tab": "Calibration",
            "score": 0.10339686685910766
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.247, mean=0.247, max=0.247, sum=0.247 (1)",
            "tab": "Robustness",
            "score": 0.24738453163162216
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.324, mean=0.324, max=0.324, sum=0.324 (1)",
            "tab": "Fairness",
            "score": 0.32414193488324744
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.507, mean=0.507, max=0.507, sum=0.507 (1)",
            "tab": "General information",
            "score": 0.507
          },
          "QuAC - truncated": {
            "description": "min=0.06, mean=0.06, max=0.06, sum=0.06 (1)",
            "tab": "General information",
            "score": 0.06
          },
          "QuAC - # prompt tokens": {
            "description": "min=1498.657, mean=1498.657, max=1498.657, sum=1498.657 (1)",
            "tab": "General information",
            "score": 1498.657
          },
          "QuAC - # output tokens": {
            "description": "min=77.743, mean=77.743, max=77.743, sum=77.743 (1)",
            "tab": "General information",
            "score": 77.743
          },
          "QuAC - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.63, mean=0.63, max=0.63, sum=0.63 (1)",
            "tab": "Bias",
            "score": 0.6296296296296295
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.408, mean=0.408, max=0.408, sum=0.408 (1)",
            "tab": "Bias",
            "score": 0.4083074125172457
          },
          "QuAC - Representation (race)": {
            "description": "min=0.289, mean=0.289, max=0.289, sum=0.289 (1)",
            "tab": "Bias",
            "score": 0.28888888888888886
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.242, mean=0.242, max=0.242, sum=0.242 (1)",
            "tab": "Bias",
            "score": 0.2418952618453865
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.001 (1)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "HellaSwag - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "HellaSwag - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "HellaSwag - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "No matching runs",
            "tab": "Calibration",
            "score": null
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "OpenbookQA - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.385,
        "details": {
          "description": "min=0.385, mean=0.385, max=0.385, sum=0.385 (1)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.316, mean=0.316, max=0.316, sum=0.316 (1)",
            "tab": "Calibration",
            "score": 0.31581376966800645
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.341, mean=0.341, max=0.341, sum=0.341 (1)",
            "tab": "Robustness",
            "score": 0.3409785932721712
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.315, mean=0.315, max=0.315, sum=0.315 (1)",
            "tab": "Fairness",
            "score": 0.3149847094801223
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=654 (1)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=524.602, mean=524.602, max=524.602, sum=524.602 (1)",
            "tab": "General information",
            "score": 524.6024464831804
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "CNN/DailyMail - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "No matching runs",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "XSUM - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "XSUM - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "XSUM - SummaC": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - QAFactEval": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Coverage": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Density": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - Compression": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "No matching runs",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.762,
        "details": {
          "description": "min=0.762, mean=0.762, max=0.762, sum=0.762 (1)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.183, mean=0.183, max=0.183, sum=0.183 (1)",
            "tab": "Calibration",
            "score": 0.18259660460611343
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.674, mean=0.674, max=0.674, sum=0.674 (1)",
            "tab": "Robustness",
            "score": 0.674
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.707, mean=0.707, max=0.707, sum=0.707 (1)",
            "tab": "Fairness",
            "score": 0.707
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.781, mean=2.781, max=2.781, sum=2.781 (1)",
            "tab": "General information",
            "score": 2.781
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1751.213, mean=1751.213, max=1751.213, sum=1751.213 (1)",
            "tab": "General information",
            "score": 1751.213
          },
          "IMDB - # output tokens": {
            "description": "min=3.32, mean=3.32, max=3.32, sum=3.32 (1)",
            "tab": "General information",
            "score": 3.32
          },
          "IMDB - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.645,
        "details": {
          "description": "min=0.247, mean=0.645, max=0.946, sum=11.602 (18)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.086, mean=0.253, max=0.415, sum=4.559 (18)",
            "tab": "Calibration",
            "score": 0.25325054290553783
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.177, mean=0.593, max=0.932, sum=10.679 (18)",
            "tab": "Robustness",
            "score": 0.5932501359027997
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.139, mean=0.569, max=0.946, sum=10.248 (18)",
            "tab": "Fairness",
            "score": 0.5693148383516141
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=6688 (18)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=90 (18)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (18)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=404.732, mean=855.241, max=1417.567, sum=15394.339 (18)",
            "tab": "General information",
            "score": 855.2410378605821
          },
          "CivilComments - # output tokens": {
            "description": "min=2, mean=2.59, max=4.159, sum=46.618 (18)",
            "tab": "General information",
            "score": 2.589879611958418
          },
          "CivilComments - # trials": {
            "description": "min=1, mean=1, max=1, sum=18 (18)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.657,
        "details": {
          "description": "min=0.175, mean=0.657, max=0.9, sum=7.225 (11)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.212, mean=0.376, max=0.701, sum=4.137 (11)",
            "tab": "Calibration",
            "score": 0.37612291287489436
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0.025, mean=0.591, max=0.875, sum=6.5 (11)",
            "tab": "Robustness",
            "score": 0.5909090909090909
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.125, mean=0.62, max=0.875, sum=6.825 (11)",
            "tab": "Fairness",
            "score": 0.6204545454545454
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=440 (11)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0.45, mean=4.552, max=5, sum=50.075 (11)",
            "tab": "General information",
            "score": 4.552272727272727
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (11)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=303.675, mean=954.111, max=1882.1, sum=10495.225 (11)",
            "tab": "General information",
            "score": 954.1113636363635
          },
          "RAFT - # output tokens": {
            "description": "min=5.3, mean=15.4, max=30, sum=169.4 (11)",
            "tab": "General information",
            "score": 15.399999999999999
          },
          "RAFT - # trials": {
            "description": "min=1, mean=1, max=1, sum=11 (11)",
            "tab": "General information",
            "score": 1.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    }
  ]
}