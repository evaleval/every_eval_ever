{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/Cohere-large-v20220720-(13.1B)/1767656643.700665",
  "retrieved_timestamp": "1767656643.700665",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Cohere large v20220720 (13.1B)",
    "id": "Cohere-large-v20220720-(13.1B)",
    "developer": "unknown",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.372,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.6524936901131783
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.3450884302942145
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.3621096552687209
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.40696820175438597
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5413536579003514
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.48450623450623453
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.5760442773600668
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.324,
        "details": {
          "description": "min=0.19, mean=0.324, max=0.4, sum=4.854 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.075, mean=0.112, max=0.151, sum=1.678 (15)",
            "tab": "Calibration",
            "score": 0.11188578153206447
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.15, mean=0.253, max=0.35, sum=3.799 (15)",
            "tab": "Robustness",
            "score": 0.25327485380116954
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.14, mean=0.281, max=0.38, sum=4.214 (15)",
            "tab": "Fairness",
            "score": 0.2809590643274854
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.292, mean=0.317, max=0.349, sum=4.752 (15)",
            "tab": "Efficiency",
            "score": 0.3167793253495066
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=372.75, mean=481.26, max=628.421, sum=7218.903 (15)",
            "tab": "General information",
            "score": 481.2602105263158
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.725,
        "details": {
          "description": "min=0.705, mean=0.725, max=0.738, sum=2.176 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.066, mean=0.088, max=0.106, sum=0.265 (3)",
            "tab": "Calibration",
            "score": 0.08825401206422555
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.514, mean=0.545, max=0.566, sum=1.635 (3)",
            "tab": "Robustness",
            "score": 0.545
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.653, mean=0.676, max=0.695, sum=2.027 (3)",
            "tab": "Fairness",
            "score": 0.6756666666666667
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.359, mean=0.421, max=0.505, sum=1.263 (3)",
            "tab": "Efficiency",
            "score": 0.4208381308593749
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=669.307, mean=925.307, max=1269.307, sum=2775.921 (3)",
            "tab": "General information",
            "score": 925.3070000000001
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.625,
        "details": {
          "description": "min=0.581, mean=0.625, max=0.647, sum=1.874 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.032, mean=0.037, max=0.044, sum=0.11 (3)",
            "tab": "Calibration",
            "score": 0.03650754887085305
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.318, mean=0.357, max=0.38, sum=1.072 (3)",
            "tab": "Robustness",
            "score": 0.3573511654752053
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.466, mean=0.512, max=0.538, sum=1.537 (3)",
            "tab": "Fairness",
            "score": 0.5123186802559418
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=0.693, mean=0.729, max=0.782, sum=2.186 (3)",
            "tab": "Efficiency",
            "score": 0.7286962533010564
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0.958, mean=1.562, max=1.997, sum=4.687 (3)",
            "tab": "General information",
            "score": 1.5624413145539906
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1601.997, mean=1634.99, max=1693.155, sum=4904.969 (3)",
            "tab": "General information",
            "score": 1634.9896713615024
          },
          "NarrativeQA - # output tokens": {
            "description": "min=5.535, mean=6.91, max=9.504, sum=20.73 (3)",
            "tab": "General information",
            "score": 6.909859154929578
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.418, mean=0.473, max=0.5, sum=1.418 (3)",
            "tab": "Bias",
            "score": 0.4726495726495727
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.193, mean=0.202, max=0.211, sum=0.607 (3)",
            "tab": "Bias",
            "score": 0.20233455199447267
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.014, mean=0.017, max=0.02, sum=0.051 (3)",
            "tab": "Toxicity",
            "score": 0.016901408450704227
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.573,
        "details": {
          "description": "min=0.553, mean=0.573, max=0.584, sum=1.72 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.02, mean=0.025, max=0.032, sum=0.074 (3)",
            "tab": "Calibration",
            "score": 0.024639111727299556
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.117, mean=0.143, max=0.158, sum=0.43 (3)",
            "tab": "Calibration",
            "score": 0.14321248401208217
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.16, mean=0.172, max=0.18, sum=0.515 (3)",
            "tab": "Robustness",
            "score": 0.17161461010403287
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.287, mean=0.347, max=0.38, sum=1.041 (3)",
            "tab": "Robustness",
            "score": 0.3470084296370371
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.176, mean=0.178, max=0.181, sum=0.535 (3)",
            "tab": "Fairness",
            "score": 0.17833773739586523
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.489, mean=0.507, max=0.516, sum=1.52 (3)",
            "tab": "Fairness",
            "score": 0.5065982888177307
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=0.332, mean=0.337, max=0.343, sum=1.012 (3)",
            "tab": "Efficiency",
            "score": 0.33722079557291607
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=0.681, mean=0.774, max=0.827, sum=2.321 (3)",
            "tab": "Efficiency",
            "score": 0.7738100833333333
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.191, mean=111.191, max=115.191, sum=333.573 (3)",
            "tab": "General information",
            "score": 111.19099999999999
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=5.441, mean=5.625, max=5.917, sum=16.875 (3)",
            "tab": "General information",
            "score": 5.625
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.538, mean=4.633, max=4.715, sum=13.899 (3)",
            "tab": "General information",
            "score": 4.633
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.039, mean=0.039, max=0.039, sum=0.117 (3)",
            "tab": "General information",
            "score": 0.039
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1261.72, mean=1481.344, max=1608.455, sum=4444.032 (3)",
            "tab": "General information",
            "score": 1481.344
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=8.71, mean=10.443, max=11.438, sum=31.329 (3)",
            "tab": "General information",
            "score": 10.443
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.25, mean=0.333, max=0.5, sum=1 (3)",
            "tab": "Bias",
            "score": 0.3333333333333333
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.244, mean=0.34, max=0.429, sum=1.021 (3)",
            "tab": "Bias",
            "score": 0.34034751045060324
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.208, mean=0.233, max=0.269, sum=0.7 (3)",
            "tab": "Bias",
            "score": 0.23326210826210825
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.37, mean=0.39, max=0.4, sum=1.17 (3)",
            "tab": "Bias",
            "score": 0.38999999999999996
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.447, mean=0.457, max=0.467, sum=1.371 (3)",
            "tab": "Bias",
            "score": 0.45706182643221777
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.125, mean=0.174, max=0.251, sum=0.523 (3)",
            "tab": "Bias",
            "score": 0.17447005829358772
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0.0, max=0.001, sum=0.001 (3)",
            "tab": "Toxicity",
            "score": 0.0003333333333333333
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.002, max=0.003, sum=0.005 (3)",
            "tab": "Toxicity",
            "score": 0.0016666666666666668
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.338,
        "details": {
          "description": "min=0.335, mean=0.338, max=0.343, sum=1.015 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.03, mean=0.033, max=0.036, sum=0.099 (3)",
            "tab": "Calibration",
            "score": 0.03288362014267938
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.197, mean=0.204, max=0.211, sum=0.613 (3)",
            "tab": "Robustness",
            "score": 0.20424911828028136
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.251, mean=0.256, max=0.259, sum=0.768 (3)",
            "tab": "Fairness",
            "score": 0.25613799535824233
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=1.189, mean=1.262, max=1.309, sum=3.785 (3)",
            "tab": "Efficiency",
            "score": 1.261730263346353
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.797, mean=0.881, max=0.969, sum=2.644 (3)",
            "tab": "General information",
            "score": 0.8813333333333334
          },
          "QuAC - truncated": {
            "description": "min=0.02, mean=0.02, max=0.02, sum=0.06 (3)",
            "tab": "General information",
            "score": 0.02
          },
          "QuAC - # prompt tokens": {
            "description": "min=1600.292, mean=1639.784, max=1661.675, sum=4919.353 (3)",
            "tab": "General information",
            "score": 1639.784333333333
          },
          "QuAC - # output tokens": {
            "description": "min=26.693, mean=30.036, max=32.515, sum=90.109 (3)",
            "tab": "General information",
            "score": 30.036333333333335
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.43, mean=0.441, max=0.46, sum=1.322 (3)",
            "tab": "Bias",
            "score": 0.4407422751666938
          },
          "QuAC - Representation (race)": {
            "description": "min=0.306, mean=0.338, max=0.358, sum=1.015 (3)",
            "tab": "Bias",
            "score": 0.3382593663469334
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.234, mean=0.238, max=0.243, sum=0.714 (3)",
            "tab": "Bias",
            "score": 0.23804653081585347
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.003, mean=0.003, max=0.004, sum=0.01 (3)",
            "tab": "Toxicity",
            "score": 0.0033333333333333335
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.736,
        "details": {
          "description": "min=0.736, mean=0.736, max=0.736, sum=0.736 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.288, mean=0.288, max=0.288, sum=0.288 (1)",
            "tab": "Calibration",
            "score": 0.28820318504565584
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.687, mean=0.687, max=0.687, sum=0.687 (1)",
            "tab": "Robustness",
            "score": 0.687
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.575, mean=0.575, max=0.575, sum=0.575 (1)",
            "tab": "Fairness",
            "score": 0.575
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "min=0.225, mean=0.225, max=0.225, sum=0.225 (1)",
            "tab": "Efficiency",
            "score": 0.22464337890624972
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.855, mean=88.855, max=88.855, sum=88.855 (1)",
            "tab": "General information",
            "score": 88.855
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.542,
        "details": {
          "description": "min=0.542, mean=0.542, max=0.542, sum=0.542 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.225, mean=0.225, max=0.225, sum=0.225 (1)",
            "tab": "Calibration",
            "score": 0.2254334966206393
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.43, mean=0.43, max=0.43, sum=0.43 (1)",
            "tab": "Robustness",
            "score": 0.43
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.446, mean=0.446, max=0.446, sum=0.446 (1)",
            "tab": "Fairness",
            "score": 0.446
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "min=0.201, mean=0.201, max=0.201, sum=0.201 (1)",
            "tab": "Efficiency",
            "score": 0.2014860078125007
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.358, mean=5.358, max=5.358, sum=5.358 (1)",
            "tab": "General information",
            "score": 5.358
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.181,
        "details": {
          "description": "min=0.161, mean=0.181, max=0.2, sum=0.544 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.097, mean=0.105, max=0.117, sum=0.316 (3)",
            "tab": "Calibration",
            "score": 0.10528939288118344
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.141, mean=0.154, max=0.173, sum=0.462 (3)",
            "tab": "Robustness",
            "score": 0.15392456676860344
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.142, mean=0.157, max=0.174, sum=0.471 (3)",
            "tab": "Fairness",
            "score": 0.15698267074413863
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.323, mean=0.325, max=0.328, sum=0.975 (3)",
            "tab": "Efficiency",
            "score": 0.3248777191442089
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.315, mean=514.648, max=532.315, sum=1543.945 (3)",
            "tab": "General information",
            "score": 514.6483180428135
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.33,
        "details": {
          "description": "min=0.292, mean=0.33, max=0.382, sum=0.991 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.109, mean=0.13, max=0.147, sum=0.39 (3)",
            "tab": "Robustness",
            "score": 0.1300338624338624
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.195, mean=0.257, max=0.323, sum=0.772 (3)",
            "tab": "Robustness",
            "score": 0.2574506868270638
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.136, mean=0.164, max=0.189, sum=0.493 (3)",
            "tab": "Fairness",
            "score": 0.16423492063492048
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.273, mean=0.312, max=0.361, sum=0.936 (3)",
            "tab": "Fairness",
            "score": 0.3120660241438415
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "min=0.322, mean=0.33, max=0.339, sum=0.989 (3)",
            "tab": "Efficiency",
            "score": 0.3298234970703125
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "min=0.319, mean=0.327, max=0.335, sum=0.98 (3)",
            "tab": "Efficiency",
            "score": 0.32664419815891477
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=497.281, mean=536.614, max=583.281, sum=1609.843 (3)",
            "tab": "General information",
            "score": 536.6143333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=1.008, mean=1.025, max=1.046, sum=3.074 (3)",
            "tab": "General information",
            "score": 1.0246666666666666
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=480.163, mean=519.496, max=566.163, sum=1558.488 (3)",
            "tab": "General information",
            "score": 519.4961240310078
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=1.023, mean=1.031, max=1.047, sum=3.093 (3)",
            "tab": "General information",
            "score": 1.0310077519379846
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.126,
        "details": {
          "description": "min=0.115, mean=0.126, max=0.134, sum=0.758 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=2.097, mean=2.269, max=2.366, sum=13.614 (6)",
            "tab": "Efficiency",
            "score": 2.2689930690607114
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1555.036, mean=1575.036, max=1602.036, sum=9450.219 (6)",
            "tab": "General information",
            "score": 1575.0364806866953
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=67.079, mean=74.505, max=78.916, sum=447.03 (6)",
            "tab": "General information",
            "score": 74.50500715307582
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.58, mean=0.626, max=0.659, sum=3.756 (6)",
            "tab": "Bias",
            "score": 0.6260369618341756
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.371, mean=0.401, max=0.431, sum=2.409 (6)",
            "tab": "Bias",
            "score": 0.40149048314255253
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.185, mean=0.238, max=0.295, sum=1.431 (6)",
            "tab": "Bias",
            "score": 0.23843844144516976
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.115, mean=0.134, max=0.153, sum=0.805 (6)",
            "tab": "Bias",
            "score": 0.1341289455316015
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.004 (6)",
            "tab": "Toxicity",
            "score": 0.000715307582260372
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.447, mean=0.5, max=0.543, sum=1.499 (3)",
            "tab": "Summarization metrics",
            "score": 0.4997740334832678
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "min=4.715, mean=4.763, max=4.822, sum=28.58 (6)",
            "tab": "Summarization metrics",
            "score": 4.763415476947068
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.227, mean=0.246, max=0.263, sum=0.737 (3)",
            "tab": "Summarization metrics",
            "score": 0.2457600895432969
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.903, mean=0.946, max=0.975, sum=5.678 (6)",
            "tab": "Summarization metrics",
            "score": 0.9463649022058865
          },
          "CNN/DailyMail - Density": {
            "description": "min=30.364, mean=37.733, max=45.984, sum=226.401 (6)",
            "tab": "Summarization metrics",
            "score": 37.73347863579329
          },
          "CNN/DailyMail - Compression": {
            "description": "min=9.977, mean=11.27, max=13.424, sum=67.62 (6)",
            "tab": "Summarization metrics",
            "score": 11.269948645908789
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.108,
        "details": {
          "description": "min=0.106, mean=0.108, max=0.11, sum=0.649 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=1.064, mean=1.075, max=1.089, sum=6.451 (6)",
            "tab": "Efficiency",
            "score": 1.0751711510617759
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.996, mean=4.998, max=5, sum=29.988 (6)",
            "tab": "General information",
            "score": 4.998069498069498
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1484.608, mean=1537.452, max=1572.616, sum=9224.71 (6)",
            "tab": "General information",
            "score": 1537.4517374517375
          },
          "XSUM - # output tokens": {
            "description": "min=22.133, mean=22.992, max=23.423, sum=137.954 (6)",
            "tab": "General information",
            "score": 22.99227799227799
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.456, mean=0.466, max=0.484, sum=2.793 (6)",
            "tab": "Bias",
            "score": 0.4655148596176822
          },
          "XSUM - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.139, mean=0.157, max=0.172, sum=0.945 (6)",
            "tab": "Bias",
            "score": 0.15743560442588508
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.008 (6)",
            "tab": "Toxicity",
            "score": 0.001287001287001287
          },
          "XSUM - SummaC": {
            "description": "min=-0.196, mean=-0.189, max=-0.185, sum=-0.567 (3)",
            "tab": "Summarization metrics",
            "score": -0.18902428828304493
          },
          "XSUM - QAFactEval": {
            "description": "min=2.852, mean=2.889, max=2.928, sum=17.336 (6)",
            "tab": "Summarization metrics",
            "score": 2.889265592037019
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.394, mean=0.398, max=0.403, sum=1.195 (3)",
            "tab": "Summarization metrics",
            "score": 0.3984961779205311
          },
          "XSUM - Coverage": {
            "description": "min=0.82, mean=0.823, max=0.825, sum=4.937 (6)",
            "tab": "Summarization metrics",
            "score": 0.8227568594164721
          },
          "XSUM - Density": {
            "description": "min=3.497, mean=3.599, max=3.746, sum=21.593 (6)",
            "tab": "Summarization metrics",
            "score": 3.5988000456323377
          },
          "XSUM - Compression": {
            "description": "min=20.099, mean=20.712, max=21.78, sum=124.27 (6)",
            "tab": "Summarization metrics",
            "score": 20.711693139962097
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.933,
        "details": {
          "description": "min=0.929, mean=0.933, max=0.94, sum=2.8 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.098, mean=0.132, max=0.183, sum=0.396 (3)",
            "tab": "Calibration",
            "score": 0.13199349625828075
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.895, mean=0.902, max=0.91, sum=2.706 (3)",
            "tab": "Robustness",
            "score": 0.902
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.912, mean=0.92, max=0.93, sum=2.759 (3)",
            "tab": "Fairness",
            "score": 0.9196666666666666
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=0.479, mean=0.536, max=0.62, sum=1.607 (3)",
            "tab": "Efficiency",
            "score": 0.5358171357421871
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.846, mean=4.93, max=4.98, sum=14.79 (3)",
            "tab": "General information",
            "score": 4.930000000000001
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1161.854, mean=1398.654, max=1747.025, sum=4195.961 (3)",
            "tab": "General information",
            "score": 1398.6536666666668
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.507,
        "details": {
          "description": "min=0, mean=0.507, max=1, sum=27.395 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.1, mean=0.384, max=0.705, sum=20.717 (54)",
            "tab": "Calibration",
            "score": 0.38365386942886265
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.333, max=0.95, sum=17.981 (54)",
            "tab": "Robustness",
            "score": 0.3329825600043121
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.443, max=1, sum=23.917 (54)",
            "tab": "Fairness",
            "score": 0.44290609222735455
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.29, mean=0.375, max=0.51, sum=20.235 (54)",
            "tab": "Efficiency",
            "score": 0.3747284900914756
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=362.293, mean=732.514, max=1288.441, sum=39555.782 (54)",
            "tab": "General information",
            "score": 732.5144825548033
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.596,
        "details": {
          "description": "min=0, mean=0.596, max=0.975, sum=19.675 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.115, mean=0.267, max=1, sum=8.804 (33)",
            "tab": "Calibration",
            "score": 0.26679166027291745
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.49, max=0.975, sum=16.175 (33)",
            "tab": "Robustness",
            "score": 0.49015151515151517
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0, mean=0.564, max=0.975, sum=18.625 (33)",
            "tab": "Fairness",
            "score": 0.5643939393939394
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.284, mean=0.444, max=0.697, sum=14.664 (33)",
            "tab": "Efficiency",
            "score": 0.4443553984670929
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.557, max=5, sum=150.375 (33)",
            "tab": "General information",
            "score": 4.556818181818182
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=270.325, mean=814.446, max=1777.025, sum=26876.725 (33)",
            "tab": "General information",
            "score": 814.446212121212
          },
          "RAFT - # output tokens": {
            "description": "min=0, mean=3.02, max=6.5, sum=99.65 (33)",
            "tab": "General information",
            "score": 3.01969696969697
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}