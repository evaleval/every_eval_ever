{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/cohere_Cohere-medium-v20221108-6.1B/1770834891.1472661",
  "retrieved_timestamp": "1770834891.1472661",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Cohere medium v20221108 6.1B",
    "id": "cohere/Cohere-medium-v20221108-6.1B",
    "developer": "cohere",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.312,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.6010395609917657
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.26965587249235745
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.339964744191663
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5558769690348637
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.6328714495381162
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.506578947368421
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.254,
        "details": {
          "description": "min=0.18, mean=0.254, max=0.32, sum=3.806 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.055, mean=0.113, max=0.167, sum=1.691 (15)",
            "tab": "Calibration",
            "score": 0.11272299343238619
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.15, mean=0.207, max=0.25, sum=3.1 (15)",
            "tab": "Robustness",
            "score": 0.20667836257309943
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.14, mean=0.22, max=0.3, sum=3.299 (15)",
            "tab": "Fairness",
            "score": 0.21994152046783624
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=372.75, mean=481.26, max=628.421, sum=7218.903 (15)",
            "tab": "General information",
            "score": 481.2602105263158
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on BoolQ",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.7,
        "details": {
          "description": "min=0.693, mean=0.7, max=0.704, sum=2.1 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.088, mean=0.095, max=0.105, sum=0.284 (3)",
            "tab": "Calibration",
            "score": 0.09459272512018041
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.508, mean=0.54, max=0.568, sum=1.62 (3)",
            "tab": "Robustness",
            "score": 0.54
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.626, mean=0.642, max=0.652, sum=1.925 (3)",
            "tab": "Fairness",
            "score": 0.6416666666666667
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=669.307, mean=925.307, max=1269.307, sum=2775.921 (3)",
            "tab": "General information",
            "score": 925.3070000000001
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.61,
        "details": {
          "description": "min=0.57, mean=0.61, max=0.642, sum=1.831 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.027, mean=0.028, max=0.03, sum=0.085 (3)",
            "tab": "Calibration",
            "score": 0.02834267942109429
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.265, mean=0.296, max=0.321, sum=0.888 (3)",
            "tab": "Robustness",
            "score": 0.2960125312478054
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.441, mean=0.497, max=0.537, sum=1.491 (3)",
            "tab": "Fairness",
            "score": 0.49703931741598933
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0.958, mean=1.562, max=1.997, sum=4.687 (3)",
            "tab": "General information",
            "score": 1.5624413145539906
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1601.997, mean=1634.99, max=1693.155, sum=4904.969 (3)",
            "tab": "General information",
            "score": 1634.9896713615024
          },
          "NarrativeQA - # output tokens": {
            "description": "min=5.544, mean=7.144, max=9.065, sum=21.431 (3)",
            "tab": "General information",
            "score": 7.143661971830986
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.417, mean=0.441, max=0.469, sum=1.323 (3)",
            "tab": "Bias",
            "score": 0.44097222222222215
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.15, mean=0.181, max=0.213, sum=0.543 (3)",
            "tab": "Bias",
            "score": 0.18104985015382555
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.008, mean=0.011, max=0.014, sum=0.034 (3)",
            "tab": "Toxicity",
            "score": 0.011267605633802818
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (open-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.517,
        "details": {
          "description": "min=0.506, mean=0.517, max=0.536, sum=1.551 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.006, mean=0.015, max=0.02, sum=0.044 (3)",
            "tab": "Calibration",
            "score": 0.01475928497137971
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.181, mean=0.233, max=0.27, sum=0.698 (3)",
            "tab": "Calibration",
            "score": 0.2327617365925914
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.099, mean=0.105, max=0.11, sum=0.314 (3)",
            "tab": "Robustness",
            "score": 0.10457862657700777
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.164, mean=0.222, max=0.282, sum=0.665 (3)",
            "tab": "Robustness",
            "score": 0.22177043436006846
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.142, mean=0.149, max=0.157, sum=0.447 (3)",
            "tab": "Fairness",
            "score": 0.14913779301489424
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.431, mean=0.45, max=0.473, sum=1.349 (3)",
            "tab": "Fairness",
            "score": 0.44971949324423194
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.191, mean=111.191, max=115.191, sum=333.573 (3)",
            "tab": "General information",
            "score": 111.19099999999999
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=6.631, mean=6.745, max=6.831, sum=20.236 (3)",
            "tab": "General information",
            "score": 6.745333333333334
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.538, mean=4.633, max=4.715, sum=13.899 (3)",
            "tab": "General information",
            "score": 4.633
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.039, mean=0.039, max=0.039, sum=0.117 (3)",
            "tab": "General information",
            "score": 0.039
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1261.72, mean=1481.344, max=1608.455, sum=4444.032 (3)",
            "tab": "General information",
            "score": 1481.344
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=7.485, mean=8.419, max=9.746, sum=25.256 (3)",
            "tab": "General information",
            "score": 8.418666666666667
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.357, mean=0.45, max=0.5, sum=1.349 (3)",
            "tab": "Bias",
            "score": 0.44969278033794163
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.382, mean=0.451, max=0.504, sum=1.353 (3)",
            "tab": "Bias",
            "score": 0.4511619362542481
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.173, mean=0.314, max=0.386, sum=0.942 (3)",
            "tab": "Bias",
            "score": 0.3140619884317363
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.233, mean=0.308, max=0.35, sum=0.923 (3)",
            "tab": "Bias",
            "score": 0.30777777777777776
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.421, mean=0.452, max=0.476, sum=1.356 (3)",
            "tab": "Bias",
            "score": 0.4519283176992704
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.056, mean=0.061, max=0.069, sum=0.184 (3)",
            "tab": "Bias",
            "score": 0.06120328473269649
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0.002, max=0.003, sum=0.005 (3)",
            "tab": "Toxicity",
            "score": 0.0016666666666666668
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on QuAC",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.314,
        "details": {
          "description": "min=0.297, mean=0.314, max=0.328, sum=0.942 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.029, mean=0.041, max=0.062, sum=0.124 (3)",
            "tab": "Calibration",
            "score": 0.04129669890931466
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.127, mean=0.152, max=0.171, sum=0.456 (3)",
            "tab": "Robustness",
            "score": 0.15189850694469184
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.208, mean=0.229, max=0.244, sum=0.688 (3)",
            "tab": "Fairness",
            "score": 0.22939607207059778
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.797, mean=0.881, max=0.969, sum=2.644 (3)",
            "tab": "General information",
            "score": 0.8813333333333334
          },
          "QuAC - truncated": {
            "description": "min=0.02, mean=0.02, max=0.02, sum=0.06 (3)",
            "tab": "General information",
            "score": 0.02
          },
          "QuAC - # prompt tokens": {
            "description": "min=1600.292, mean=1639.784, max=1661.675, sum=4919.353 (3)",
            "tab": "General information",
            "score": 1639.784333333333
          },
          "QuAC - # output tokens": {
            "description": "min=18.756, mean=22.84, max=26.573, sum=68.519 (3)",
            "tab": "General information",
            "score": 22.83966666666667
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.619, mean=0.651, max=0.667, sum=1.952 (3)",
            "tab": "Bias",
            "score": 0.6507936507936508
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.436, mean=0.441, max=0.444, sum=1.322 (3)",
            "tab": "Bias",
            "score": 0.4407764298624513
          },
          "QuAC - Representation (race)": {
            "description": "min=0.345, mean=0.353, max=0.359, sum=1.06 (3)",
            "tab": "Bias",
            "score": 0.35330965547213355
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.248, mean=0.251, max=0.255, sum=0.753 (3)",
            "tab": "Bias",
            "score": 0.2510004319407244
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.002, mean=0.002, max=0.002, sum=0.006 (3)",
            "tab": "Toxicity",
            "score": 0.002
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on HellaSwag",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.726,
        "details": {
          "description": "min=0.726, mean=0.726, max=0.726, sum=0.726 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.281, mean=0.281, max=0.281, sum=0.281 (1)",
            "tab": "Calibration",
            "score": 0.2814688190554964
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.687, mean=0.687, max=0.687, sum=0.687 (1)",
            "tab": "Robustness",
            "score": 0.687
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.567, mean=0.567, max=0.567, sum=0.567 (1)",
            "tab": "Fairness",
            "score": 0.567
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.855, mean=88.855, max=88.855, sum=88.855 (1)",
            "tab": "General information",
            "score": 88.855
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.538,
        "details": {
          "description": "min=0.538, mean=0.538, max=0.538, sum=0.538 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.23, mean=0.23, max=0.23, sum=0.23 (1)",
            "tab": "Calibration",
            "score": 0.2303402231123461
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.414, mean=0.414, max=0.414, sum=0.414 (1)",
            "tab": "Robustness",
            "score": 0.414
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.44, mean=0.44, max=0.44, sum=0.44 (1)",
            "tab": "Fairness",
            "score": 0.44
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.358, mean=5.358, max=5.358, sum=5.358 (1)",
            "tab": "General information",
            "score": 5.358
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on TruthfulQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.215,
        "details": {
          "description": "min=0.19, mean=0.215, max=0.237, sum=0.645 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.057, mean=0.08, max=0.106, sum=0.24 (3)",
            "tab": "Calibration",
            "score": 0.07993899696218487
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.156, mean=0.17, max=0.19, sum=0.511 (3)",
            "tab": "Robustness",
            "score": 0.17023445463812437
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.156, mean=0.182, max=0.205, sum=0.546 (3)",
            "tab": "Fairness",
            "score": 0.18195718654434248
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.315, mean=514.648, max=532.315, sum=1543.945 (3)",
            "tab": "General information",
            "score": 514.6483180428135
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC)",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "NDCG@10 on MS MARCO (TREC)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.373,
        "details": {
          "description": "min=0.329, mean=0.373, max=0.4, sum=1.118 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.11, mean=0.13, max=0.144, sum=0.389 (3)",
            "tab": "Robustness",
            "score": 0.12963544973544971
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.265, mean=0.314, max=0.339, sum=0.942 (3)",
            "tab": "Robustness",
            "score": 0.3140445596258007
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.123, mean=0.145, max=0.162, sum=0.436 (3)",
            "tab": "Fairness",
            "score": 0.1454550264550264
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.311, mean=0.353, max=0.384, sum=1.058 (3)",
            "tab": "Fairness",
            "score": 0.35251421077315565
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=497.281, mean=536.614, max=583.281, sum=1609.843 (3)",
            "tab": "General information",
            "score": 536.6143333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=1, mean=1.005, max=1.008, sum=3.015 (3)",
            "tab": "General information",
            "score": 1.005
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=480.163, mean=519.496, max=566.163, sum=1558.488 (3)",
            "tab": "General information",
            "score": 519.4961240310078
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on CNN/DailyMail",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.121,
        "details": {
          "description": "min=0.116, mean=0.121, max=0.13, sum=0.728 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1555.036, mean=1575.036, max=1602.036, sum=9450.219 (6)",
            "tab": "General information",
            "score": 1575.0364806866953
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=60.474, mean=68.601, max=77.918, sum=411.605 (6)",
            "tab": "General information",
            "score": 68.60085836909872
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.604, mean=0.612, max=0.618, sum=3.671 (6)",
            "tab": "Bias",
            "score": 0.6118203882651768
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.401, mean=0.408, max=0.419, sum=2.449 (6)",
            "tab": "Bias",
            "score": 0.408087030039703
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.257, mean=0.287, max=0.318, sum=1.72 (6)",
            "tab": "Bias",
            "score": 0.2867291116025263
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.117, mean=0.141, max=0.159, sum=0.844 (6)",
            "tab": "Bias",
            "score": 0.14067727789435583
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.004 (6)",
            "tab": "Toxicity",
            "score": 0.000715307582260372
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.231, mean=0.359, max=0.443, sum=1.077 (3)",
            "tab": "Summarization metrics",
            "score": 0.35895859214347764
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.195, mean=0.218, max=0.246, sum=0.654 (3)",
            "tab": "Summarization metrics",
            "score": 0.21796490870344257
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.801, mean=0.899, max=0.957, sum=5.391 (6)",
            "tab": "Summarization metrics",
            "score": 0.8985701854042452
          },
          "CNN/DailyMail - Density": {
            "description": "min=16.696, mean=24.344, max=33.085, sum=146.063 (6)",
            "tab": "Summarization metrics",
            "score": 24.343863209587038
          },
          "CNN/DailyMail - Compression": {
            "description": "min=9.239, mean=11.42, max=13.421, sum=68.523 (6)",
            "tab": "Summarization metrics",
            "score": 11.420494637224708
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on XSUM",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.099,
        "details": {
          "description": "min=0.095, mean=0.099, max=0.106, sum=0.596 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.996, mean=4.998, max=5, sum=29.988 (6)",
            "tab": "General information",
            "score": 4.998069498069498
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1484.608, mean=1537.452, max=1572.616, sum=9224.71 (6)",
            "tab": "General information",
            "score": 1537.4517374517375
          },
          "XSUM - # output tokens": {
            "description": "min=23.5, mean=23.626, max=23.749, sum=141.757 (6)",
            "tab": "General information",
            "score": 23.626126126126128
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.424, mean=0.436, max=0.453, sum=2.616 (6)",
            "tab": "Bias",
            "score": 0.43605987410335234
          },
          "XSUM - Representation (race)": {
            "description": "min=0.373, mean=0.393, max=0.404, sum=2.359 (6)",
            "tab": "Bias",
            "score": 0.393188854489164
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.181, mean=0.194, max=0.206, sum=1.165 (6)",
            "tab": "Bias",
            "score": 0.194128141174599
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "XSUM - SummaC": {
            "description": "min=-0.192, mean=-0.171, max=-0.149, sum=-0.513 (3)",
            "tab": "Summarization metrics",
            "score": -0.17113255308913036
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.382, mean=0.384, max=0.388, sum=1.152 (3)",
            "tab": "Summarization metrics",
            "score": 0.38412741233326225
          },
          "XSUM - Coverage": {
            "description": "min=0.842, mean=0.842, max=0.842, sum=5.051 (6)",
            "tab": "Summarization metrics",
            "score": 0.8418943137133965
          },
          "XSUM - Density": {
            "description": "min=3.715, mean=3.815, max=3.914, sum=22.889 (6)",
            "tab": "Summarization metrics",
            "score": 3.8148335440941747
          },
          "XSUM - Compression": {
            "description": "min=19.45, mean=19.703, max=19.907, sum=118.221 (6)",
            "tab": "Summarization metrics",
            "score": 19.7034371773279
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on IMDB",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.935,
        "details": {
          "description": "min=0.917, mean=0.935, max=0.947, sum=2.804 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.335, mean=0.36, max=0.394, sum=1.079 (3)",
            "tab": "Calibration",
            "score": 0.3598306140598746
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.878, mean=0.888, max=0.896, sum=2.665 (3)",
            "tab": "Robustness",
            "score": 0.8883333333333333
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.896, mean=0.917, max=0.936, sum=2.752 (3)",
            "tab": "Fairness",
            "score": 0.9173333333333334
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.903, mean=4.229, max=4.983, sum=12.688 (3)",
            "tab": "General information",
            "score": 4.229333333333333
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1283.038, mean=1562.808, max=1784.2, sum=4688.425 (3)",
            "tab": "General information",
            "score": 1562.8083333333334
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1.003, max=1.01, sum=3.01 (3)",
            "tab": "General information",
            "score": 1.0033333333333332
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on CivilComments",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.5,
        "details": {
          "description": "min=0, mean=0.5, max=1, sum=27.019 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.265, mean=0.487, max=0.736, sum=26.317 (54)",
            "tab": "Calibration",
            "score": 0.4873543575629644
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.353, max=0.931, sum=19.089 (54)",
            "tab": "Robustness",
            "score": 0.35349935695509527
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.493, max=1, sum=26.609 (54)",
            "tab": "Fairness",
            "score": 0.49275536816045606
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=362.293, mean=732.514, max=1288.441, sum=39555.782 (54)",
            "tab": "General information",
            "score": 732.5144825548033
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on RAFT",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.591,
        "details": {
          "description": "min=0.1, mean=0.591, max=0.975, sum=19.5 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.11, mean=0.253, max=0.545, sum=8.337 (33)",
            "tab": "Calibration",
            "score": 0.25263340417043
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0.025, mean=0.502, max=0.975, sum=16.55 (33)",
            "tab": "Robustness",
            "score": 0.5015151515151515
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.1, mean=0.571, max=0.975, sum=18.85 (33)",
            "tab": "Fairness",
            "score": 0.5712121212121212
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.557, max=5, sum=150.375 (33)",
            "tab": "General information",
            "score": 4.556818181818182
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=270.325, mean=814.446, max=1777.025, sum=26876.725 (33)",
            "tab": "General information",
            "score": 814.446212121212
          },
          "RAFT - # output tokens": {
            "description": "min=0.575, mean=3.038, max=6.375, sum=100.25 (33)",
            "tab": "General information",
            "score": 3.0378787878787885
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}