{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/cohere_Cohere-xlarge-v20221108-52.4B/1770834891.1472661",
  "retrieved_timestamp": "1770834891.1472661",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Cohere xlarge v20221108 52.4B",
    "id": "cohere/Cohere-xlarge-v20221108-52.4B",
    "developer": "cohere",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.664,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.5846823928461301
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.5964421748070247
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.6082341462764155
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.601504827172334
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.5642015392015391
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.7039473684210527
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.382,
        "details": {
          "description": "min=0.21, mean=0.382, max=0.67, sum=5.731 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.104, mean=0.143, max=0.197, sum=2.146 (15)",
            "tab": "Calibration",
            "score": 0.14305203655556303
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.12, mean=0.299, max=0.6, sum=4.49 (15)",
            "tab": "Robustness",
            "score": 0.29933333333333334
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.13, mean=0.317, max=0.57, sum=4.748 (15)",
            "tab": "Fairness",
            "score": 0.31652631578947366
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=372.75, mean=481.26, max=628.421, sum=7218.903 (15)",
            "tab": "General information",
            "score": 481.2602105263158
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on BoolQ",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.762,
        "details": {
          "description": "min=0.761, mean=0.762, max=0.763, sum=2.285 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.037, mean=0.051, max=0.062, sum=0.154 (3)",
            "tab": "Calibration",
            "score": 0.05127903463780418
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.712, mean=0.718, max=0.722, sum=2.153 (3)",
            "tab": "Robustness",
            "score": 0.7176666666666667
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.702, mean=0.708, max=0.72, sum=2.124 (3)",
            "tab": "Fairness",
            "score": 0.7079999999999999
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=669.307, mean=925.307, max=1269.307, sum=2775.921 (3)",
            "tab": "General information",
            "score": 925.3070000000001
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.672,
        "details": {
          "description": "min=0.607, mean=0.672, max=0.708, sum=2.017 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.042, mean=0.059, max=0.072, sum=0.178 (3)",
            "tab": "Calibration",
            "score": 0.059183266964369506
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.313, mean=0.39, max=0.434, sum=1.171 (3)",
            "tab": "Robustness",
            "score": 0.3901906178600691
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.486, mean=0.553, max=0.589, sum=1.659 (3)",
            "tab": "Fairness",
            "score": 0.5530542667501213
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0.958, mean=1.562, max=1.997, sum=4.687 (3)",
            "tab": "General information",
            "score": 1.5624413145539906
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1601.997, mean=1634.99, max=1693.155, sum=4904.969 (3)",
            "tab": "General information",
            "score": 1634.9896713615024
          },
          "NarrativeQA - # output tokens": {
            "description": "min=5.792, mean=6.729, max=8.434, sum=20.186 (3)",
            "tab": "General information",
            "score": 6.728638497652582
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.417, mean=0.472, max=0.5, sum=1.417 (3)",
            "tab": "Bias",
            "score": 0.47222222222222227
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.184, mean=0.192, max=0.197, sum=0.575 (3)",
            "tab": "Bias",
            "score": 0.19158509798903886
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.008, mean=0.013, max=0.02, sum=0.039 (3)",
            "tab": "Toxicity",
            "score": 0.013145539906103287
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (open-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.628,
        "details": {
          "description": "min=0.619, mean=0.628, max=0.634, sum=1.885 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.044, mean=0.054, max=0.064, sum=0.163 (3)",
            "tab": "Calibration",
            "score": 0.05430103491623906
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.064, mean=0.073, max=0.08, sum=0.219 (3)",
            "tab": "Calibration",
            "score": 0.07296237131206641
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.276, mean=0.283, max=0.288, sum=0.85 (3)",
            "tab": "Robustness",
            "score": 0.28349840532468856
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.49, mean=0.533, max=0.555, sum=1.598 (3)",
            "tab": "Robustness",
            "score": 0.532530651706331
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.295, mean=0.299, max=0.303, sum=0.898 (3)",
            "tab": "Fairness",
            "score": 0.299210546403295
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.548, mean=0.566, max=0.58, sum=1.699 (3)",
            "tab": "Fairness",
            "score": 0.5664508489119625
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.191, mean=111.191, max=115.191, sum=333.573 (3)",
            "tab": "General information",
            "score": 111.19099999999999
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=4.528, mean=4.808, max=5.211, sum=14.424 (3)",
            "tab": "General information",
            "score": 4.808
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.538, mean=4.633, max=4.715, sum=13.899 (3)",
            "tab": "General information",
            "score": 4.633
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.039, mean=0.039, max=0.039, sum=0.117 (3)",
            "tab": "General information",
            "score": 0.039
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1261.72, mean=1481.344, max=1608.455, sum=4444.032 (3)",
            "tab": "General information",
            "score": 1481.344
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=5.836, mean=6.093, max=6.582, sum=18.278 (3)",
            "tab": "General information",
            "score": 6.092666666666666
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.333, mean=0.444, max=0.5, sum=1.333 (3)",
            "tab": "Bias",
            "score": 0.4444444444444444
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.407, mean=0.48, max=0.556, sum=1.441 (3)",
            "tab": "Bias",
            "score": 0.4804079441760602
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.147, mean=0.247, max=0.385, sum=0.741 (3)",
            "tab": "Bias",
            "score": 0.24693627450980396
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=1.333 (2)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.186, mean=0.232, max=0.278, sum=0.697 (3)",
            "tab": "Bias",
            "score": 0.2324074074074074
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.467, mean=0.474, max=0.483, sum=1.423 (3)",
            "tab": "Bias",
            "score": 0.4744480248239647
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.092, mean=0.113, max=0.135, sum=0.339 (3)",
            "tab": "Bias",
            "score": 0.11298873219533077
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.001, mean=0.001, max=0.001, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on QuAC",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.374,
        "details": {
          "description": "min=0.367, mean=0.374, max=0.378, sum=1.122 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.053, mean=0.063, max=0.072, sum=0.189 (3)",
            "tab": "Calibration",
            "score": 0.06295082132498765
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.221, mean=0.229, max=0.234, sum=0.686 (3)",
            "tab": "Robustness",
            "score": 0.22865454547247813
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.269, mean=0.275, max=0.278, sum=0.824 (3)",
            "tab": "Fairness",
            "score": 0.27469570002834404
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.797, mean=0.881, max=0.969, sum=2.644 (3)",
            "tab": "General information",
            "score": 0.8813333333333334
          },
          "QuAC - truncated": {
            "description": "min=0.02, mean=0.02, max=0.02, sum=0.06 (3)",
            "tab": "General information",
            "score": 0.02
          },
          "QuAC - # prompt tokens": {
            "description": "min=1600.292, mean=1639.784, max=1661.675, sum=4919.353 (3)",
            "tab": "General information",
            "score": 1639.784333333333
          },
          "QuAC - # output tokens": {
            "description": "min=24.612, mean=27.944, max=31.344, sum=83.832 (3)",
            "tab": "General information",
            "score": 27.944
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.543, mean=0.571, max=0.589, sum=1.713 (3)",
            "tab": "Bias",
            "score": 0.570980870980871
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.371, mean=0.395, max=0.426, sum=1.185 (3)",
            "tab": "Bias",
            "score": 0.3948930748680999
          },
          "QuAC - Representation (race)": {
            "description": "min=0.253, mean=0.304, max=0.331, sum=0.912 (3)",
            "tab": "Bias",
            "score": 0.3038684617631986
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.211, mean=0.233, max=0.263, sum=0.699 (3)",
            "tab": "Bias",
            "score": 0.2330910766304025
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.001, mean=0.002, max=0.003, sum=0.007 (3)",
            "tab": "Toxicity",
            "score": 0.0023333333333333335
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on HellaSwag",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.81,
        "details": {
          "description": "min=0.81, mean=0.81, max=0.81, sum=0.81 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
            "tab": "Calibration",
            "score": 0.3332417863062664
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.764, mean=0.764, max=0.764, sum=0.764 (1)",
            "tab": "Robustness",
            "score": 0.764
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.687, mean=0.687, max=0.687, sum=0.687 (1)",
            "tab": "Fairness",
            "score": 0.687
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.855, mean=88.855, max=88.855, sum=88.855 (1)",
            "tab": "General information",
            "score": 88.855
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.588,
        "details": {
          "description": "min=0.588, mean=0.588, max=0.588, sum=0.588 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.207, mean=0.207, max=0.207, sum=0.207 (1)",
            "tab": "Calibration",
            "score": 0.20665896753536225
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.482, mean=0.482, max=0.482, sum=0.482 (1)",
            "tab": "Robustness",
            "score": 0.482
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
            "tab": "Fairness",
            "score": 0.5
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.358, mean=5.358, max=5.358, sum=5.358 (1)",
            "tab": "General information",
            "score": 5.358
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on TruthfulQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.169,
        "details": {
          "description": "min=0.164, mean=0.169, max=0.179, sum=0.508 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.18, mean=0.211, max=0.233, sum=0.633 (3)",
            "tab": "Calibration",
            "score": 0.21105124875435366
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.106, mean=0.116, max=0.13, sum=0.349 (3)",
            "tab": "Robustness",
            "score": 0.1162079510703364
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.112, mean=0.12, max=0.124, sum=0.359 (3)",
            "tab": "Fairness",
            "score": 0.1197757390417941
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.315, mean=514.648, max=532.315, sum=1543.945 (3)",
            "tab": "General information",
            "score": 514.6483180428135
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC)",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "NDCG@10 on MS MARCO (TREC)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.55,
        "details": {
          "description": "min=0.526, mean=0.55, max=0.573, sum=1.65 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.201, mean=0.242, max=0.292, sum=0.725 (3)",
            "tab": "Robustness",
            "score": 0.24177817460317433
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.449, mean=0.482, max=0.527, sum=1.446 (3)",
            "tab": "Robustness",
            "score": 0.48206153384583117
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.239, mean=0.267, max=0.302, sum=0.802 (3)",
            "tab": "Fairness",
            "score": 0.2673071428571425
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.51, mean=0.522, max=0.544, sum=1.565 (3)",
            "tab": "Fairness",
            "score": 0.5216640091882355
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=497.281, mean=536.614, max=583.281, sum=1609.843 (3)",
            "tab": "General information",
            "score": 536.6143333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=1, mean=1.002, max=1.005, sum=3.005 (3)",
            "tab": "General information",
            "score": 1.0016666666666667
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=480.163, mean=519.496, max=566.163, sum=1558.488 (3)",
            "tab": "General information",
            "score": 519.4961240310078
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on CNN/DailyMail",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.153,
        "details": {
          "description": "min=0.153, mean=0.153, max=0.154, sum=0.92 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1555.036, mean=1575.036, max=1602.036, sum=9450.219 (6)",
            "tab": "General information",
            "score": 1575.0364806866953
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=89.47, mean=91.338, max=92.403, sum=548.03 (6)",
            "tab": "General information",
            "score": 91.33834048640915
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.579, mean=0.607, max=0.649, sum=3.642 (6)",
            "tab": "Bias",
            "score": 0.606957921303154
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.362, mean=0.383, max=0.409, sum=2.3 (6)",
            "tab": "Bias",
            "score": 0.3833873353199473
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.223, mean=0.266, max=0.328, sum=1.597 (6)",
            "tab": "Bias",
            "score": 0.26620678930063096
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.128, mean=0.133, max=0.14, sum=0.796 (6)",
            "tab": "Bias",
            "score": 0.1326032519141558
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.009 (6)",
            "tab": "Toxicity",
            "score": 0.001430615164520744
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.469, mean=0.514, max=0.552, sum=1.542 (3)",
            "tab": "Summarization metrics",
            "score": 0.5141110990456594
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.281, mean=0.286, max=0.295, sum=0.858 (3)",
            "tab": "Summarization metrics",
            "score": 0.2858638938260981
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.946, mean=0.971, max=0.984, sum=5.823 (6)",
            "tab": "Summarization metrics",
            "score": 0.9705641483765838
          },
          "CNN/DailyMail - Density": {
            "description": "min=41.158, mean=44.772, max=50.734, sum=268.631 (6)",
            "tab": "Summarization metrics",
            "score": 44.771778103334206
          },
          "CNN/DailyMail - Compression": {
            "description": "min=7.733, mean=8.026, max=8.278, sum=48.156 (6)",
            "tab": "Summarization metrics",
            "score": 8.02592370223569
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "ROUGE-2 on XSUM",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.153,
        "details": {
          "description": "min=0.148, mean=0.153, max=0.158, sum=0.919 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.996, mean=4.998, max=5, sum=29.988 (6)",
            "tab": "General information",
            "score": 4.998069498069498
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1484.608, mean=1537.452, max=1572.616, sum=9224.71 (6)",
            "tab": "General information",
            "score": 1537.4517374517375
          },
          "XSUM - # output tokens": {
            "description": "min=25.925, mean=26.153, max=26.423, sum=156.919 (6)",
            "tab": "General information",
            "score": 26.153153153153156
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.447, mean=0.454, max=0.463, sum=2.724 (6)",
            "tab": "Bias",
            "score": 0.45401696819707577
          },
          "XSUM - Representation (race)": {
            "description": "min=0.515, mean=0.537, max=0.565, sum=3.223 (6)",
            "tab": "Bias",
            "score": 0.5371029656743943
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.204, mean=0.218, max=0.236, sum=1.306 (6)",
            "tab": "Bias",
            "score": 0.2176913745770286
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.004 (6)",
            "tab": "Toxicity",
            "score": 0.0006435006435006435
          },
          "XSUM - SummaC": {
            "description": "min=-0.28, mean=-0.258, max=-0.245, sum=-0.774 (3)",
            "tab": "Summarization metrics",
            "score": -0.25799066096812756
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.447, mean=0.451, max=0.454, sum=1.354 (3)",
            "tab": "Summarization metrics",
            "score": 0.45133514557325344
          },
          "XSUM - Coverage": {
            "description": "min=0.79, mean=0.798, max=0.803, sum=4.787 (6)",
            "tab": "Summarization metrics",
            "score": 0.7978456468638059
          },
          "XSUM - Density": {
            "description": "min=2.823, mean=3.009, max=3.208, sum=18.053 (6)",
            "tab": "Summarization metrics",
            "score": 3.008801536227543
          },
          "XSUM - Compression": {
            "description": "min=17.074, mean=17.188, max=17.359, sum=103.128 (6)",
            "tab": "Summarization metrics",
            "score": 17.187984260626735
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on IMDB",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.956,
        "details": {
          "description": "min=0.941, mean=0.956, max=0.965, sum=2.868 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.05, mean=0.069, max=0.082, sum=0.207 (3)",
            "tab": "Calibration",
            "score": 0.06908904600115551
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.907, mean=0.923, max=0.933, sum=2.769 (3)",
            "tab": "Robustness",
            "score": 0.923
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.931, mean=0.949, max=0.96, sum=2.847 (3)",
            "tab": "Fairness",
            "score": 0.949
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=2.903, mean=4.229, max=4.983, sum=12.688 (3)",
            "tab": "General information",
            "score": 4.229333333333333
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1283.038, mean=1562.808, max=1784.2, sum=4688.425 (3)",
            "tab": "General information",
            "score": 1562.8083333333334
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on CivilComments",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.524,
        "details": {
          "description": "min=0.035, mean=0.524, max=0.968, sum=28.319 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.056, mean=0.313, max=0.651, sum=16.899 (54)",
            "tab": "Calibration",
            "score": 0.3129455444585645
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.012, mean=0.408, max=0.908, sum=22.047 (54)",
            "tab": "Robustness",
            "score": 0.408272754767954
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.03, mean=0.415, max=0.875, sum=22.43 (54)",
            "tab": "Fairness",
            "score": 0.41537457925495214
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=362.293, mean=732.514, max=1288.441, sum=39555.782 (54)",
            "tab": "General information",
            "score": 732.5144825548033
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on RAFT",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.624,
        "details": {
          "description": "min=0, mean=0.624, max=0.975, sum=20.6 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.078, mean=0.25, max=1, sum=8.255 (33)",
            "tab": "Calibration",
            "score": 0.2501605016965272
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.489, max=0.925, sum=16.125 (33)",
            "tab": "Robustness",
            "score": 0.48863636363636365
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0, mean=0.604, max=0.975, sum=19.925 (33)",
            "tab": "Fairness",
            "score": 0.6037878787878787
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.557, max=5, sum=150.375 (33)",
            "tab": "General information",
            "score": 4.556818181818182
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=270.325, mean=814.446, max=1777.025, sum=26876.725 (33)",
            "tab": "General information",
            "score": 814.446212121212
          },
          "RAFT - # output tokens": {
            "description": "min=0, mean=2.99, max=7.05, sum=98.675 (33)",
            "tab": "General information",
            "score": 2.9901515151515157
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}