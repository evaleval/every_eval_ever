{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_classic/cohere_Cohere-small-v20220720-410M/1768090731.5328572",
  "retrieved_timestamp": "1768090731.5328572",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Cohere small v20220720 410M",
    "id": "cohere/Cohere-small-v20220720-410M",
    "developer": "cohere",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.109,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.6085000742339626
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.1469566826886926
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.15386697669576083
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.5343333333333333
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.45155563090416306
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.412334270667604
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.29156223893065997
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.264,
        "details": {
          "description": "min=0.18, mean=0.264, max=0.42, sum=3.963 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.049, mean=0.136, max=0.202, sum=2.04 (15)",
            "tab": "Calibration",
            "score": 0.13602108170852936
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.13, mean=0.226, max=0.42, sum=3.397 (15)",
            "tab": "Robustness",
            "score": 0.22644444444444442
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.1, mean=0.222, max=0.4, sum=3.334 (15)",
            "tab": "Fairness",
            "score": 0.22225730994152046
          },
          "MMLU - Denoised inference time (s)": {
            "description": "min=0.265, mean=0.284, max=0.312, sum=4.267 (15)",
            "tab": "Efficiency",
            "score": 0.284456830180921
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=372.75, mean=481.26, max=628.421, sum=7218.903 (15)",
            "tab": "General information",
            "score": 481.2602105263158
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "BoolQ - EM",
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.457,
        "details": {
          "description": "min=0.447, mean=0.457, max=0.464, sum=1.372 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.072, mean=0.095, max=0.124, sum=0.285 (3)",
            "tab": "Calibration",
            "score": 0.09496766959019069
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.352, mean=0.361, max=0.378, sum=1.083 (3)",
            "tab": "Robustness",
            "score": 0.361
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.346, mean=0.374, max=0.396, sum=1.121 (3)",
            "tab": "Fairness",
            "score": 0.37366666666666665
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "min=0.319, mean=0.367, max=0.436, sum=1.101 (3)",
            "tab": "Efficiency",
            "score": 0.36694511328125
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=669.307, mean=925.307, max=1269.307, sum=2775.921 (3)",
            "tab": "General information",
            "score": 925.3070000000001
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1.001, max=1.004, sum=3.004 (3)",
            "tab": "General information",
            "score": 1.0013333333333334
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.294,
        "details": {
          "description": "min=0.281, mean=0.294, max=0.309, sum=0.881 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.029, mean=0.031, max=0.033, sum=0.093 (3)",
            "tab": "Calibration",
            "score": 0.031094283389380417
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.076, mean=0.078, max=0.081, sum=0.235 (3)",
            "tab": "Robustness",
            "score": 0.07821074014295328
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.167, mean=0.179, max=0.197, sum=0.538 (3)",
            "tab": "Fairness",
            "score": 0.17918507973514153
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "min=0.544, mean=0.56, max=0.583, sum=1.681 (3)",
            "tab": "Efficiency",
            "score": 0.5603894916373239
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=0.958, mean=1.562, max=1.997, sum=4.687 (3)",
            "tab": "General information",
            "score": 1.5624413145539906
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1601.997, mean=1634.99, max=1693.155, sum=4904.969 (3)",
            "tab": "General information",
            "score": 1634.9896713615024
          },
          "NarrativeQA - # output tokens": {
            "description": "min=8.149, mean=11.007, max=15.597, sum=33.02 (3)",
            "tab": "General information",
            "score": 11.006572769953053
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.411, mean=0.418, max=0.429, sum=1.255 (3)",
            "tab": "Bias",
            "score": 0.4184126984126984
          },
          "NarrativeQA - Representation (race)": {
            "description": "min=0.333, mean=0.556, max=0.667, sum=1.667 (3)",
            "tab": "Bias",
            "score": 0.5555555555555556
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.186, mean=0.202, max=0.217, sum=0.606 (3)",
            "tab": "Bias",
            "score": 0.20205501924662395
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.025, mean=0.027, max=0.031, sum=0.082 (3)",
            "tab": "Toxicity",
            "score": 0.027230046948356807
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.309,
        "details": {
          "description": "min=0.291, mean=0.309, max=0.334, sum=0.928 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.02, mean=0.023, max=0.027, sum=0.07 (3)",
            "tab": "Calibration",
            "score": 0.023328620693919305
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.18, mean=0.198, max=0.221, sum=0.594 (3)",
            "tab": "Calibration",
            "score": 0.198062019189297
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.024, mean=0.025, max=0.027, sum=0.075 (3)",
            "tab": "Robustness",
            "score": 0.025009279663584086
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.066, mean=0.074, max=0.08, sum=0.222 (3)",
            "tab": "Robustness",
            "score": 0.07408175909872887
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.052, mean=0.055, max=0.062, sum=0.166 (3)",
            "tab": "Fairness",
            "score": 0.055406816944260924
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.198, mean=0.219, max=0.246, sum=0.657 (3)",
            "tab": "Fairness",
            "score": 0.21887630944724534
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "min=0.246, mean=0.251, max=0.259, sum=0.753 (3)",
            "tab": "Efficiency",
            "score": 0.2509381953124994
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "min=0.552, mean=0.605, max=0.643, sum=1.815 (3)",
            "tab": "Efficiency",
            "score": 0.6049964999999996
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=109.191, mean=111.191, max=115.191, sum=333.573 (3)",
            "tab": "General information",
            "score": 111.19099999999999
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=4.325, mean=5.149, max=6.46, sum=15.446 (3)",
            "tab": "General information",
            "score": 5.148666666666667
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.538, mean=4.633, max=4.715, sum=13.899 (3)",
            "tab": "General information",
            "score": 4.633
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.039, mean=0.039, max=0.039, sum=0.117 (3)",
            "tab": "General information",
            "score": 0.039
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1261.72, mean=1481.344, max=1608.455, sum=4444.032 (3)",
            "tab": "General information",
            "score": 1481.344
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=20.452, mean=22.835, max=25.41, sum=68.505 (3)",
            "tab": "General information",
            "score": 22.834999999999997
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1 (2)",
            "tab": "Bias",
            "score": 0.5
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.238, mean=0.415, max=0.539, sum=1.244 (3)",
            "tab": "Bias",
            "score": 0.41471861471861476
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.167, mean=0.234, max=0.286, sum=0.702 (3)",
            "tab": "Bias",
            "score": 0.2341269841269841
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.48, mean=0.485, max=0.494, sum=1.455 (3)",
            "tab": "Bias",
            "score": 0.48499285130718955
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.382, mean=0.435, max=0.467, sum=1.306 (3)",
            "tab": "Bias",
            "score": 0.43543086336382425
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.234, mean=0.265, max=0.3, sum=0.796 (3)",
            "tab": "Bias",
            "score": 0.2653339127915399
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.001, sum=0.002 (3)",
            "tab": "Toxicity",
            "score": 0.0006666666666666666
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0.002, mean=0.003, max=0.003, sum=0.008 (3)",
            "tab": "Toxicity",
            "score": 0.0026666666666666666
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "QuAC - F1",
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.219,
        "details": {
          "description": "min=0.208, mean=0.219, max=0.238, sum=0.656 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.03, mean=0.036, max=0.042, sum=0.108 (3)",
            "tab": "Calibration",
            "score": 0.035862172954873824
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.094, mean=0.098, max=0.101, sum=0.293 (3)",
            "tab": "Robustness",
            "score": 0.09766108203425072
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.135, mean=0.144, max=0.162, sum=0.433 (3)",
            "tab": "Fairness",
            "score": 0.14446776305873513
          },
          "QuAC - Denoised inference time (s)": {
            "description": "min=0.611, mean=0.619, max=0.625, sum=1.856 (3)",
            "tab": "Efficiency",
            "score": 0.6185995332031252
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.797, mean=0.881, max=0.969, sum=2.644 (3)",
            "tab": "General information",
            "score": 0.8813333333333334
          },
          "QuAC - truncated": {
            "description": "min=0.02, mean=0.02, max=0.02, sum=0.06 (3)",
            "tab": "General information",
            "score": 0.02
          },
          "QuAC - # prompt tokens": {
            "description": "min=1600.292, mean=1639.784, max=1661.675, sum=4919.353 (3)",
            "tab": "General information",
            "score": 1639.784333333333
          },
          "QuAC - # output tokens": {
            "description": "min=18.807, mean=20.639, max=21.99, sum=61.916 (3)",
            "tab": "General information",
            "score": 20.638666666666666
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=2 (3)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.447, mean=0.458, max=0.468, sum=1.375 (3)",
            "tab": "Bias",
            "score": 0.45823351891324243
          },
          "QuAC - Representation (race)": {
            "description": "min=0.329, mean=0.341, max=0.364, sum=1.022 (3)",
            "tab": "Bias",
            "score": 0.34075560523096593
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.277, mean=0.285, max=0.299, sum=0.854 (3)",
            "tab": "Bias",
            "score": 0.2847879707506289
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.001, mean=0.003, max=0.004, sum=0.008 (3)",
            "tab": "Toxicity",
            "score": 0.0026666666666666666
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.483,
        "details": {
          "description": "min=0.483, mean=0.483, max=0.483, sum=0.483 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.083, mean=0.083, max=0.083, sum=0.083 (1)",
            "tab": "Calibration",
            "score": 0.08312318484699062
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.405, mean=0.405, max=0.405, sum=0.405 (1)",
            "tab": "Robustness",
            "score": 0.405
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.308, mean=0.308, max=0.308, sum=0.308 (1)",
            "tab": "Fairness",
            "score": 0.308
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "min=0.223, mean=0.223, max=0.223, sum=0.223 (1)",
            "tab": "Efficiency",
            "score": 0.22341269531249972
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=88.855, mean=88.855, max=88.855, sum=88.855 (1)",
            "tab": "General information",
            "score": 88.855
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.348,
        "details": {
          "description": "min=0.348, mean=0.348, max=0.348, sum=0.348 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.379, mean=0.379, max=0.379, sum=0.379 (1)",
            "tab": "Calibration",
            "score": 0.37852917669250147
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.238, mean=0.238, max=0.238, sum=0.238 (1)",
            "tab": "Robustness",
            "score": 0.238
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.28, mean=0.28, max=0.28, sum=0.28 (1)",
            "tab": "Fairness",
            "score": 0.28
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "min=0.214, mean=0.214, max=0.214, sum=0.214 (1)",
            "tab": "Efficiency",
            "score": 0.2136278906249995
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.358, mean=5.358, max=5.358, sum=5.358 (1)",
            "tab": "General information",
            "score": 5.358
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.217,
        "details": {
          "description": "min=0.202, mean=0.217, max=0.226, sum=0.65 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.059, mean=0.076, max=0.098, sum=0.229 (3)",
            "tab": "Calibration",
            "score": 0.07625390965133329
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.2, mean=0.204, max=0.211, sum=0.612 (3)",
            "tab": "Robustness",
            "score": 0.2038735983690112
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.194, mean=0.203, max=0.214, sum=0.609 (3)",
            "tab": "Fairness",
            "score": 0.20285423037716613
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "min=0.287, mean=0.289, max=0.295, sum=0.868 (3)",
            "tab": "Efficiency",
            "score": 0.2894203160837155
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=505.315, mean=514.648, max=532.315, sum=1543.945 (3)",
            "tab": "General information",
            "score": 514.6483180428135
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.304,
        "details": {
          "description": "min=0.258, mean=0.304, max=0.338, sum=0.911 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "No matching runs",
            "tab": "Robustness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.22, mean=0.252, max=0.287, sum=0.757 (3)",
            "tab": "Robustness",
            "score": 0.2521940956196658
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "No matching runs",
            "tab": "Fairness",
            "score": null
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.228, mean=0.28, max=0.324, sum=0.84 (3)",
            "tab": "Fairness",
            "score": 0.2798487582673837
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "No matching runs",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "min=0.282, mean=0.291, max=0.303, sum=0.872 (3)",
            "tab": "Efficiency",
            "score": 0.29054985767926356
          },
          "MS MARCO (regular) - # eval": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # train": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - truncated": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (regular) - # trials": {
            "description": "No matching runs",
            "tab": "General information",
            "score": null
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=480.163, mean=519.496, max=566.163, sum=1558.488 (3)",
            "tab": "General information",
            "score": 519.4961240310078
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=1, mean=1.031, max=1.093, sum=3.093 (3)",
            "tab": "General information",
            "score": 1.0310077519379846
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "No matching runs",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "No matching runs",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.063,
        "details": {
          "description": "min=0.031, mean=0.063, max=0.087, sum=0.377 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "min=0.781, mean=0.954, max=1.052, sum=5.724 (6)",
            "tab": "Efficiency",
            "score": 0.9539734693535404
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1555.036, mean=1575.036, max=1602.036, sum=9450.219 (6)",
            "tab": "General information",
            "score": 1575.0364806866953
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=49.71, mean=78.352, max=93.899, sum=470.112 (6)",
            "tab": "General information",
            "score": 78.3519313304721
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.625, mean=0.648, max=0.667, sum=3.885 (6)",
            "tab": "Bias",
            "score": 0.6475615887380594
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.405, mean=0.42, max=0.449, sum=2.522 (6)",
            "tab": "Bias",
            "score": 0.4203329386778049
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.099, mean=0.145, max=0.201, sum=0.868 (6)",
            "tab": "Bias",
            "score": 0.14468337947687135
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.163, mean=0.182, max=0.21, sum=1.09 (6)",
            "tab": "Bias",
            "score": 0.18171396544569016
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.009 (6)",
            "tab": "Toxicity",
            "score": 0.001430615164520744
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=-0.077, mean=0.054, max=0.168, sum=0.161 (3)",
            "tab": "Summarization metrics",
            "score": 0.053643734154981075
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "min=0.051, mean=2.638, max=4.057, sum=15.831 (6)",
            "tab": "Summarization metrics",
            "score": 2.6384596103973283
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=-0.069, mean=0.026, max=0.075, sum=0.077 (3)",
            "tab": "Summarization metrics",
            "score": 0.025643326292308758
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.532, mean=0.744, max=0.913, sum=4.465 (6)",
            "tab": "Summarization metrics",
            "score": 0.7441391663831297
          },
          "CNN/DailyMail - Density": {
            "description": "min=11.632, mean=25.238, max=33.415, sum=151.427 (6)",
            "tab": "Summarization metrics",
            "score": 25.237906513316556
          },
          "CNN/DailyMail - Compression": {
            "description": "min=9.053, mean=13.243, max=20.787, sum=79.46 (6)",
            "tab": "Summarization metrics",
            "score": 13.243377373187593
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.033,
        "details": {
          "description": "min=0.031, mean=0.033, max=0.037, sum=0.199 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "min=0.637, mean=0.642, max=0.649, sum=3.85 (6)",
            "tab": "Efficiency",
            "score": 0.6416181225868728
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.996, mean=4.998, max=5, sum=29.988 (6)",
            "tab": "General information",
            "score": 4.998069498069498
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1484.608, mean=1537.452, max=1572.616, sum=9224.71 (6)",
            "tab": "General information",
            "score": 1537.4517374517375
          },
          "XSUM - # output tokens": {
            "description": "min=25.859, mean=27.394, max=28.226, sum=164.363 (6)",
            "tab": "General information",
            "score": 27.393822393822393
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.399, mean=0.43, max=0.493, sum=2.58 (6)",
            "tab": "Bias",
            "score": 0.43004930254930257
          },
          "XSUM - Representation (race)": {
            "description": "min=0.542, mean=0.556, max=0.583, sum=3.333 (6)",
            "tab": "Bias",
            "score": 0.5555555555555556
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.224, mean=0.246, max=0.283, sum=1.474 (6)",
            "tab": "Bias",
            "score": 0.2457025240044108
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.004 (6)",
            "tab": "Toxicity",
            "score": 0.0006435006435006435
          },
          "XSUM - SummaC": {
            "description": "min=0.0, mean=0.028, max=0.073, sum=0.085 (3)",
            "tab": "Summarization metrics",
            "score": 0.02834827232857105
          },
          "XSUM - QAFactEval": {
            "description": "min=2.873, mean=3.094, max=3.373, sum=18.563 (6)",
            "tab": "Summarization metrics",
            "score": 3.0938511325795113
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.173, mean=0.195, max=0.221, sum=0.585 (3)",
            "tab": "Summarization metrics",
            "score": 0.1951040609680371
          },
          "XSUM - Coverage": {
            "description": "min=0.853, mean=0.863, max=0.87, sum=5.178 (6)",
            "tab": "Summarization metrics",
            "score": 0.8630576414302875
          },
          "XSUM - Density": {
            "description": "min=9.489, mean=10.557, max=12.063, sum=63.341 (6)",
            "tab": "Summarization metrics",
            "score": 10.556911526268395
          },
          "XSUM - Compression": {
            "description": "min=16.738, mean=17.551, max=18.157, sum=105.306 (6)",
            "tab": "Summarization metrics",
            "score": 17.55096225657148
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "IMDB - EM",
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.578,
        "details": {
          "description": "min=0.53, mean=0.578, max=0.618, sum=1.735 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.085, mean=0.134, max=0.174, sum=0.401 (3)",
            "tab": "Calibration",
            "score": 0.13354341899719424
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.447, mean=0.473, max=0.498, sum=1.418 (3)",
            "tab": "Robustness",
            "score": 0.4726666666666666
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.49, mean=0.518, max=0.54, sum=1.554 (3)",
            "tab": "Fairness",
            "score": 0.518
          },
          "IMDB - Denoised inference time (s)": {
            "description": "min=0.414, mean=0.458, max=0.52, sum=1.373 (3)",
            "tab": "Efficiency",
            "score": 0.45773176757812467
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.846, mean=4.93, max=4.98, sum=14.79 (3)",
            "tab": "General information",
            "score": 4.930000000000001
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1161.854, mean=1398.654, max=1747.025, sum=4195.961 (3)",
            "tab": "General information",
            "score": 1398.6536666666668
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "CivilComments - EM",
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.501,
        "details": {
          "description": "min=0, mean=0.501, max=1, sum=27.062 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.201, mean=0.486, max=0.8, sum=26.269 (54)",
            "tab": "Calibration",
            "score": 0.4864679961449666
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0, mean=0.434, max=1, sum=23.451 (54)",
            "tab": "Robustness",
            "score": 0.4342847473494527
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0, mean=0.495, max=1, sum=26.744 (54)",
            "tab": "Fairness",
            "score": 0.49526155082406725
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "min=0.264, mean=0.329, max=0.439, sum=17.76 (54)",
            "tab": "Efficiency",
            "score": 0.32889709084919744
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=362.293, mean=732.514, max=1288.441, sum=39555.782 (54)",
            "tab": "General information",
            "score": 732.5144825548033
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "RAFT - EM",
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.492,
        "details": {
          "description": "min=0, mean=0.492, max=0.975, sum=16.225 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.084, mean=0.234, max=0.631, sum=7.714 (33)",
            "tab": "Calibration",
            "score": 0.23374335739699753
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.403, max=0.975, sum=13.3 (33)",
            "tab": "Robustness",
            "score": 0.40303030303030307
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0, mean=0.452, max=0.975, sum=14.9 (33)",
            "tab": "Fairness",
            "score": 0.4515151515151515
          },
          "RAFT - Denoised inference time (s)": {
            "description": "min=0.256, mean=0.36, max=0.547, sum=11.878 (33)",
            "tab": "Efficiency",
            "score": 0.3599495087594697
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.557, max=5, sum=150.375 (33)",
            "tab": "General information",
            "score": 4.556818181818182
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=270.325, mean=814.446, max=1777.025, sum=26876.725 (33)",
            "tab": "General information",
            "score": 814.446212121212
          },
          "RAFT - # output tokens": {
            "description": "min=1, mean=3.239, max=5.575, sum=106.9 (33)",
            "tab": "General information",
            "score": 3.2393939393939393
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "min=0.5, mean=0.5, max=0.5, sum=1.5 (3)",
            "tab": "Bias",
            "score": 0.5
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {}
    }
  ]
}