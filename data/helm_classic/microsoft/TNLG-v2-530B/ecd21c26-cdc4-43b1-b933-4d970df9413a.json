{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_classic/microsoft_TNLG-v2-530B/1770830385.7573261",
  "retrieved_timestamp": "1770830385.7573261",
  "source_metadata": {
    "source_name": "helm_classic",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "TNLG v2 530B",
    "id": "microsoft/TNLG-v2-530B",
    "developer": "microsoft",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_classic",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperform on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.787,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Calibration": {
            "description": null,
            "tab": "Calibration",
            "score": 0.6152996196936993
          },
          "Mean win rate - Robustness": {
            "description": null,
            "tab": "Robustness",
            "score": 0.6503510949562118
          },
          "Mean win rate - Fairness": {
            "description": null,
            "tab": "Fairness",
            "score": 0.7516679834811092
          },
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": null
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          },
          "Mean win rate - Bias": {
            "description": null,
            "tab": "Bias",
            "score": 0.5308990441173578
          },
          "Mean win rate - Toxicity": {
            "description": null,
            "tab": "Toxicity",
            "score": 0.3298371381704715
          },
          "Mean win rate - Summarization metrics": {
            "description": null,
            "tab": "Summarization metrics",
            "score": 0.756578947368421
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.469,
        "details": {
          "description": "min=0.24, mean=0.469, max=0.78, sum=7.035 (15)",
          "tab": "Accuracy",
          "MMLU - ECE (10-bin)": {
            "description": "min=0.073, mean=0.127, max=0.202, sum=1.908 (15)",
            "tab": "Calibration",
            "score": 0.12722994020701678
          },
          "MMLU - EM (Robustness)": {
            "description": "min=0.15, mean=0.403, max=0.75, sum=6.051 (15)",
            "tab": "Robustness",
            "score": 0.40336842105263154
          },
          "MMLU - EM (Fairness)": {
            "description": "min=0.17, mean=0.418, max=0.75, sum=6.266 (15)",
            "tab": "Fairness",
            "score": 0.41770760233918125
          },
          "MMLU - Denoised inference time (s)": {
            "description": "5 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=1542 (15)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=75 (15)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (15)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=371.38, mean=472.274, max=624.07, sum=7084.111 (15)",
            "tab": "General information",
            "score": 472.2740350877193
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=15 (15)",
            "tab": "General information",
            "score": 1.0
          },
          "MMLU - # trials": {
            "description": "min=3, mean=3, max=3, sum=45 (15)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "BoolQ - EM",
      "source_data": {
        "dataset_name": "BoolQ",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.809,
        "details": {
          "description": "min=0.798, mean=0.809, max=0.829, sum=2.428 (3)",
          "tab": "Accuracy",
          "BoolQ - ECE (10-bin)": {
            "description": "min=0.017, mean=0.048, max=0.088, sum=0.144 (3)",
            "tab": "Calibration",
            "score": 0.04811928896988451
          },
          "BoolQ - EM (Robustness)": {
            "description": "min=0.724, mean=0.733, max=0.747, sum=2.198 (3)",
            "tab": "Robustness",
            "score": 0.7326666666666667
          },
          "BoolQ - EM (Fairness)": {
            "description": "min=0.756, mean=0.767, max=0.777, sum=2.3 (3)",
            "tab": "Fairness",
            "score": 0.7666666666666667
          },
          "BoolQ - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "BoolQ - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "BoolQ - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "BoolQ - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "BoolQ - # prompt tokens": {
            "description": "min=660.073, mean=908.406, max=1242.073, sum=2725.219 (3)",
            "tab": "General information",
            "score": 908.4063333333334
          },
          "BoolQ - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "BoolQ - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "BoolQ - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "BoolQ - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.722,
        "details": {
          "description": "min=0.692, mean=0.722, max=0.743, sum=2.166 (3)",
          "tab": "Accuracy",
          "NarrativeQA - ECE (10-bin)": {
            "description": "min=0.026, mean=0.05, max=0.075, sum=0.15 (3)",
            "tab": "Calibration",
            "score": 0.05012197972633472
          },
          "NarrativeQA - F1 (Robustness)": {
            "description": "min=0.22, mean=0.319, max=0.405, sum=0.957 (3)",
            "tab": "Robustness",
            "score": 0.31894751591392195
          },
          "NarrativeQA - F1 (Fairness)": {
            "description": "min=0.601, mean=0.632, max=0.664, sum=1.895 (3)",
            "tab": "Fairness",
            "score": 0.6318169391667601
          },
          "NarrativeQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=1065 (3)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=1.051, mean=1.646, max=2.085, sum=4.938 (3)",
            "tab": "General information",
            "score": 1.6460093896713615
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=1600.366, mean=1651.848, max=1705.003, sum=4955.544 (3)",
            "tab": "General information",
            "score": 1651.8478873239437
          },
          "NarrativeQA - # output tokens": {
            "description": "min=5.113, mean=5.982, max=7.265, sum=17.946 (3)",
            "tab": "General information",
            "score": 5.982159624413145
          },
          "NarrativeQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NarrativeQA - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Stereotypes (gender)": {
            "description": "min=0.375, mean=0.395, max=0.436, sum=1.186 (3)",
            "tab": "Bias",
            "score": 0.3952991452991453
          },
          "NarrativeQA - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NarrativeQA - Representation (gender)": {
            "description": "min=0.204, mean=0.221, max=0.239, sum=0.663 (3)",
            "tab": "Bias",
            "score": 0.22112892189926373
          },
          "NarrativeQA - Toxic fraction": {
            "description": "min=0.011, mean=0.012, max=0.014, sum=0.037 (3)",
            "tab": "Toxicity",
            "score": 0.012206572769953052
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "source_data": {
        "dataset_name": "NaturalQuestions (open-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.642,
        "details": {
          "description": "min=0.617, mean=0.642, max=0.656, sum=1.926 (3)",
          "tab": "Accuracy",
          "NaturalQuestions (closed-book) - ECE (10-bin)": {
            "description": "min=0.038, mean=0.04, max=0.041, sum=0.119 (3)",
            "tab": "Calibration",
            "score": 0.039723290660202144
          },
          "NaturalQuestions (open-book) - ECE (10-bin)": {
            "description": "min=0.071, mean=0.075, max=0.078, sum=0.225 (3)",
            "tab": "Calibration",
            "score": 0.07490014228309726
          },
          "NaturalQuestions (closed-book) - F1 (Robustness)": {
            "description": "min=0.291, mean=0.307, max=0.322, sum=0.922 (3)",
            "tab": "Robustness",
            "score": 0.3074701383832172
          },
          "NaturalQuestions (open-book) - F1 (Robustness)": {
            "description": "min=0.483, mean=0.525, max=0.549, sum=1.576 (3)",
            "tab": "Robustness",
            "score": 0.5253631735860874
          },
          "NaturalQuestions (closed-book) - F1 (Fairness)": {
            "description": "min=0.306, mean=0.318, max=0.324, sum=0.953 (3)",
            "tab": "Fairness",
            "score": 0.3175020164111731
          },
          "NaturalQuestions (open-book) - F1 (Fairness)": {
            "description": "min=0.575, mean=0.598, max=0.61, sum=1.794 (3)",
            "tab": "Fairness",
            "score": 0.5979278798197498
          },
          "NaturalQuestions (closed-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (open-book) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=110.254, mean=112.254, max=116.254, sum=336.762 (3)",
            "tab": "General information",
            "score": 112.254
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=3.8, mean=4.569, max=5.632, sum=13.707 (3)",
            "tab": "General information",
            "score": 4.569
          },
          "NaturalQuestions (closed-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.647, mean=4.691, max=4.723, sum=14.072 (3)",
            "tab": "General information",
            "score": 4.690666666666666
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.036, mean=0.036, max=0.036, sum=0.108 (3)",
            "tab": "General information",
            "score": 0.036
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1231.056, mean=1419.328, max=1523.222, sum=4257.983 (3)",
            "tab": "General information",
            "score": 1419.3276666666668
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=5.953, mean=6.015, max=6.134, sum=18.045 (3)",
            "tab": "General information",
            "score": 6.015000000000001
          },
          "NaturalQuestions (open-book) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "NaturalQuestions (closed-book) - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "NaturalQuestions (closed-book) - Stereotypes (gender)": {
            "description": "min=0.25, mean=0.342, max=0.443, sum=1.026 (3)",
            "tab": "Bias",
            "score": 0.342063492063492
          },
          "NaturalQuestions (closed-book) - Representation (race)": {
            "description": "min=0.53, mean=0.559, max=0.573, sum=1.676 (3)",
            "tab": "Bias",
            "score": 0.5587121212121212
          },
          "NaturalQuestions (closed-book) - Representation (gender)": {
            "description": "min=0.206, mean=0.289, max=0.419, sum=0.867 (3)",
            "tab": "Bias",
            "score": 0.2891147156537034
          },
          "NaturalQuestions (open-book) - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
            "tab": "Bias",
            "score": 0.6666666666666667
          },
          "NaturalQuestions (open-book) - Stereotypes (gender)": {
            "description": "min=0.191, mean=0.277, max=0.345, sum=0.83 (3)",
            "tab": "Bias",
            "score": 0.27656250000000004
          },
          "NaturalQuestions (open-book) - Representation (race)": {
            "description": "min=0.457, mean=0.469, max=0.484, sum=1.408 (3)",
            "tab": "Bias",
            "score": 0.4693006584979578
          },
          "NaturalQuestions (open-book) - Representation (gender)": {
            "description": "min=0.254, mean=0.259, max=0.261, sum=0.776 (3)",
            "tab": "Bias",
            "score": 0.2587447378492154
          },
          "NaturalQuestions (closed-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.002, sum=0.003 (3)",
            "tab": "Toxicity",
            "score": 0.001
          },
          "NaturalQuestions (open-book) - Toxic fraction": {
            "description": "min=0, mean=0.001, max=0.001, sum=0.002 (3)",
            "tab": "Toxicity",
            "score": 0.0006666666666666666
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "QuAC - F1",
      "source_data": {
        "dataset_name": "QuAC",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.39,
        "details": {
          "description": "min=0.388, mean=0.39, max=0.393, sum=1.171 (3)",
          "tab": "Accuracy",
          "QuAC - ECE (10-bin)": {
            "description": "min=0.059, mean=0.08, max=0.106, sum=0.241 (3)",
            "tab": "Calibration",
            "score": 0.08020003145494241
          },
          "QuAC - F1 (Robustness)": {
            "description": "min=0.183, mean=0.194, max=0.203, sum=0.583 (3)",
            "tab": "Robustness",
            "score": 0.19421481147358363
          },
          "QuAC - F1 (Fairness)": {
            "description": "min=0.304, mean=0.313, max=0.32, sum=0.94 (3)",
            "tab": "Fairness",
            "score": 0.3132392185201357
          },
          "QuAC - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "QuAC - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "QuAC - # train": {
            "description": "min=0.845, mean=0.944, max=1.084, sum=2.831 (3)",
            "tab": "General information",
            "score": 0.9436666666666667
          },
          "QuAC - truncated": {
            "description": "min=0.016, mean=0.016, max=0.016, sum=0.048 (3)",
            "tab": "General information",
            "score": 0.016
          },
          "QuAC - # prompt tokens": {
            "description": "min=1624.371, mean=1644.436, max=1670.589, sum=4933.308 (3)",
            "tab": "General information",
            "score": 1644.436
          },
          "QuAC - # output tokens": {
            "description": "min=25.915, mean=29.956, max=32.756, sum=89.867 (3)",
            "tab": "General information",
            "score": 29.95566666666667
          },
          "QuAC - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "QuAC - Stereotypes (race)": {
            "description": "min=0.56, mean=0.579, max=0.599, sum=1.738 (3)",
            "tab": "Bias",
            "score": 0.5794166151309009
          },
          "QuAC - Stereotypes (gender)": {
            "description": "min=0.428, mean=0.435, max=0.448, sum=1.305 (3)",
            "tab": "Bias",
            "score": 0.43504680341335694
          },
          "QuAC - Representation (race)": {
            "description": "min=0.282, mean=0.333, max=0.369, sum=0.999 (3)",
            "tab": "Bias",
            "score": 0.33315102716024375
          },
          "QuAC - Representation (gender)": {
            "description": "min=0.24, mean=0.25, max=0.259, sum=0.75 (3)",
            "tab": "Bias",
            "score": 0.2499075403684782
          },
          "QuAC - Toxic fraction": {
            "description": "min=0.002, mean=0.003, max=0.003, sum=0.008 (3)",
            "tab": "Toxicity",
            "score": 0.0026666666666666666
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "HellaSwag - EM",
      "source_data": {
        "dataset_name": "HellaSwag",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.799,
        "details": {
          "description": "min=0.799, mean=0.799, max=0.799, sum=0.799 (1)",
          "tab": "Accuracy",
          "HellaSwag - ECE (10-bin)": {
            "description": "min=0.322, mean=0.322, max=0.322, sum=0.322 (1)",
            "tab": "Calibration",
            "score": 0.32242755675811835
          },
          "HellaSwag - EM (Robustness)": {
            "description": "min=0.757, mean=0.757, max=0.757, sum=0.757 (1)",
            "tab": "Robustness",
            "score": 0.757
          },
          "HellaSwag - EM (Fairness)": {
            "description": "min=0.678, mean=0.678, max=0.678, sum=0.678 (1)",
            "tab": "Fairness",
            "score": 0.678
          },
          "HellaSwag - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "HellaSwag - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "HellaSwag - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # prompt tokens": {
            "description": "min=87.888, mean=87.888, max=87.888, sum=87.888 (1)",
            "tab": "General information",
            "score": 87.888
          },
          "HellaSwag - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "HellaSwag - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.562,
        "details": {
          "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
          "tab": "Accuracy",
          "OpenbookQA - ECE (10-bin)": {
            "description": "min=0.243, mean=0.243, max=0.243, sum=0.243 (1)",
            "tab": "Calibration",
            "score": 0.2425759072363007
          },
          "OpenbookQA - EM (Robustness)": {
            "description": "min=0.476, mean=0.476, max=0.476, sum=0.476 (1)",
            "tab": "Robustness",
            "score": 0.476
          },
          "OpenbookQA - EM (Fairness)": {
            "description": "min=0.504, mean=0.504, max=0.504, sum=0.504 (1)",
            "tab": "Fairness",
            "score": 0.504
          },
          "OpenbookQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=5.27, mean=5.27, max=5.27, sum=5.27 (1)",
            "tab": "General information",
            "score": 5.27
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # trials": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "TruthfulQA - EM",
      "source_data": {
        "dataset_name": "TruthfulQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.251,
        "details": {
          "description": "min=0.22, mean=0.251, max=0.275, sum=0.752 (3)",
          "tab": "Accuracy",
          "TruthfulQA - ECE (10-bin)": {
            "description": "min=0.174, mean=0.226, max=0.252, sum=0.678 (3)",
            "tab": "Calibration",
            "score": 0.22594889867402287
          },
          "TruthfulQA - EM (Robustness)": {
            "description": "min=0.187, mean=0.202, max=0.217, sum=0.607 (3)",
            "tab": "Robustness",
            "score": 0.20234454638124363
          },
          "TruthfulQA - EM (Fairness)": {
            "description": "min=0.177, mean=0.197, max=0.213, sum=0.59 (3)",
            "tab": "Fairness",
            "score": 0.19673802242609584
          },
          "TruthfulQA - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "TruthfulQA - # eval": {
            "description": "min=654, mean=654, max=654, sum=1962 (3)",
            "tab": "General information",
            "score": 654.0
          },
          "TruthfulQA - # train": {
            "description": "min=5, mean=5, max=5, sum=15 (3)",
            "tab": "General information",
            "score": 5.0
          },
          "TruthfulQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "TruthfulQA - # prompt tokens": {
            "description": "min=501.121, mean=511.121, max=529.121, sum=1533.362 (3)",
            "tab": "General information",
            "score": 511.12079510703364
          },
          "TruthfulQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "TruthfulQA - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MS MARCO (TREC) - NDCG@10",
      "source_data": {
        "dataset_name": "MS MARCO (TREC)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).\n\nNDCG@10: Normalized discounted cumulative gain at 10 in information retrieval.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.643,
        "details": {
          "description": "min=0.621, mean=0.643, max=0.662, sum=1.93 (3)",
          "tab": "Accuracy",
          "MS MARCO (regular) - RR@10 (Robustness)": {
            "description": "min=0.264, mean=0.287, max=0.315, sum=0.86 (3)",
            "tab": "Robustness",
            "score": 0.28667883597883553
          },
          "MS MARCO (TREC) - NDCG@10 (Robustness)": {
            "description": "min=0.54, mean=0.565, max=0.586, sum=1.696 (3)",
            "tab": "Robustness",
            "score": 0.5653481865448796
          },
          "MS MARCO (regular) - RR@10 (Fairness)": {
            "description": "min=0.332, mean=0.341, max=0.354, sum=1.024 (3)",
            "tab": "Fairness",
            "score": 0.3414910052910049
          },
          "MS MARCO (TREC) - NDCG@10 (Fairness)": {
            "description": "min=0.592, mean=0.612, max=0.629, sum=1.836 (3)",
            "tab": "Fairness",
            "score": 0.6120938886543282
          },
          "MS MARCO (regular) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (TREC) - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "MS MARCO (regular) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "MS MARCO (regular) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (regular) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (regular) - # prompt tokens": {
            "description": "min=495.232, mean=532.565, max=577.232, sum=1597.696 (3)",
            "tab": "General information",
            "score": 532.5653333333333
          },
          "MS MARCO (regular) - # output tokens": {
            "description": "min=1.004, mean=1.011, max=1.02, sum=3.034 (3)",
            "tab": "General information",
            "score": 1.0113333333333334
          },
          "MS MARCO (regular) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (TREC) - # eval": {
            "description": "min=43, mean=43, max=43, sum=129 (3)",
            "tab": "General information",
            "score": 43.0
          },
          "MS MARCO (TREC) - # train": {
            "description": "min=2, mean=2, max=2, sum=6 (3)",
            "tab": "General information",
            "score": 2.0
          },
          "MS MARCO (TREC) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "MS MARCO (TREC) - # prompt tokens": {
            "description": "min=478.488, mean=515.822, max=560.488, sum=1547.465 (3)",
            "tab": "General information",
            "score": 515.8217054263565
          },
          "MS MARCO (TREC) - # output tokens": {
            "description": "min=1, mean=1.016, max=1.023, sum=3.047 (3)",
            "tab": "General information",
            "score": 1.0155038759689923
          },
          "MS MARCO (TREC) - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "MS MARCO (regular) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (TREC) - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "MS MARCO (regular) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          },
          "MS MARCO (TREC) - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CNN/DailyMail - ROUGE-2",
      "source_data": {
        "dataset_name": "CNN/DailyMail",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.161,
        "details": {
          "description": "min=0.151, mean=0.161, max=0.166, sum=0.966 (6)",
          "tab": "Accuracy",
          "CNN/DailyMail - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CNN/DailyMail - # eval": {
            "description": "min=466, mean=466, max=466, sum=2796 (6)",
            "tab": "General information",
            "score": 466.0
          },
          "CNN/DailyMail - # train": {
            "description": "min=5, mean=5, max=5, sum=30 (6)",
            "tab": "General information",
            "score": 5.0
          },
          "CNN/DailyMail - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "CNN/DailyMail - # prompt tokens": {
            "description": "min=1531.586, mean=1549.919, max=1567.586, sum=9299.515 (6)",
            "tab": "General information",
            "score": 1549.9191702432045
          },
          "CNN/DailyMail - # output tokens": {
            "description": "min=64.44, mean=66.904, max=70.5, sum=401.425 (6)",
            "tab": "General information",
            "score": 66.9041487839771
          },
          "CNN/DailyMail - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "CNN/DailyMail - Stereotypes (race)": {
            "description": "min=0.601, mean=0.629, max=0.647, sum=3.773 (6)",
            "tab": "Bias",
            "score": 0.6288257738993034
          },
          "CNN/DailyMail - Stereotypes (gender)": {
            "description": "min=0.377, mean=0.398, max=0.411, sum=2.388 (6)",
            "tab": "Bias",
            "score": 0.3980717194410541
          },
          "CNN/DailyMail - Representation (race)": {
            "description": "min=0.135, mean=0.227, max=0.309, sum=1.359 (6)",
            "tab": "Bias",
            "score": 0.22651255675216078
          },
          "CNN/DailyMail - Representation (gender)": {
            "description": "min=0.114, mean=0.12, max=0.124, sum=0.721 (6)",
            "tab": "Bias",
            "score": 0.12013592572007394
          },
          "CNN/DailyMail - Toxic fraction": {
            "description": "min=0.002, mean=0.003, max=0.004, sum=0.017 (6)",
            "tab": "Toxicity",
            "score": 0.002861230329041488
          },
          "CNN/DailyMail - SummaC": {
            "description": "min=0.553, mean=0.573, max=0.595, sum=1.718 (3)",
            "tab": "Summarization metrics",
            "score": 0.5727510890981916
          },
          "CNN/DailyMail - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - BERTScore (F1)": {
            "description": "min=0.296, mean=0.316, max=0.326, sum=0.947 (3)",
            "tab": "Summarization metrics",
            "score": 0.3157002201673737
          },
          "CNN/DailyMail - Coverage": {
            "description": "min=0.975, mean=0.977, max=0.981, sum=5.862 (6)",
            "tab": "Summarization metrics",
            "score": 0.9770276969879915
          },
          "CNN/DailyMail - Density": {
            "description": "min=25.944, mean=26.968, max=27.893, sum=161.808 (6)",
            "tab": "Summarization metrics",
            "score": 26.967920888770376
          },
          "CNN/DailyMail - Compression": {
            "description": "min=9.708, mean=10.317, max=10.928, sum=61.905 (6)",
            "tab": "Summarization metrics",
            "score": 10.317434111699901
          },
          "CNN/DailyMail - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "CNN/DailyMail - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "XSUM - ROUGE-2",
      "source_data": {
        "dataset_name": "XSUM",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).\n\nROUGE-2: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.169,
        "details": {
          "description": "min=0.162, mean=0.169, max=0.172, sum=1.013 (6)",
          "tab": "Accuracy",
          "XSUM - Denoised inference time (s)": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "XSUM - # eval": {
            "description": "min=518, mean=518, max=518, sum=3108 (6)",
            "tab": "General information",
            "score": 518.0
          },
          "XSUM - # train": {
            "description": "min=4.998, mean=4.999, max=5, sum=29.992 (6)",
            "tab": "General information",
            "score": 4.998712998712999
          },
          "XSUM - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (6)",
            "tab": "General information",
            "score": 0.0
          },
          "XSUM - # prompt tokens": {
            "description": "min=1456.402, mean=1510.418, max=1538.921, sum=9062.51 (6)",
            "tab": "General information",
            "score": 1510.4182754182755
          },
          "XSUM - # output tokens": {
            "description": "min=27.172, mean=27.501, max=27.815, sum=165.008 (6)",
            "tab": "General information",
            "score": 27.501287001287
          },
          "XSUM - # trials": {
            "description": "min=3, mean=3, max=3, sum=18 (6)",
            "tab": "General information",
            "score": 3.0
          },
          "XSUM - Stereotypes (race)": {
            "description": "min=0.667, mean=0.667, max=0.667, sum=4 (6)",
            "tab": "Bias",
            "score": 0.6666666666666666
          },
          "XSUM - Stereotypes (gender)": {
            "description": "min=0.443, mean=0.449, max=0.459, sum=2.696 (6)",
            "tab": "Bias",
            "score": 0.4493607590885817
          },
          "XSUM - Representation (race)": {
            "description": "min=0.362, mean=0.486, max=0.567, sum=2.914 (6)",
            "tab": "Bias",
            "score": 0.4857302118171683
          },
          "XSUM - Representation (gender)": {
            "description": "min=0.195, mean=0.204, max=0.217, sum=1.223 (6)",
            "tab": "Bias",
            "score": 0.2037662889603199
          },
          "XSUM - Toxic fraction": {
            "description": "min=0, mean=0.003, max=0.004, sum=0.015 (6)",
            "tab": "Toxicity",
            "score": 0.002574002574002574
          },
          "XSUM - SummaC": {
            "description": "min=-0.297, mean=-0.281, max=-0.266, sum=-0.842 (3)",
            "tab": "Summarization metrics",
            "score": -0.2807751739040458
          },
          "XSUM - QAFactEval": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - BERTScore (F1)": {
            "description": "min=0.472, mean=0.473, max=0.476, sum=1.42 (3)",
            "tab": "Summarization metrics",
            "score": 0.4734549353569219
          },
          "XSUM - Coverage": {
            "description": "min=0.772, mean=0.774, max=0.777, sum=4.641 (6)",
            "tab": "Summarization metrics",
            "score": 0.7735373951395458
          },
          "XSUM - Density": {
            "description": "min=2.174, mean=2.322, max=2.471, sum=13.929 (6)",
            "tab": "Summarization metrics",
            "score": 2.321577703631062
          },
          "XSUM - Compression": {
            "description": "min=15.596, mean=15.776, max=15.931, sum=94.655 (6)",
            "tab": "Summarization metrics",
            "score": 15.775903485860036
          },
          "XSUM - HumanEval-faithfulness": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-relevance": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          },
          "XSUM - HumanEval-coherence": {
            "description": "2 matching runs, but no matching metrics",
            "tab": "Summarization metrics",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "IMDB - EM",
      "source_data": {
        "dataset_name": "IMDB",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.941,
        "details": {
          "description": "min=0.939, mean=0.941, max=0.942, sum=2.822 (3)",
          "tab": "Accuracy",
          "IMDB - ECE (10-bin)": {
            "description": "min=0.065, mean=0.087, max=0.106, sum=0.262 (3)",
            "tab": "Calibration",
            "score": 0.08729270886734875
          },
          "IMDB - EM (Robustness)": {
            "description": "min=0.92, mean=0.921, max=0.922, sum=2.763 (3)",
            "tab": "Robustness",
            "score": 0.9210000000000002
          },
          "IMDB - EM (Fairness)": {
            "description": "min=0.933, mean=0.936, max=0.94, sum=2.807 (3)",
            "tab": "Fairness",
            "score": 0.9356666666666666
          },
          "IMDB - Denoised inference time (s)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "IMDB - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=3000 (3)",
            "tab": "General information",
            "score": 1000.0
          },
          "IMDB - # train": {
            "description": "min=4.845, mean=4.932, max=4.985, sum=14.796 (3)",
            "tab": "General information",
            "score": 4.9319999999999995
          },
          "IMDB - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (3)",
            "tab": "General information",
            "score": 0.0
          },
          "IMDB - # prompt tokens": {
            "description": "min=1152.524, mean=1389.183, max=1743.988, sum=4167.55 (3)",
            "tab": "General information",
            "score": 1389.1833333333332
          },
          "IMDB - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=3 (3)",
            "tab": "General information",
            "score": 1.0
          },
          "IMDB - # trials": {
            "description": "min=3, mean=3, max=3, sum=9 (3)",
            "tab": "General information",
            "score": 3.0
          },
          "IMDB - Stereotypes (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Stereotypes (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (race)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Representation (gender)": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Bias",
            "score": null
          },
          "IMDB - Toxic fraction": {
            "description": "1 matching runs, but no matching metrics",
            "tab": "Toxicity",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "CivilComments - EM",
      "source_data": {
        "dataset_name": "CivilComments",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/pdf/1903.04561.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.601,
        "details": {
          "description": "min=0.171, mean=0.601, max=0.983, sum=32.472 (54)",
          "tab": "Accuracy",
          "CivilComments - ECE (10-bin)": {
            "description": "min=0.058, mean=0.213, max=0.447, sum=11.516 (54)",
            "tab": "Calibration",
            "score": 0.2132557883443423
          },
          "CivilComments - EM (Robustness)": {
            "description": "min=0.069, mean=0.409, max=0.689, sum=22.106 (54)",
            "tab": "Robustness",
            "score": 0.4093704023963013
          },
          "CivilComments - EM (Fairness)": {
            "description": "min=0.047, mean=0.48, max=0.97, sum=25.944 (54)",
            "tab": "Fairness",
            "score": 0.48044223702694133
          },
          "CivilComments - Denoised inference time (s)": {
            "description": "9 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "CivilComments - # eval": {
            "description": "min=74, mean=371.556, max=683, sum=20064 (54)",
            "tab": "General information",
            "score": 371.55555555555554
          },
          "CivilComments - # train": {
            "description": "min=5, mean=5, max=5, sum=270 (54)",
            "tab": "General information",
            "score": 5.0
          },
          "CivilComments - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "General information",
            "score": 0.0
          },
          "CivilComments - # prompt tokens": {
            "description": "min=356.537, mean=722.635, max=1267.519, sum=39022.317 (54)",
            "tab": "General information",
            "score": 722.6354931173206
          },
          "CivilComments - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=54 (54)",
            "tab": "General information",
            "score": 1.0
          },
          "CivilComments - # trials": {
            "description": "min=3, mean=3, max=3, sum=162 (54)",
            "tab": "General information",
            "score": 3.0
          },
          "CivilComments - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "CivilComments - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (54)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "RAFT - EM",
      "source_data": {
        "dataset_name": "RAFT",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/benchmark_output/releases/v0.4.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.679,
        "details": {
          "description": "min=0.025, mean=0.679, max=0.975, sum=22.4 (33)",
          "tab": "Accuracy",
          "RAFT - ECE (10-bin)": {
            "description": "min=0.089, mean=0.244, max=0.908, sum=8.049 (33)",
            "tab": "Calibration",
            "score": 0.24392205141094134
          },
          "RAFT - EM (Robustness)": {
            "description": "min=0, mean=0.545, max=0.85, sum=17.975 (33)",
            "tab": "Robustness",
            "score": 0.5446969696969698
          },
          "RAFT - EM (Fairness)": {
            "description": "min=0.025, mean=0.644, max=0.975, sum=21.25 (33)",
            "tab": "Fairness",
            "score": 0.6439393939393939
          },
          "RAFT - Denoised inference time (s)": {
            "description": "11 matching runs, but no matching metrics",
            "tab": "Efficiency",
            "score": null
          },
          "RAFT - # eval": {
            "description": "min=40, mean=40, max=40, sum=1320 (33)",
            "tab": "General information",
            "score": 40.0
          },
          "RAFT - # train": {
            "description": "min=0, mean=4.556, max=5, sum=150.35 (33)",
            "tab": "General information",
            "score": 4.556060606060607
          },
          "RAFT - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "General information",
            "score": 0.0
          },
          "RAFT - # prompt tokens": {
            "description": "min=257.35, mean=812.938, max=1773.675, sum=26826.95 (33)",
            "tab": "General information",
            "score": 812.937878787879
          },
          "RAFT - # output tokens": {
            "description": "min=0.15, mean=3.023, max=6.625, sum=99.75 (33)",
            "tab": "General information",
            "score": 3.022727272727273
          },
          "RAFT - # trials": {
            "description": "min=3, mean=3, max=3, sum=99 (33)",
            "tab": "General information",
            "score": 3.0
          },
          "RAFT - Stereotypes (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Stereotypes (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (race)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Representation (gender)": {
            "description": "(0)",
            "tab": "Bias",
            "score": null
          },
          "RAFT - Toxic fraction": {
            "description": "min=0, mean=0, max=0, sum=0 (33)",
            "tab": "Toxicity",
            "score": 0.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}