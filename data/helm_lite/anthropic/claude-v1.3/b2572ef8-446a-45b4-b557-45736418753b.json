{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_lite/anthropic_claude-v1.3/1770834614.1822479",
  "retrieved_timestamp": "1770834614.1822479",
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Claude v1.3",
    "id": "anthropic/claude-v1.3",
    "developer": "anthropic",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_lite",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.518,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.09352059925093632
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.723,
        "details": {
          "description": "min=0.723, mean=0.723, max=0.723, sum=0.723 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=6.114, mean=6.114, max=6.114, sum=6.114 (1)",
            "tab": "Efficiency",
            "score": 6.113923052666893
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3709.741, mean=3709.741, max=3709.741, sum=3709.741 (1)",
            "tab": "General information",
            "score": 3709.7408450704224
          },
          "NarrativeQA - # output tokens": {
            "description": "min=9.338, mean=9.338, max=9.338, sum=9.338 (1)",
            "tab": "General information",
            "score": 9.338028169014084
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (closed-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (closed-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.409,
        "details": {
          "description": "min=0.409, mean=0.409, max=0.409, sum=0.409 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=3.523, mean=3.523, max=3.523, sum=3.523 (1)",
            "tab": "Efficiency",
            "score": 3.5226667501174913
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=2.059, mean=2.059, max=2.059, sum=2.059 (1)",
            "tab": "Efficiency",
            "score": 2.0589215233325957
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.964, mean=4.964, max=4.964, sum=4.964 (1)",
            "tab": "General information",
            "score": 4.964
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.007, mean=0.007, max=0.007, sum=0.007 (1)",
            "tab": "General information",
            "score": 0.007
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1734.363, mean=1734.363, max=1734.363, sum=1734.363 (1)",
            "tab": "General information",
            "score": 1734.363
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=4.973, mean=4.973, max=4.973, sum=4.973 (1)",
            "tab": "General information",
            "score": 4.973
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=189.259, mean=189.259, max=189.259, sum=189.259 (1)",
            "tab": "General information",
            "score": 189.259
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=3.722, mean=3.722, max=3.722, sum=3.722 (1)",
            "tab": "General information",
            "score": 3.722
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "mode": "closedbook"
        }
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.908,
        "details": {
          "description": "min=0.908, mean=0.908, max=0.908, sum=0.908 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=3.375, mean=3.375, max=3.375, sum=3.375 (1)",
            "tab": "Efficiency",
            "score": 3.375496371269226
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=328.79, mean=328.79, max=328.79, sum=328.79 (1)",
            "tab": "General information",
            "score": 328.79
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "dataset": "openbookqa",
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.631,
        "details": {
          "description": "min=0.35, mean=0.631, max=0.93, sum=3.155 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=1.228, mean=1.482, max=1.741, sum=7.41 (5)",
            "tab": "Efficiency",
            "score": 1.4820951028288456
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=435.26, mean=543.747, max=684.596, sum=2718.736 (5)",
            "tab": "General information",
            "score": 543.747298245614
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "college_chemistry",
            "computer_security",
            "econometrics",
            "us_foreign_policy"
          ],
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MATH",
      "source_data": {
        "dataset_name": "MATH",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "Equivalent (CoT) on MATH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.54,
        "details": {
          "description": "min=0.368, mean=0.54, max=0.826, sum=3.783 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=3.85, mean=6.109, max=8.225, sum=42.762 (7)",
            "tab": "Efficiency",
            "score": 6.10879439056091
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=947.259, mean=1361.814, max=2379.808, sum=9532.699 (7)",
            "tab": "General information",
            "score": 1361.8141219676104
          },
          "MATH - # output tokens": {
            "description": "min=53.133, mean=79.493, max=97.564, sum=556.452 (7)",
            "tab": "General information",
            "score": 79.49312981320325
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "algebra",
            "counting_and_probability",
            "geometry",
            "intermediate_algebra",
            "number_theory",
            "prealgebra",
            "precalculus"
          ],
          "level": "1",
          "use_official_examples": "False",
          "use_chain_of_thought": "True"
        }
      }
    },
    {
      "evaluation_name": "GSM8K",
      "source_data": {
        "dataset_name": "GSM8K",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on GSM8K",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.784,
        "details": {
          "description": "min=0.784, mean=0.784, max=0.784, sum=0.784 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=6.653, mean=6.653, max=6.653, sum=6.653 (1)",
            "tab": "Efficiency",
            "score": 6.653211696863174
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=1012.712, mean=1012.712, max=1012.712, sum=1012.712 (1)",
            "tab": "General information",
            "score": 1012.712
          },
          "GSM8K - # output tokens": {
            "description": "min=104.726, mean=104.726, max=104.726, sum=104.726 (1)",
            "tab": "General information",
            "score": 104.726
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "LegalBench",
      "source_data": {
        "dataset_name": "LegalBench",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on LegalBench",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.629,
        "details": {
          "description": "min=0.417, mean=0.629, max=0.916, sum=3.147 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=1.081, mean=3.536, max=8.614, sum=17.681 (5)",
            "tab": "Efficiency",
            "score": 3.536136101917547
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=4, mean=4.798, max=5, sum=23.99 (5)",
            "tab": "General information",
            "score": 4.797959183673469
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "LegalBench - # prompt tokens": {
            "description": "min=280.653, mean=1621.356, max=6484.969, sum=8106.779 (5)",
            "tab": "General information",
            "score": 1621.3558670820687
          },
          "LegalBench - # output tokens": {
            "description": "min=1, mean=1.354, max=2.232, sum=6.771 (5)",
            "tab": "General information",
            "score": 1.3542176968306323
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": [
            "abercrombie",
            "corporate_lobbying",
            "function_of_decision_section",
            "international_citizenship_questions",
            "proa"
          ]
        }
      }
    },
    {
      "evaluation_name": "MedQA",
      "source_data": {
        "dataset_name": "MedQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MedQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.618,
        "details": {
          "description": "min=0.618, mean=0.618, max=0.618, sum=0.618 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=3.39, mean=3.39, max=3.39, sum=3.39 (1)",
            "tab": "Efficiency",
            "score": 3.3901417141643244
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1092.437, mean=1092.437, max=1092.437, sum=1092.437 (1)",
            "tab": "General information",
            "score": 1092.4373757455269
          },
          "MedQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "WMT 2014",
      "source_data": {
        "dataset_name": "WMT 2014",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "BLEU-4 on WMT 2014",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.219,
        "details": {
          "description": "min=0.152, mean=0.219, max=0.28, sum=1.093 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=1.391, mean=2.232, max=3.755, sum=11.161 (5)",
            "tab": "Efficiency",
            "score": 2.232213549153336
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=197.406, mean=218.573, max=240.974, sum=1092.866 (5)",
            "tab": "General information",
            "score": 218.57322077152472
          },
          "WMT 2014 - # output tokens": {
            "description": "min=24.004, mean=25.611, max=26.28, sum=128.057 (5)",
            "tab": "General information",
            "score": 25.611364027374215
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "language_pair": [
            "cs-en",
            "de-en",
            "fr-en",
            "hi-en",
            "ru-en"
          ]
        }
      }
    }
  ]
}