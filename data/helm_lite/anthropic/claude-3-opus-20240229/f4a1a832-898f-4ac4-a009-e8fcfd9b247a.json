{
  "schema_version": "0.0.1",
  "evaluation_id": "helm_lite/anthropic_claude-3-opus-20240229/1765639040.409865",
  "retrieved_timestamp": "1765639040.409865",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Claude 3 Opus (20240229)",
    "id": "anthropic/claude-3-opus-20240229",
    "developer": "anthropic",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.683,
        "details": {
          "description": null,
          "tab": "Accuracy"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.351,
        "details": {
          "description": "min=0.351, mean=0.351, max=0.351, sum=0.351 (1)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.264,
        "details": {
          "description": "min=0.264, mean=0.264, max=0.264, sum=0.264 (1)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.441,
        "details": {
          "description": "min=0.441, mean=0.441, max=0.441, sum=0.441 (1)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.956,
        "details": {
          "description": "min=0.956, mean=0.956, max=0.956, sum=0.956 (1)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.768,
        "details": {
          "description": "min=0.6, mean=0.768, max=0.96, sum=3.839 (5)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MATH - Equivalent (CoT)",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\nEquivalent (CoT): Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thought prompting.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.76,
        "details": {
          "description": "min=0.526, mean=0.76, max=0.889, sum=5.322 (7)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "GSM8K - EM",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nExact match (final number): Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.924,
        "details": {
          "description": "min=0.924, mean=0.924, max=0.924, sum=0.924 (1)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "LegalBench - EM",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.662,
        "details": {
          "description": "min=0.153, mean=0.662, max=0.989, sum=3.31 (5)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "MedQA - EM",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.775,
        "details": {
          "description": "min=0.775, mean=0.775, max=0.775, sum=0.775 (1)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "WMT 2014 - BLEU-4",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\nBLEU-4: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 4-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.24,
        "details": {
          "description": "min=0.188, mean=0.24, max=0.285, sum=1.199 (5)",
          "tab": "Accuracy"
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.091,
        "details": {
          "description": null,
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 42.0
      },
      "score_details": {
        "score": 3.996,
        "details": {
          "description": "min=3.996, mean=3.996, max=3.996, sum=3.996 (1)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 69.0
      },
      "score_details": {
        "score": 4.273,
        "details": {
          "description": "min=4.273, mean=4.273, max=4.273, sum=4.273 (1)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 64.0
      },
      "score_details": {
        "score": 1.647,
        "details": {
          "description": "min=1.647, mean=1.647, max=1.647, sum=1.647 (1)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "OpenbookQA - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 2.168,
        "details": {
          "description": "min=2.168, mean=2.168, max=2.168, sum=2.168 (1)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 13.0
      },
      "score_details": {
        "score": 4.19,
        "details": {
          "description": "min=4.003, mean=4.19, max=4.373, sum=20.948 (5)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MATH - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 72.0
      },
      "score_details": {
        "score": 7.542,
        "details": {
          "description": "min=6.095, mean=7.542, max=9.041, sum=52.793 (7)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "GSM8K - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 75.0
      },
      "score_details": {
        "score": 7.469,
        "details": {
          "description": "min=7.469, mean=7.469, max=7.469, sum=7.469 (1)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "LegalBench - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 9.0
      },
      "score_details": {
        "score": 2.57,
        "details": {
          "description": "min=1.391, mean=2.57, max=4.856, sum=12.851 (5)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "MedQA - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 2.65,
        "details": {
          "description": "min=2.65, mean=2.65, max=2.65, sum=2.65 (1)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "WMT 2014 - Observed inference time (s)",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "lower_is_better": true,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 21.0
      },
      "score_details": {
        "score": 2.447,
        "details": {
          "description": "min=2.279, mean=2.447, max=2.661, sum=12.233 (5)",
          "tab": "Efficiency"
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": null,
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - # eval",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 355.0
      },
      "score_details": {
        "score": 355.0,
        "details": {
          "description": "min=355, mean=355, max=355, sum=355 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - # train",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - truncated",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 4415.0
      },
      "score_details": {
        "score": 3709.741,
        "details": {
          "description": "min=3709.741, mean=3709.741, max=3709.741, sum=3709.741 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - # output tokens",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 45.0
      },
      "score_details": {
        "score": 13.589,
        "details": {
          "description": "min=13.589, mean=13.589, max=13.589, sum=13.589 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - # eval",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1000.0
      },
      "score_details": {
        "score": 1000.0,
        "details": {
          "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - # train",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - truncated",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 2538.0
      },
      "score_details": {
        "score": 1781.799,
        "details": {
          "description": "min=1781.799, mean=1781.799, max=1781.799, sum=1781.799 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - # output tokens",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 40.0
      },
      "score_details": {
        "score": 39.248,
        "details": {
          "description": "min=39.248, mean=39.248, max=39.248, sum=39.248 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - # eval",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1000.0
      },
      "score_details": {
        "score": 1000.0,
        "details": {
          "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - # train",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - truncated",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 212.0
      },
      "score_details": {
        "score": 189.259,
        "details": {
          "description": "min=189.259, mean=189.259, max=189.259, sum=189.259 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - # output tokens",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 51.0
      },
      "score_details": {
        "score": 5.66,
        "details": {
          "description": "min=5.66, mean=5.66, max=5.66, sum=5.66 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "OpenbookQA - # eval",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 500.0
      },
      "score_details": {
        "score": 500.0,
        "details": {
          "description": "min=500, mean=500, max=500, sum=500 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "OpenbookQA - # train",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "OpenbookQA - truncated",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "OpenbookQA - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 329.0
      },
      "score_details": {
        "score": 263.79,
        "details": {
          "description": "min=263.79, mean=263.79, max=263.79, sum=263.79 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "OpenbookQA - # output tokens",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - # eval",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 103.0
      },
      "score_details": {
        "score": 102.8,
        "details": {
          "description": "min=100, mean=102.8, max=114, sum=514 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - # train",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=25 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - truncated",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 544.0
      },
      "score_details": {
        "score": 478.747,
        "details": {
          "description": "min=370.26, mean=478.747, max=619.596, sum=2393.736 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - # output tokens",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=5 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MATH - # eval",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 63.0
      },
      "score_details": {
        "score": 62.429,
        "details": {
          "description": "min=30, mean=62.429, max=135, sum=437 (7)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "MATH - # train",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 8.0
      },
      "score_details": {
        "score": 8.0,
        "details": {
          "description": "min=8, mean=8, max=8, sum=56 (7)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "MATH - truncated",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (7)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "MATH - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1526.0
      },
      "score_details": {
        "score": 1362.814,
        "details": {
          "description": "min=948.259, mean=1362.814, max=2380.808, sum=9539.699 (7)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "MATH - # output tokens",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 373.0
      },
      "score_details": {
        "score": 113.906,
        "details": {
          "description": "min=82.965, mean=113.906, max=138.263, sum=797.345 (7)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "GSM8K - # eval",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1000.0
      },
      "score_details": {
        "score": 1000.0,
        "details": {
          "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "GSM8K - # train",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "GSM8K - truncated",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "GSM8K - # prompt tokens",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1256.0
      },
      "score_details": {
        "score": 1012.712,
        "details": {
          "description": "min=1012.712, mean=1012.712, max=1012.712, sum=1012.712 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "GSM8K - # output tokens",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 400.0
      },
      "score_details": {
        "score": 115.934,
        "details": {
          "description": "min=115.934, mean=115.934, max=115.934, sum=115.934 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "LegalBench - # eval",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 410.0
      },
      "score_details": {
        "score": 409.4,
        "details": {
          "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "LegalBench - # train",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 4.8,
        "details": {
          "description": "min=4, mean=4.8, max=5, sum=24 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "LegalBench - truncated",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "LegalBench - # prompt tokens",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1840.0
      },
      "score_details": {
        "score": 1557.242,
        "details": {
          "description": "min=214.653, mean=1557.242, max=6428.398, sum=7786.208 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "LegalBench - # output tokens",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 13.0
      },
      "score_details": {
        "score": 1.605,
        "details": {
          "description": "min=1, mean=1.605, max=2.932, sum=8.023 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "MedQA - # eval",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 503.0
      },
      "score_details": {
        "score": 503.0,
        "details": {
          "description": "min=503, mean=503, max=503, sum=503 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MedQA - # train",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 5.0
      },
      "score_details": {
        "score": 5.0,
        "details": {
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MedQA - truncated",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MedQA - # prompt tokens",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1244.0
      },
      "score_details": {
        "score": 1027.437,
        "details": {
          "description": "min=1027.437, mean=1027.437, max=1027.437, sum=1027.437 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MedQA - # output tokens",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "tab": "General information"
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "WMT 2014 - # eval",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\n# eval: Number of evaluation instances.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 586.0
      },
      "score_details": {
        "score": 568.8,
        "details": {
          "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    },
    {
      "evaluation_name": "WMT 2014 - # train",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\n# train: Number of training instances (e.g., in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 1.0,
        "details": {
          "description": "min=1, mean=1, max=1, sum=5 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    },
    {
      "evaluation_name": "WMT 2014 - truncated",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": -1.0,
        "details": {
          "description": "min=0, mean=0, max=0, sum=0 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    },
    {
      "evaluation_name": "WMT 2014 - # prompt tokens",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\n# prompt tokens: Number of tokens in the prompt.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 221.0
      },
      "score_details": {
        "score": 219.573,
        "details": {
          "description": "min=198.406, mean=219.573, max=241.974, sum=1097.866 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    },
    {
      "evaluation_name": "WMT 2014 - # output tokens",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\n# output tokens: Actual number of output tokens.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 100.0
      },
      "score_details": {
        "score": 25.837,
        "details": {
          "description": "min=24.332, mean=25.837, max=26.616, sum=129.185 (5)",
          "tab": "General information"
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    }
  ]
}