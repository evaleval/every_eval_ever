{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_lite/writer_palmyra-x-004/1767655940.500768",
  "retrieved_timestamp": "1767655940.500768",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Palmyra-X-004",
    "id": "writer/palmyra-x-004",
    "developer": "writer",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.808,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.4045318352059925
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.773,
        "details": {
          "description": "min=0.773, mean=0.773, max=0.773, sum=0.773 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=1.634, mean=1.634, max=1.634, sum=1.634 (1)",
            "tab": "Efficiency",
            "score": 1.634409177135414
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3484.268, mean=3484.268, max=3484.268, sum=3484.268 (1)",
            "tab": "General information",
            "score": 3484.2676056338028
          },
          "NarrativeQA - # output tokens": {
            "description": "min=6.338, mean=6.338, max=6.338, sum=6.338 (1)",
            "tab": "General information",
            "score": 6.338028169014085
          }
        }
      },
      "generation_config": {
        "stop": "none"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.457,
        "details": {
          "description": "min=0.457, mean=0.457, max=0.457, sum=0.457 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=1.221, mean=1.221, max=1.221, sum=1.221 (1)",
            "tab": "Efficiency",
            "score": 1.22119681596756
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=1.213, mean=1.213, max=1.213, sum=1.213 (1)",
            "tab": "Efficiency",
            "score": 1.2129934797286988
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.965, mean=4.965, max=4.965, sum=4.965 (1)",
            "tab": "General information",
            "score": 4.965
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.007, mean=0.007, max=0.007, sum=0.007 (1)",
            "tab": "General information",
            "score": 0.007
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1675.231, mean=1675.231, max=1675.231, sum=1675.231 (1)",
            "tab": "General information",
            "score": 1675.231
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=10.295, mean=10.295, max=10.295, sum=10.295 (1)",
            "tab": "General information",
            "score": 10.295
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=129.12, mean=129.12, max=129.12, sum=129.12 (1)",
            "tab": "General information",
            "score": 129.12
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=12.549, mean=12.549, max=12.549, sum=12.549 (1)",
            "tab": "General information",
            "score": 12.549
          }
        }
      },
      "generation_config": {
        "mode": "closedbook",
        "stop": "none"
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.926,
        "details": {
          "description": "min=0.926, mean=0.926, max=0.926, sum=0.926 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=0.271, mean=0.271, max=0.271, sum=0.271 (1)",
            "tab": "Efficiency",
            "score": 0.2705215420722961
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=249.776, mean=249.776, max=249.776, sum=249.776 (1)",
            "tab": "General information",
            "score": 249.776
          },
          "OpenbookQA - # output tokens": {
            "description": "min=0.992, mean=0.992, max=0.992, sum=0.992 (1)",
            "tab": "General information",
            "score": 0.992
          }
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.739,
        "details": {
          "description": "min=0.52, mean=0.739, max=0.92, sum=3.694 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=0.309, mean=0.396, max=0.722, sum=1.982 (5)",
            "tab": "Efficiency",
            "score": 0.39635124337045774
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=373.43, mean=467.686, max=614.421, sum=2338.431 (5)",
            "tab": "General information",
            "score": 467.6862105263158
          },
          "MMLU - # output tokens": {
            "description": "min=0.97, mean=0.99, max=1, sum=4.951 (5)",
            "tab": "General information",
            "score": 0.9902456140350877
          }
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MATH - Equivalent (CoT)",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\nEquivalent (CoT): Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thought prompting.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.767,
        "details": {
          "description": "min=0.553, mean=0.767, max=0.948, sum=5.371 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=5.13, mean=14.827, max=45.729, sum=103.786 (7)",
            "tab": "Efficiency",
            "score": 14.82662017363065
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=881.363, mean=1262.909, max=2197.577, sum=8840.364 (7)",
            "tab": "General information",
            "score": 1262.9092130545007
          },
          "MATH - # output tokens": {
            "description": "min=174.547, mean=209.333, max=238.692, sum=1465.33 (7)",
            "tab": "General information",
            "score": 209.3327932233685
          }
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True",
        "stop": "none"
      }
    },
    {
      "evaluation_name": "GSM8K - EM",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nExact match (final number): Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.905,
        "details": {
          "description": "min=0.905, mean=0.905, max=0.905, sum=0.905 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=11.45, mean=11.45, max=11.45, sum=11.45 (1)",
            "tab": "Efficiency",
            "score": 11.449529441833496
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=959.032, mean=959.032, max=959.032, sum=959.032 (1)",
            "tab": "General information",
            "score": 959.032
          },
          "GSM8K - # output tokens": {
            "description": "min=174.327, mean=174.327, max=174.327, sum=174.327 (1)",
            "tab": "General information",
            "score": 174.327
          }
        }
      },
      "generation_config": {
        "stop": "none"
      }
    },
    {
      "evaluation_name": "LegalBench - EM",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.73,
        "details": {
          "description": "min=0.433, mean=0.73, max=0.989, sum=3.648 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=0.478, mean=0.504, max=0.522, sum=2.519 (5)",
            "tab": "Efficiency",
            "score": 0.5037181089898329
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=4, mean=4.798, max=5, sum=23.992 (5)",
            "tab": "General information",
            "score": 4.798367346938775
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "LegalBench - # prompt tokens": {
            "description": "min=216.442, mean=1524.207, max=6297.633, sum=7621.033 (5)",
            "tab": "General information",
            "score": 1524.206501356544
          },
          "LegalBench - # output tokens": {
            "description": "min=1, mean=1.416, max=2.021, sum=7.082 (5)",
            "tab": "General information",
            "score": 1.4163162483866343
          }
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ],
        "stop": "none"
      }
    },
    {
      "evaluation_name": "MedQA - EM",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.775,
        "details": {
          "description": "min=0.775, mean=0.775, max=0.775, sum=0.775 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=0.399, mean=0.399, max=0.399, sum=0.399 (1)",
            "tab": "Efficiency",
            "score": 0.39942375139498093
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1025.274, mean=1025.274, max=1025.274, sum=1025.274 (1)",
            "tab": "General information",
            "score": 1025.2743538767395
          },
          "MedQA - # output tokens": {
            "description": "min=0.992, mean=0.992, max=0.992, sum=0.992 (1)",
            "tab": "General information",
            "score": 0.9920477137176938
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "WMT 2014 - BLEU-4",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\nBLEU-4: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 4-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.203,
        "details": {
          "description": "min=0.144, mean=0.203, max=0.249, sum=1.016 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=1.801, mean=2.046, max=2.515, sum=10.228 (5)",
            "tab": "Efficiency",
            "score": 2.045695114985284
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=96.139, mean=115.712, max=136.117, sum=578.559 (5)",
            "tab": "General information",
            "score": 115.71178123566294
          },
          "WMT 2014 - # output tokens": {
            "description": "min=26.191, mean=29.362, max=37.718, sum=146.808 (5)",
            "tab": "General information",
            "score": 29.36160106667686
          }
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ],
        "stop": "none"
      }
    }
  ]
}