{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_lite/amazon_nova-pro-v1:0/1770834614.1822479",
  "retrieved_timestamp": "1770834614.1822479",
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Amazon Nova Pro",
    "id": "amazon/nova-pro-v1:0",
    "developer": "amazon",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_lite",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.885,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.9342571785268414
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.791,
        "details": {
          "description": "min=0.791, mean=0.791, max=0.791, sum=0.791 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=0.246, mean=0.246, max=0.246, sum=0.246 (1)",
            "tab": "Efficiency",
            "score": 0.24631830985915482
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3550.577, mean=3550.577, max=3550.577, sum=3550.577 (1)",
            "tab": "General information",
            "score": 3550.5774647887324
          },
          "NarrativeQA - # output tokens": {
            "description": "min=4.651, mean=4.651, max=4.651, sum=4.651 (1)",
            "tab": "General information",
            "score": 4.650704225352112
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (closed-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (closed-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.405,
        "details": {
          "description": "min=0.405, mean=0.405, max=0.405, sum=0.405 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=0.266, mean=0.266, max=0.266, sum=0.266 (1)",
            "tab": "Efficiency",
            "score": 0.26591999999999993
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=0.203, mean=0.203, max=0.203, sum=0.203 (1)",
            "tab": "Efficiency",
            "score": 0.203244
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1773.944, mean=1773.944, max=1773.944, sum=1773.944 (1)",
            "tab": "General information",
            "score": 1773.944
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=5.646, mean=5.646, max=5.646, sum=5.646 (1)",
            "tab": "General information",
            "score": 5.646
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=153.254, mean=153.254, max=153.254, sum=153.254 (1)",
            "tab": "General information",
            "score": 153.254
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=4.207, mean=4.207, max=4.207, sum=4.207 (1)",
            "tab": "General information",
            "score": 4.207
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "mode": "closedbook"
        }
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.96,
        "details": {
          "description": "min=0.96, mean=0.96, max=0.96, sum=0.96 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=0.129, mean=0.129, max=0.129, sum=0.129 (1)",
            "tab": "Efficiency",
            "score": 0.12889800000000004
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=282.21, mean=282.21, max=282.21, sum=282.21 (1)",
            "tab": "General information",
            "score": 282.21
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "dataset": "openbookqa",
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.758,
        "details": {
          "description": "min=0.63, mean=0.758, max=0.93, sum=3.792 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=0.141, mean=0.145, max=0.152, sum=0.725 (5)",
            "tab": "Efficiency",
            "score": 0.1449304210526316
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=399.38, mean=500.274, max=652.07, sum=2501.37 (5)",
            "tab": "General information",
            "score": 500.2740350877192
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "college_chemistry",
            "computer_security",
            "econometrics",
            "us_foreign_policy"
          ],
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MATH",
      "source_data": {
        "dataset_name": "MATH",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "Equivalent (CoT) on MATH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.821,
        "details": {
          "description": "min=0.7, mean=0.821, max=0.93, sum=5.749 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=1.139, mean=1.695, max=2.518, sum=11.863 (7)",
            "tab": "Efficiency",
            "score": 1.6947358347418935
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=925.556, mean=1394.735, max=2468.942, sum=9763.147 (7)",
            "tab": "General information",
            "score": 1394.7353092779651
          },
          "MATH - # output tokens": {
            "description": "min=66.088, mean=98.114, max=154.135, sum=686.8 (7)",
            "tab": "General information",
            "score": 98.11425246180445
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "algebra",
            "counting_and_probability",
            "geometry",
            "intermediate_algebra",
            "number_theory",
            "prealgebra",
            "precalculus"
          ],
          "level": "1",
          "use_official_examples": "False",
          "use_chain_of_thought": "True"
        }
      }
    },
    {
      "evaluation_name": "GSM8K",
      "source_data": {
        "dataset_name": "GSM8K",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on GSM8K",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.87,
        "details": {
          "description": "min=0.87, mean=0.87, max=0.87, sum=0.87 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=1.566, mean=1.566, max=1.566, sum=1.566 (1)",
            "tab": "Efficiency",
            "score": 1.5656869999999996
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=957.869, mean=957.869, max=957.869, sum=957.869 (1)",
            "tab": "General information",
            "score": 957.869
          },
          "GSM8K - # output tokens": {
            "description": "min=73.847, mean=73.847, max=73.847, sum=73.847 (1)",
            "tab": "General information",
            "score": 73.847
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "stop": "none"
        }
      }
    },
    {
      "evaluation_name": "LegalBench",
      "source_data": {
        "dataset_name": "LegalBench",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on LegalBench",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.736,
        "details": {
          "description": "min=0.444, mean=0.736, max=0.958, sum=3.681 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=0.139, mean=0.166, max=0.232, sum=0.83 (5)",
            "tab": "Efficiency",
            "score": 0.16605967288111284
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=4, mean=4.8, max=5, sum=24 (5)",
            "tab": "General information",
            "score": 4.8
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "LegalBench - # prompt tokens": {
            "description": "min=241.632, mean=1581.083, max=6449.798, sum=7905.414 (5)",
            "tab": "General information",
            "score": 1581.0827222540588
          },
          "LegalBench - # output tokens": {
            "description": "min=1, mean=1.387, max=2.358, sum=6.936 (5)",
            "tab": "General information",
            "score": 1.3871102825182848
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": [
            "abercrombie",
            "corporate_lobbying",
            "function_of_decision_section",
            "international_citizenship_questions",
            "proa"
          ],
          "stop": "none"
        }
      }
    },
    {
      "evaluation_name": "MedQA",
      "source_data": {
        "dataset_name": "MedQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MedQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.811,
        "details": {
          "description": "min=0.811, mean=0.811, max=0.811, sum=0.811 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=0.142, mean=0.142, max=0.142, sum=0.142 (1)",
            "tab": "Efficiency",
            "score": 0.14219284294234621
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1066.861, mean=1066.861, max=1066.861, sum=1066.861 (1)",
            "tab": "General information",
            "score": 1066.8608349900596
          },
          "MedQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "WMT 2014",
      "source_data": {
        "dataset_name": "WMT 2014",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "BLEU-4 on WMT 2014",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.229,
        "details": {
          "description": "min=0.184, mean=0.229, max=0.281, sum=1.144 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=0.483, mean=0.504, max=0.519, sum=2.52 (5)",
            "tab": "Efficiency",
            "score": 0.5040968109611562
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=163.93, mean=208.694, max=268.662, sum=1043.469 (5)",
            "tab": "General information",
            "score": 208.69386660804403
          },
          "WMT 2014 - # output tokens": {
            "description": "min=23.903, mean=25.328, max=25.92, sum=126.641 (5)",
            "tab": "General information",
            "score": 25.32825594509864
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "language_pair": [
            "cs-en",
            "de-en",
            "fr-en",
            "hi-en",
            "ru-en"
          ]
        }
      }
    }
  ]
}