{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_lite/snowflake_snowflake-arctic-instruct/1770834614.1822479",
  "retrieved_timestamp": "1770834614.1822479",
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Arctic Instruct",
    "id": "snowflake/snowflake-arctic-instruct",
    "developer": "snowflake",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_lite",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.338,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.7606242197253433
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.654,
        "details": {
          "description": "min=0.654, mean=0.654, max=0.654, sum=0.654 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=0.624, mean=0.624, max=0.624, sum=0.624 (1)",
            "tab": "Efficiency",
            "score": 0.6239793220036466
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=4.262, mean=4.262, max=4.262, sum=4.262 (1)",
            "tab": "General information",
            "score": 4.261971830985916
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3603.217, mean=3603.217, max=3603.217, sum=3603.217 (1)",
            "tab": "General information",
            "score": 3603.2169014084507
          },
          "NarrativeQA - # output tokens": {
            "description": "min=11.907, mean=11.907, max=11.907, sum=11.907 (1)",
            "tab": "General information",
            "score": 11.907042253521126
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (closed-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (closed-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.39,
        "details": {
          "description": "min=0.39, mean=0.39, max=0.39, sum=0.39 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=0.636, mean=0.636, max=0.636, sum=0.636 (1)",
            "tab": "Efficiency",
            "score": 0.6355201268196106
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=0.469, mean=0.469, max=0.469, sum=0.469 (1)",
            "tab": "Efficiency",
            "score": 0.4687326259613037
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.825, mean=4.825, max=4.825, sum=4.825 (1)",
            "tab": "General information",
            "score": 4.825
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.028, mean=0.028, max=0.028, sum=0.028 (1)",
            "tab": "General information",
            "score": 0.028
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=2311.514, mean=2311.514, max=2311.514, sum=2311.514 (1)",
            "tab": "General information",
            "score": 2311.514
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=18.701, mean=18.701, max=18.701, sum=18.701 (1)",
            "tab": "General information",
            "score": 18.701
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=166.383, mean=166.383, max=166.383, sum=166.383 (1)",
            "tab": "General information",
            "score": 166.383
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=14.473, mean=14.473, max=14.473, sum=14.473 (1)",
            "tab": "General information",
            "score": 14.473
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "mode": "closedbook"
        }
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.828,
        "details": {
          "description": "min=0.828, mean=0.828, max=0.828, sum=0.828 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=0.284, mean=0.284, max=0.284, sum=0.284 (1)",
            "tab": "Efficiency",
            "score": 0.2840936713218689
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=291.574, mean=291.574, max=291.574, sum=291.574 (1)",
            "tab": "General information",
            "score": 291.574
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "dataset": "openbookqa",
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.575,
        "details": {
          "description": "min=0.31, mean=0.575, max=0.88, sum=2.876 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=0.293, mean=0.303, max=0.317, sum=1.516 (5)",
            "tab": "Efficiency",
            "score": 0.30325288054817606
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=406.65, mean=531.547, max=693.675, sum=2657.735 (5)",
            "tab": "General information",
            "score": 531.5470877192982
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "college_chemistry",
            "computer_security",
            "econometrics",
            "us_foreign_policy"
          ],
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MATH",
      "source_data": {
        "dataset_name": "MATH",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "Equivalent (CoT) on MATH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.519,
        "details": {
          "description": "min=0.316, mean=0.519, max=0.785, sum=3.636 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=1.482, mean=1.724, max=1.995, sum=12.068 (7)",
            "tab": "Efficiency",
            "score": 1.723981539653867
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=971.652, mean=1438.636, max=2490.962, sum=10070.453 (7)",
            "tab": "General information",
            "score": 1438.6362030100095
          },
          "MATH - # output tokens": {
            "description": "min=82.872, mean=98.802, max=122.233, sum=691.615 (7)",
            "tab": "General information",
            "score": 98.80208187931566
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "algebra",
            "counting_and_probability",
            "geometry",
            "intermediate_algebra",
            "number_theory",
            "prealgebra",
            "precalculus"
          ],
          "level": "1",
          "use_official_examples": "False",
          "use_chain_of_thought": "True"
        }
      }
    },
    {
      "evaluation_name": "GSM8K",
      "source_data": {
        "dataset_name": "GSM8K",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on GSM8K",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.768,
        "details": {
          "description": "min=0.768, mean=0.768, max=0.768, sum=0.768 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=2.961, mean=2.961, max=2.961, sum=2.961 (1)",
            "tab": "Efficiency",
            "score": 2.9610197002887726
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=1207.746, mean=1207.746, max=1207.746, sum=1207.746 (1)",
            "tab": "General information",
            "score": 1207.746
          },
          "GSM8K - # output tokens": {
            "description": "min=189.305, mean=189.305, max=189.305, sum=189.305 (1)",
            "tab": "General information",
            "score": 189.305
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "stop": "none"
        }
      }
    },
    {
      "evaluation_name": "LegalBench",
      "source_data": {
        "dataset_name": "LegalBench",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on LegalBench",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.588,
        "details": {
          "description": "min=0.351, mean=0.588, max=0.874, sum=2.94 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=0.292, mean=0.346, max=0.462, sum=1.729 (5)",
            "tab": "Efficiency",
            "score": 0.34576316386866485
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=1.81, mean=4.162, max=5, sum=20.81 (5)",
            "tab": "General information",
            "score": 4.162040816326531
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0.002, max=0.008, sum=0.008 (5)",
            "tab": "General information",
            "score": 0.0016326530612244899
          },
          "LegalBench - # prompt tokens": {
            "description": "min=239.137, mean=1024.722, max=3561.237, sum=5123.61 (5)",
            "tab": "General information",
            "score": 1024.7220443430492
          },
          "LegalBench - # output tokens": {
            "description": "min=2, mean=2.438, max=3.421, sum=12.188 (5)",
            "tab": "General information",
            "score": 2.4375592890361366
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": [
            "abercrombie",
            "corporate_lobbying",
            "function_of_decision_section",
            "international_citizenship_questions",
            "proa"
          ]
        }
      }
    },
    {
      "evaluation_name": "MedQA",
      "source_data": {
        "dataset_name": "MedQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MedQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.581,
        "details": {
          "description": "min=0.581, mean=0.581, max=0.581, sum=0.581 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=0.313, mean=0.313, max=0.313, sum=0.313 (1)",
            "tab": "Efficiency",
            "score": 0.31300480038697864
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1243.901, mean=1243.901, max=1243.901, sum=1243.901 (1)",
            "tab": "General information",
            "score": 1243.9005964214712
          },
          "MedQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "WMT 2014",
      "source_data": {
        "dataset_name": "WMT 2014",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "BLEU-4 on WMT 2014",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.172,
        "details": {
          "description": "min=0.09, mean=0.172, max=0.217, sum=0.86 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=0.65, mean=0.681, max=0.702, sum=3.405 (5)",
            "tab": "Efficiency",
            "score": 0.681007040066764
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=145.523, mean=160.288, max=182.972, sum=801.438 (5)",
            "tab": "General information",
            "score": 160.28751290334915
          },
          "WMT 2014 - # output tokens": {
            "description": "min=28.596, mean=30.59, max=31.485, sum=152.951 (5)",
            "tab": "General information",
            "score": 30.59012702630372
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "language_pair": [
            "cs-en",
            "de-en",
            "fr-en",
            "hi-en",
            "ru-en"
          ]
        }
      }
    }
  ]
}