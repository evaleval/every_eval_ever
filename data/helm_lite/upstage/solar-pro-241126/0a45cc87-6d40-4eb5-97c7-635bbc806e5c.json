{
  "schema_version": "0.0.1",
  "evaluation_id": "helm_lite/upstage_solar-pro-241126/1764179530.8475971",
  "retrieved_timestamp": "1764179530.8475971",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
  ],
  "evaluation_source": {
    "evaluation_source_name": "helm_lite",
    "evaluation_source_type": "leaderboard"
  },
  "source_metadata": {
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Solar Pro",
    "id": "upstage/solar-pro-241126",
    "developer": "upstage",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "WMT 2014 - BLEU-4",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\nBLEU-4: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 4-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.602,
        "details": {
          "accuracy_description": null,
          "efficiency_description": null,
          "eval_time_mean_win_rate": 0.482
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.753,
        "details": {
          "accuracy_description": "min=0.753, mean=0.753, max=0.753, sum=0.753 (1)",
          "efficiency_description": "min=2.29, mean=2.29, max=2.29, sum=2.29 (1)",
          "eval_time": 2.29
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.792,
        "details": {
          "accuracy_description": "min=0.792, mean=0.792, max=0.792, sum=0.792 (1)",
          "efficiency_description": "min=1.102, mean=1.102, max=1.102, sum=1.102 (1)",
          "eval_time": 1.102
        }
      },
      "generation_config": {
        "mode": "openbook_longans"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (open-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.297,
        "details": {
          "accuracy_description": "min=0.297, mean=0.297, max=0.297, sum=0.297 (1)",
          "efficiency_description": "min=0.588, mean=0.588, max=0.588, sum=0.588 (1)",
          "eval_time": 0.588
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.922,
        "details": {
          "accuracy_description": "min=0.922, mean=0.922, max=0.922, sum=0.922 (1)",
          "efficiency_description": "min=0.431, mean=0.431, max=0.431, sum=0.431 (1)",
          "eval_time": 0.431
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.679,
        "details": {
          "accuracy_description": "min=0.46, mean=0.679, max=0.97, sum=3.395 (5)",
          "efficiency_description": "min=0.429, mean=0.529, max=0.765, sum=2.644 (5)",
          "eval_time": 0.529
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.567,
        "details": {
          "accuracy_description": "min=0.421, mean=0.567, max=0.741, sum=3.968 (7)",
          "efficiency_description": "min=1.926, mean=2.29, max=2.87, sum=16.027 (7)",
          "eval_time": 2.29
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "MATH - Equivalent (CoT)",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\nEquivalent (CoT): Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thought prompting.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.871,
        "details": {
          "accuracy_description": "min=0.871, mean=0.871, max=0.871, sum=0.871 (1)",
          "efficiency_description": "min=2.666, mean=2.666, max=2.666, sum=2.666 (1)",
          "eval_time": 2.666
        }
      },
      "generation_config": {
        "stop": "none"
      }
    },
    {
      "evaluation_name": "GSM8K - EM",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nExact match (final number): Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.67,
        "details": {
          "accuracy_description": "min=0.384, mean=0.67, max=0.905, sum=3.348 (5)",
          "efficiency_description": "min=0.438, mean=0.654, max=1.454, sum=3.271 (5)",
          "eval_time": 0.654
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "LegalBench - EM",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.698,
        "details": {
          "accuracy_description": "min=0.698, mean=0.698, max=0.698, sum=0.698 (1)",
          "efficiency_description": "min=0.596, mean=0.596, max=0.596, sum=0.596 (1)",
          "eval_time": 0.596
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "MedQA - EM",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.169,
        "details": {
          "accuracy_description": "min=0.085, mean=0.169, max=0.229, sum=0.844 (5)",
          "efficiency_description": "min=0.839, mean=0.871, max=0.895, sum=4.357 (5)",
          "eval_time": 0.871
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    }
  ]
}