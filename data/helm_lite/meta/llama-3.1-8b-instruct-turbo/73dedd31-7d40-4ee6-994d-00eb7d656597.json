{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_lite/meta_llama-3.1-8b-instruct-turbo/1770834614.1822479",
  "retrieved_timestamp": "1770834614.1822479",
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "Llama 3.1 Instruct Turbo 8B",
    "id": "meta/llama-3.1-8b-instruct-turbo",
    "developer": "meta",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_lite",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.303,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.5896504369538077
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.756,
        "details": {
          "description": "min=0.756, mean=0.756, max=0.756, sum=0.756 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=0.581, mean=0.581, max=0.581, sum=0.581 (1)",
            "tab": "Efficiency",
            "score": 0.5813529316808136
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3484.268, mean=3484.268, max=3484.268, sum=3484.268 (1)",
            "tab": "General information",
            "score": 3484.2676056338028
          },
          "NarrativeQA - # output tokens": {
            "description": "min=7.287, mean=7.287, max=7.287, sum=7.287 (1)",
            "tab": "General information",
            "score": 7.2873239436619714
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (closed-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (closed-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.209,
        "details": {
          "description": "min=0.209, mean=0.209, max=0.209, sum=0.209 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=0.544, mean=0.544, max=0.544, sum=0.544 (1)",
            "tab": "Efficiency",
            "score": 0.5441543731689453
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=0.752, mean=0.752, max=0.752, sum=0.752 (1)",
            "tab": "Efficiency",
            "score": 0.751717613697052
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1716.78, mean=1716.78, max=1716.78, sum=1716.78 (1)",
            "tab": "General information",
            "score": 1716.78
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=8.736, mean=8.736, max=8.736, sum=8.736 (1)",
            "tab": "General information",
            "score": 8.736
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=129.12, mean=129.12, max=129.12, sum=129.12 (1)",
            "tab": "General information",
            "score": 129.12
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=11.732, mean=11.732, max=11.732, sum=11.732 (1)",
            "tab": "General information",
            "score": 11.732
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "mode": "closedbook"
        }
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.74,
        "details": {
          "description": "min=0.74, mean=0.74, max=0.74, sum=0.74 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=2.937, mean=2.937, max=2.937, sum=2.937 (1)",
            "tab": "Efficiency",
            "score": 2.9374450149536133
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=249.776, mean=249.776, max=249.776, sum=249.776 (1)",
            "tab": "General information",
            "score": 249.776
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "dataset": "openbookqa",
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.5,
        "details": {
          "description": "min=0.26, mean=0.5, max=0.79, sum=2.501 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=0.284, mean=0.417, max=0.567, sum=2.086 (5)",
            "tab": "Efficiency",
            "score": 0.41729471965421716
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=373.43, mean=467.686, max=614.421, sum=2338.431 (5)",
            "tab": "General information",
            "score": 467.6862105263158
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "college_chemistry",
            "computer_security",
            "econometrics",
            "us_foreign_policy"
          ],
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MATH",
      "source_data": {
        "dataset_name": "MATH",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "Equivalent (CoT) on MATH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.703,
        "details": {
          "description": "min=0.509, mean=0.703, max=0.849, sum=4.92 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=1.617, mean=1.927, max=2.175, sum=13.492 (7)",
            "tab": "Efficiency",
            "score": 1.9274194573191807
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=881.363, mean=1262.909, max=2197.577, sum=8840.364 (7)",
            "tab": "General information",
            "score": 1262.9092130545007
          },
          "MATH - # output tokens": {
            "description": "min=203.384, mean=253.982, max=288.596, sum=1777.872 (7)",
            "tab": "General information",
            "score": 253.98170179473732
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "algebra",
            "counting_and_probability",
            "geometry",
            "intermediate_algebra",
            "number_theory",
            "prealgebra",
            "precalculus"
          ],
          "level": "1",
          "use_official_examples": "False",
          "use_chain_of_thought": "True"
        }
      }
    },
    {
      "evaluation_name": "GSM8K",
      "source_data": {
        "dataset_name": "GSM8K",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on GSM8K",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.798,
        "details": {
          "description": "min=0.798, mean=0.798, max=0.798, sum=0.798 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=2.109, mean=2.109, max=2.109, sum=2.109 (1)",
            "tab": "Efficiency",
            "score": 2.108796592712402
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=959.032, mean=959.032, max=959.032, sum=959.032 (1)",
            "tab": "General information",
            "score": 959.032
          },
          "GSM8K - # output tokens": {
            "description": "min=150.02, mean=150.02, max=150.02, sum=150.02 (1)",
            "tab": "General information",
            "score": 150.02
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "stop": "none"
        }
      }
    },
    {
      "evaluation_name": "LegalBench",
      "source_data": {
        "dataset_name": "LegalBench",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on LegalBench",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.342,
        "details": {
          "description": "min=0, mean=0.342, max=0.8, sum=1.71 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=0.409, mean=0.481, max=0.626, sum=2.407 (5)",
            "tab": "Efficiency",
            "score": 0.4814103188942614
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=4, mean=4.8, max=5, sum=24 (5)",
            "tab": "General information",
            "score": 4.8
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "LegalBench - # prompt tokens": {
            "description": "min=197.442, mean=1513.882, max=6300.012, sum=7569.412 (5)",
            "tab": "General information",
            "score": 1513.8824197238912
          },
          "LegalBench - # output tokens": {
            "description": "min=2.032, mean=6.824, max=10.886, sum=34.118 (5)",
            "tab": "General information",
            "score": 6.823557876005701
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": [
            "abercrombie",
            "corporate_lobbying",
            "function_of_decision_section",
            "international_citizenship_questions",
            "proa"
          ]
        }
      }
    },
    {
      "evaluation_name": "MedQA",
      "source_data": {
        "dataset_name": "MedQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MedQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.245,
        "details": {
          "description": "min=0.245, mean=0.245, max=0.245, sum=0.245 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=0.743, mean=0.743, max=0.743, sum=0.743 (1)",
            "tab": "Efficiency",
            "score": 0.742541556803891
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1025.274, mean=1025.274, max=1025.274, sum=1025.274 (1)",
            "tab": "General information",
            "score": 1025.2743538767395
          },
          "MedQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "WMT 2014",
      "source_data": {
        "dataset_name": "WMT 2014",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "BLEU-4 on WMT 2014",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.181,
        "details": {
          "description": "min=0.132, mean=0.181, max=0.219, sum=0.907 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=0.439, mean=0.565, max=0.727, sum=2.826 (5)",
            "tab": "Efficiency",
            "score": 0.5651802479746801
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=101.139, mean=120.712, max=141.117, sum=603.559 (5)",
            "tab": "General information",
            "score": 120.71178123566294
          },
          "WMT 2014 - # output tokens": {
            "description": "min=24.354, mean=25.779, max=26.833, sum=128.893 (5)",
            "tab": "General information",
            "score": 25.778561802263347
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "language_pair": [
            "cs-en",
            "de-en",
            "fr-en",
            "hi-en",
            "ru-en"
          ]
        }
      }
    }
  ]
}