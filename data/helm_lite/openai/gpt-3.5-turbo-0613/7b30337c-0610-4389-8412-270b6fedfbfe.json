{
  "schema_version": "0.1.0",
  "evaluation_id": "helm_lite/openai_gpt-3.5-turbo-0613/1767655940.500768",
  "retrieved_timestamp": "1767655940.500768",
  "source_data": [
    "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
  ],
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "GPT-3.5 Turbo (0613)",
    "id": "openai/gpt-3.5-turbo-0613",
    "developer": "openai",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.358,
        "details": {
          "description": null,
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.956641697877653
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NarrativeQA - F1",
      "metric_config": {
        "evaluation_description": "The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.655,
        "details": {
          "description": "min=0.655, mean=0.655, max=0.655, sum=0.655 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=0.381, mean=0.381, max=0.381, sum=0.381 (1)",
            "tab": "Efficiency",
            "score": 0.3810261323418416
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=4.946, mean=4.946, max=4.946, sum=4.946 (1)",
            "tab": "General information",
            "score": 4.946478873239436
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3493.662, mean=3493.662, max=3493.662, sum=3493.662 (1)",
            "tab": "General information",
            "score": 3493.6619718309857
          },
          "NarrativeQA - # output tokens": {
            "description": "min=9.91, mean=9.91, max=9.91, sum=9.91 (1)",
            "tab": "General information",
            "score": 9.909859154929578
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book) - F1",
      "metric_config": {
        "evaluation_description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.335,
        "details": {
          "description": "min=0.335, mean=0.335, max=0.335, sum=0.335 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=0.305, mean=0.305, max=0.305, sum=0.305 (1)",
            "tab": "Efficiency",
            "score": 0.30532183837890625
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=0.221, mean=0.221, max=0.221, sum=0.221 (1)",
            "tab": "Efficiency",
            "score": 0.22069251775741577
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=4.884, mean=4.884, max=4.884, sum=4.884 (1)",
            "tab": "General information",
            "score": 4.884
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0.019, mean=0.019, max=0.019, sum=0.019 (1)",
            "tab": "General information",
            "score": 0.019
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1649.552, mean=1649.552, max=1649.552, sum=1649.552 (1)",
            "tab": "General information",
            "score": 1649.552
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=9.389, mean=9.389, max=9.389, sum=9.389 (1)",
            "tab": "General information",
            "score": 9.389
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=173.127, mean=173.127, max=173.127, sum=173.127 (1)",
            "tab": "General information",
            "score": 173.127
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=5.576, mean=5.576, max=5.576, sum=5.576 (1)",
            "tab": "General information",
            "score": 5.576
          }
        }
      },
      "generation_config": {
        "mode": "closedbook"
      }
    },
    {
      "evaluation_name": "OpenbookQA - EM",
      "metric_config": {
        "evaluation_description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.838,
        "details": {
          "description": "min=0.838, mean=0.838, max=0.838, sum=0.838 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=0.172, mean=0.172, max=0.172, sum=0.172 (1)",
            "tab": "Efficiency",
            "score": 0.17227248001098633
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=242.782, mean=242.782, max=242.782, sum=242.782 (1)",
            "tab": "General information",
            "score": 242.782
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "dataset": "openbookqa",
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MMLU - EM",
      "metric_config": {
        "evaluation_description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2009.03300.pdf).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.614,
        "details": {
          "description": "min=0.38, mean=0.614, max=0.88, sum=3.07 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=0.171, mean=0.175, max=0.177, sum=0.875 (5)",
            "tab": "Efficiency",
            "score": 0.1750619323630082
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=366.44, mean=460.72, max=607.43, sum=2303.6 (5)",
            "tab": "General information",
            "score": 460.71996491228066
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "subject": [
          "abstract_algebra",
          "college_chemistry",
          "computer_security",
          "econometrics",
          "us_foreign_policy"
        ],
        "method": "multiple_choice_joint"
      }
    },
    {
      "evaluation_name": "MATH - Equivalent (CoT)",
      "metric_config": {
        "evaluation_description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://arxiv.org/pdf/2103.03874.pdf).\n\nEquivalent (CoT): Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thought prompting.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.667,
        "details": {
          "description": "min=0.533, mean=0.667, max=0.826, sum=4.667 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=0.741, mean=0.813, max=0.963, sum=5.69 (7)",
            "tab": "Efficiency",
            "score": 0.8128212395123947
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=942.363, mean=1323.911, max=2258.577, sum=9267.376 (7)",
            "tab": "General information",
            "score": 1323.910874184069
          },
          "MATH - # output tokens": {
            "description": "min=53.5, mean=60.844, max=77.4, sum=425.908 (7)",
            "tab": "General information",
            "score": 60.844003793024605
          }
        }
      },
      "generation_config": {
        "subject": [
          "algebra",
          "counting_and_probability",
          "geometry",
          "intermediate_algebra",
          "number_theory",
          "prealgebra",
          "precalculus"
        ],
        "level": "1",
        "use_official_examples": "False",
        "use_chain_of_thought": "True"
      }
    },
    {
      "evaluation_name": "GSM8K - EM",
      "metric_config": {
        "evaluation_description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nExact match (final number): Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.501,
        "details": {
          "description": "min=0.501, mean=0.501, max=0.501, sum=0.501 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=0.898, mean=0.898, max=0.898, sum=0.898 (1)",
            "tab": "Efficiency",
            "score": 0.8983073465824127
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=1020.035, mean=1020.035, max=1020.035, sum=1020.035 (1)",
            "tab": "General information",
            "score": 1020.035
          },
          "GSM8K - # output tokens": {
            "description": "min=77.29, mean=77.29, max=77.29, sum=77.29 (1)",
            "tab": "General information",
            "score": 77.29
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "LegalBench - EM",
      "metric_config": {
        "evaluation_description": "LegalBench is a large collaboratively constructed benchmark of legal reasoning tasks [(Guha et al, 2023)](https://arxiv.org/pdf/2308.11462.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.528,
        "details": {
          "description": "min=0.302, mean=0.528, max=0.747, sum=2.642 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=0.178, mean=0.202, max=0.277, sum=1.011 (5)",
            "tab": "Efficiency",
            "score": 0.20213919553681423
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=2.09, mean=4.218, max=5, sum=21.09 (5)",
            "tab": "General information",
            "score": 4.21795918367347
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "LegalBench - # prompt tokens": {
            "description": "min=253.442, mean=949.517, max=3254.159, sum=4747.586 (5)",
            "tab": "General information",
            "score": 949.5172570702738
          },
          "LegalBench - # output tokens": {
            "description": "min=1, mean=1.387, max=2.032, sum=6.934 (5)",
            "tab": "General information",
            "score": 1.3868394951957552
          }
        }
      },
      "generation_config": {
        "subset": [
          "abercrombie",
          "corporate_lobbying",
          "function_of_decision_section",
          "international_citizenship_questions",
          "proa"
        ]
      }
    },
    {
      "evaluation_name": "MedQA - EM",
      "metric_config": {
        "evaluation_description": "MedQA is an open domain question answering dataset composed of questions from professional medical board exams ([Jin et al. 2020](https://arxiv.org/pdf/2009.13081.pdf)).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.622,
        "details": {
          "description": "min=0.622, mean=0.622, max=0.622, sum=0.622 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=0.194, mean=0.194, max=0.194, sum=0.194 (1)",
            "tab": "Efficiency",
            "score": 0.19374941736755977
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1020.414, mean=1020.414, max=1020.414, sum=1020.414 (1)",
            "tab": "General information",
            "score": 1020.4135188866799
          },
          "MedQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {}
    },
    {
      "evaluation_name": "WMT 2014 - BLEU-4",
      "metric_config": {
        "evaluation_description": "WMT 2014 is a collection of machine translation datasets [(website)](https://www.statmt.org/wmt14/index.html).\n\nBLEU-4: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 4-gram overlap.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.187,
        "details": {
          "description": "min=0.1, mean=0.187, max=0.23, sum=0.937 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=0.367, mean=0.394, max=0.409, sum=1.968 (5)",
            "tab": "Efficiency",
            "score": 0.39351808213963385
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=169.901, mean=193.043, max=213.185, sum=965.213 (5)",
            "tab": "General information",
            "score": 193.04258583116683
          },
          "WMT 2014 - # output tokens": {
            "description": "min=21.983, mean=25.038, max=26.352, sum=125.192 (5)",
            "tab": "General information",
            "score": 25.038384118366725
          }
        }
      },
      "generation_config": {
        "language_pair": [
          "cs-en",
          "de-en",
          "fr-en",
          "hi-en",
          "ru-en"
        ]
      }
    }
  ]
}