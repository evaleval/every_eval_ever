{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_lite/openai_gpt-4-1106-preview/1770834614.1822479",
  "retrieved_timestamp": "1770834614.1822479",
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "GPT-4 Turbo 1106 preview",
    "id": "openai/gpt-4-1106-preview",
    "developer": "openai",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_lite",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.698,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.3935580524344569
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.727,
        "details": {
          "description": "min=0.727, mean=0.727, max=0.727, sum=0.727 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=1.068, mean=1.068, max=1.068, sum=1.068 (1)",
            "tab": "Efficiency",
            "score": 1.068114177945634
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3522.67, mean=3522.67, max=3522.67, sum=3522.67 (1)",
            "tab": "General information",
            "score": 3522.6704225352114
          },
          "NarrativeQA - # output tokens": {
            "description": "min=9.885, mean=9.885, max=9.885, sum=9.885 (1)",
            "tab": "General information",
            "score": 9.88450704225352
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (closed-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (closed-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.435,
        "details": {
          "description": "min=0.435, mean=0.435, max=0.435, sum=0.435 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=0.867, mean=0.867, max=0.867, sum=0.867 (1)",
            "tab": "Efficiency",
            "score": 0.8667134034633637
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=1.131, mean=1.131, max=1.131, sum=1.131 (1)",
            "tab": "Efficiency",
            "score": 1.1312835423946381
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1762.593, mean=1762.593, max=1762.593, sum=1762.593 (1)",
            "tab": "General information",
            "score": 1762.593
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=8.753, mean=8.753, max=8.753, sum=8.753 (1)",
            "tab": "General information",
            "score": 8.753
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=173.127, mean=173.127, max=173.127, sum=173.127 (1)",
            "tab": "General information",
            "score": 173.127
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=14.157, mean=14.157, max=14.157, sum=14.157 (1)",
            "tab": "General information",
            "score": 14.157
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "mode": "closedbook"
        }
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.95,
        "details": {
          "description": "min=0.95, mean=0.95, max=0.95, sum=0.95 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=0.512, mean=0.512, max=0.512, sum=0.512 (1)",
            "tab": "Efficiency",
            "score": 0.5122070140838623
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=242.782, mean=242.782, max=242.782, sum=242.782 (1)",
            "tab": "General information",
            "score": 242.782
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "dataset": "openbookqa",
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.699,
        "details": {
          "description": "min=0.47, mean=0.699, max=0.96, sum=3.495 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=0.397, mean=0.447, max=0.515, sum=2.236 (5)",
            "tab": "Efficiency",
            "score": 0.4471675806380155
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=366.44, mean=460.72, max=607.43, sum=2303.6 (5)",
            "tab": "General information",
            "score": 460.71996491228066
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "college_chemistry",
            "computer_security",
            "econometrics",
            "us_foreign_policy"
          ],
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MATH",
      "source_data": {
        "dataset_name": "MATH",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "Equivalent (CoT) on MATH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.857,
        "details": {
          "description": "min=0.711, mean=0.857, max=0.97, sum=5.998 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=10.989, mean=12.704, max=15.09, sum=88.928 (7)",
            "tab": "Efficiency",
            "score": 12.704059314714486
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=942.363, mean=1323.911, max=2258.577, sum=9267.376 (7)",
            "tab": "General information",
            "score": 1323.910874184069
          },
          "MATH - # output tokens": {
            "description": "min=122.465, mean=161.876, max=186.673, sum=1133.133 (7)",
            "tab": "General information",
            "score": 161.87607288445722
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "algebra",
            "counting_and_probability",
            "geometry",
            "intermediate_algebra",
            "number_theory",
            "prealgebra",
            "precalculus"
          ],
          "level": "1",
          "use_official_examples": "False",
          "use_chain_of_thought": "True"
        }
      }
    },
    {
      "evaluation_name": "GSM8K",
      "source_data": {
        "dataset_name": "GSM8K",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on GSM8K",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.668,
        "details": {
          "description": "min=0.668, mean=0.668, max=0.668, sum=0.668 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=5.738, mean=5.738, max=5.738, sum=5.738 (1)",
            "tab": "Efficiency",
            "score": 5.738402992963791
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=1020.035, mean=1020.035, max=1020.035, sum=1020.035 (1)",
            "tab": "General information",
            "score": 1020.035
          },
          "GSM8K - # output tokens": {
            "description": "min=98.073, mean=98.073, max=98.073, sum=98.073 (1)",
            "tab": "General information",
            "score": 98.073
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "LegalBench",
      "source_data": {
        "dataset_name": "LegalBench",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on LegalBench",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.626,
        "details": {
          "description": "min=0.368, mean=0.626, max=0.989, sum=3.13 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=0.445, mean=0.603, max=0.98, sum=3.017 (5)",
            "tab": "Efficiency",
            "score": 0.6033123332286346
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=4, mean=4.8, max=5, sum=24 (5)",
            "tab": "General information",
            "score": 4.8
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "LegalBench - # prompt tokens": {
            "description": "min=253.442, mean=1570.163, max=6357.388, sum=7850.815 (5)",
            "tab": "General information",
            "score": 1570.162971355988
          },
          "LegalBench - # output tokens": {
            "description": "min=1, mean=1.458, max=2.695, sum=7.291 (5)",
            "tab": "General information",
            "score": 1.458208948802524
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": [
            "abercrombie",
            "corporate_lobbying",
            "function_of_decision_section",
            "international_citizenship_questions",
            "proa"
          ]
        }
      }
    },
    {
      "evaluation_name": "MedQA",
      "source_data": {
        "dataset_name": "MedQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MedQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.817,
        "details": {
          "description": "min=0.817, mean=0.817, max=0.817, sum=0.817 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=0.392, mean=0.392, max=0.392, sum=0.392 (1)",
            "tab": "Efficiency",
            "score": 0.3924491192190121
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1020.414, mean=1020.414, max=1020.414, sum=1020.414 (1)",
            "tab": "General information",
            "score": 1020.4135188866799
          },
          "MedQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "WMT 2014",
      "source_data": {
        "dataset_name": "WMT 2014",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "BLEU-4 on WMT 2014",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.205,
        "details": {
          "description": "min=0.156, mean=0.205, max=0.241, sum=1.023 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=1.797, mean=2.1, max=2.349, sum=10.502 (5)",
            "tab": "Efficiency",
            "score": 2.1004491326059744
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=169.901, mean=193.043, max=213.185, sum=965.213 (5)",
            "tab": "General information",
            "score": 193.04258583116683
          },
          "WMT 2014 - # output tokens": {
            "description": "min=26.229, mean=26.996, max=28.59, sum=134.98 (5)",
            "tab": "General information",
            "score": 26.995945480960394
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "language_pair": [
            "cs-en",
            "de-en",
            "fr-en",
            "hi-en",
            "ru-en"
          ]
        }
      }
    }
  ]
}