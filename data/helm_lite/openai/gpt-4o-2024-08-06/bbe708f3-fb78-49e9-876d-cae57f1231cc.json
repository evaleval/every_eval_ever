{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_lite/openai_gpt-4o-2024-08-06/1770834614.1822479",
  "retrieved_timestamp": "1770834614.1822479",
  "source_metadata": {
    "source_name": "helm_lite",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "GPT-4o 2024-08-06",
    "id": "openai/gpt-4o-2024-08-06",
    "developer": "openai",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean win rate",
      "source_data": {
        "dataset_name": "helm_lite",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "How many models this model outperforms on average (over columns).",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.928,
        "details": {
          "tab": "Accuracy",
          "Mean win rate - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 0.6728589263420724
          },
          "Mean win rate - General information": {
            "description": null,
            "tab": "General information",
            "score": null
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NarrativeQA",
      "source_data": {
        "dataset_name": "NarrativeQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NarrativeQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.795,
        "details": {
          "description": "min=0.795, mean=0.795, max=0.795, sum=0.795 (1)",
          "tab": "Accuracy",
          "NarrativeQA - Observed inference time (s)": {
            "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
            "tab": "Efficiency",
            "score": 0.5615828097706109
          },
          "NarrativeQA - # eval": {
            "description": "min=355, mean=355, max=355, sum=355 (1)",
            "tab": "General information",
            "score": 355.0
          },
          "NarrativeQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NarrativeQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NarrativeQA - # prompt tokens": {
            "description": "min=3451.668, mean=3451.668, max=3451.668, sum=3451.668 (1)",
            "tab": "General information",
            "score": 3451.667605633803
          },
          "NarrativeQA - # output tokens": {
            "description": "min=5.076, mean=5.076, max=5.076, sum=5.076 (1)",
            "tab": "General information",
            "score": 5.076056338028169
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "NaturalQuestions (closed-book)",
      "source_data": {
        "dataset_name": "NaturalQuestions (closed-book)",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "F1 on NaturalQuestions (closed-book)",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.496,
        "details": {
          "description": "min=0.496, mean=0.496, max=0.496, sum=0.496 (1)",
          "tab": "Accuracy",
          "NaturalQuestions (open-book) - Observed inference time (s)": {
            "description": "min=0.616, mean=0.616, max=0.616, sum=0.616 (1)",
            "tab": "Efficiency",
            "score": 0.6156781461238862
          },
          "NaturalQuestions (closed-book) - Observed inference time (s)": {
            "description": "min=0.418, mean=0.418, max=0.418, sum=0.418 (1)",
            "tab": "Efficiency",
            "score": 0.4182390425205231
          },
          "NaturalQuestions (open-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (open-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (open-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (open-book) - # prompt tokens": {
            "description": "min=1714.02, mean=1714.02, max=1714.02, sum=1714.02 (1)",
            "tab": "General information",
            "score": 1714.02
          },
          "NaturalQuestions (open-book) - # output tokens": {
            "description": "min=6.504, mean=6.504, max=6.504, sum=6.504 (1)",
            "tab": "General information",
            "score": 6.504
          },
          "NaturalQuestions (closed-book) - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "NaturalQuestions (closed-book) - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "NaturalQuestions (closed-book) - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "NaturalQuestions (closed-book) - # prompt tokens": {
            "description": "min=129.953, mean=129.953, max=129.953, sum=129.953 (1)",
            "tab": "General information",
            "score": 129.953
          },
          "NaturalQuestions (closed-book) - # output tokens": {
            "description": "min=5.032, mean=5.032, max=5.032, sum=5.032 (1)",
            "tab": "General information",
            "score": 5.032
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "mode": "closedbook"
        }
      }
    },
    {
      "evaluation_name": "OpenbookQA",
      "source_data": {
        "dataset_name": "OpenbookQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on OpenbookQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.968,
        "details": {
          "description": "min=0.968, mean=0.968, max=0.968, sum=0.968 (1)",
          "tab": "Accuracy",
          "OpenbookQA - Observed inference time (s)": {
            "description": "min=0.401, mean=0.401, max=0.401, sum=0.401 (1)",
            "tab": "Efficiency",
            "score": 0.40116420984268186
          },
          "OpenbookQA - # eval": {
            "description": "min=500, mean=500, max=500, sum=500 (1)",
            "tab": "General information",
            "score": 500.0
          },
          "OpenbookQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "OpenbookQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "OpenbookQA - # prompt tokens": {
            "description": "min=245.486, mean=245.486, max=245.486, sum=245.486 (1)",
            "tab": "General information",
            "score": 245.486
          },
          "OpenbookQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "dataset": "openbookqa",
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MMLU",
      "source_data": {
        "dataset_name": "MMLU",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MMLU",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.738,
        "details": {
          "description": "min=0.58, mean=0.738, max=0.95, sum=3.691 (5)",
          "tab": "Accuracy",
          "MMLU - Observed inference time (s)": {
            "description": "min=0.335, mean=0.441, max=0.512, sum=2.204 (5)",
            "tab": "Efficiency",
            "score": 0.4407063991228739
          },
          "MMLU - # eval": {
            "description": "min=100, mean=102.8, max=114, sum=514 (5)",
            "tab": "General information",
            "score": 102.8
          },
          "MMLU - # train": {
            "description": "min=5, mean=5, max=5, sum=25 (5)",
            "tab": "General information",
            "score": 5.0
          },
          "MMLU - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU - # prompt tokens": {
            "description": "min=373.42, mean=466.992, max=613.228, sum=2334.958 (5)",
            "tab": "General information",
            "score": 466.9916140350877
          },
          "MMLU - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "abstract_algebra",
            "college_chemistry",
            "computer_security",
            "econometrics",
            "us_foreign_policy"
          ],
          "method": "multiple_choice_joint"
        }
      }
    },
    {
      "evaluation_name": "MATH",
      "source_data": {
        "dataset_name": "MATH",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "Equivalent (CoT) on MATH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.853,
        "details": {
          "description": "min=0.731, mean=0.853, max=0.956, sum=5.968 (7)",
          "tab": "Accuracy",
          "MATH - Observed inference time (s)": {
            "description": "min=3.205, mean=4.321, max=6.062, sum=30.245 (7)",
            "tab": "Efficiency",
            "score": 4.320655013573451
          },
          "MATH - # eval": {
            "description": "min=30, mean=62.429, max=135, sum=437 (7)",
            "tab": "General information",
            "score": 62.42857142857143
          },
          "MATH - # train": {
            "description": "min=8, mean=8, max=8, sum=56 (7)",
            "tab": "General information",
            "score": 8.0
          },
          "MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (7)",
            "tab": "General information",
            "score": 0.0
          },
          "MATH - # prompt tokens": {
            "description": "min=888.43, mean=1273.32, max=2222.25, sum=8913.243 (7)",
            "tab": "General information",
            "score": 1273.320452019534
          },
          "MATH - # output tokens": {
            "description": "min=157.721, mean=210.124, max=243.135, sum=1470.869 (7)",
            "tab": "General information",
            "score": 210.1241379885811
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subject": [
            "algebra",
            "counting_and_probability",
            "geometry",
            "intermediate_algebra",
            "number_theory",
            "prealgebra",
            "precalculus"
          ],
          "level": "1",
          "use_official_examples": "False",
          "use_chain_of_thought": "True"
        }
      }
    },
    {
      "evaluation_name": "GSM8K",
      "source_data": {
        "dataset_name": "GSM8K",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on GSM8K",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.909,
        "details": {
          "description": "min=0.909, mean=0.909, max=0.909, sum=0.909 (1)",
          "tab": "Accuracy",
          "GSM8K - Observed inference time (s)": {
            "description": "min=2.937, mean=2.937, max=2.937, sum=2.937 (1)",
            "tab": "Efficiency",
            "score": 2.9373713800907133
          },
          "GSM8K - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "GSM8K - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "GSM8K - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GSM8K - # prompt tokens": {
            "description": "min=952.617, mean=952.617, max=952.617, sum=952.617 (1)",
            "tab": "General information",
            "score": 952.617
          },
          "GSM8K - # output tokens": {
            "description": "min=167.729, mean=167.729, max=167.729, sum=167.729 (1)",
            "tab": "General information",
            "score": 167.729
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "stop": "none"
        }
      }
    },
    {
      "evaluation_name": "LegalBench",
      "source_data": {
        "dataset_name": "LegalBench",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on LegalBench",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.721,
        "details": {
          "description": "min=0.422, mean=0.721, max=0.979, sum=3.605 (5)",
          "tab": "Accuracy",
          "LegalBench - Observed inference time (s)": {
            "description": "min=0.312, mean=0.38, max=0.526, sum=1.901 (5)",
            "tab": "Efficiency",
            "score": 0.38022537218958125
          },
          "LegalBench - # eval": {
            "description": "min=95, mean=409.4, max=1000, sum=2047 (5)",
            "tab": "General information",
            "score": 409.4
          },
          "LegalBench - # train": {
            "description": "min=4, mean=4.8, max=5, sum=24 (5)",
            "tab": "General information",
            "score": 4.8
          },
          "LegalBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "LegalBench - # prompt tokens": {
            "description": "min=198.179, mean=1502.795, max=6244.98, sum=7513.977 (5)",
            "tab": "General information",
            "score": 1502.7954037538377
          },
          "LegalBench - # output tokens": {
            "description": "min=1, mean=1.298, max=2.021, sum=6.49 (5)",
            "tab": "General information",
            "score": 1.298021970457479
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": [
            "abercrombie",
            "corporate_lobbying",
            "function_of_decision_section",
            "international_citizenship_questions",
            "proa"
          ]
        }
      }
    },
    {
      "evaluation_name": "MedQA",
      "source_data": {
        "dataset_name": "MedQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "EM on MedQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.863,
        "details": {
          "description": "min=0.863, mean=0.863, max=0.863, sum=0.863 (1)",
          "tab": "Accuracy",
          "MedQA - Observed inference time (s)": {
            "description": "min=0.307, mean=0.307, max=0.307, sum=0.307 (1)",
            "tab": "Efficiency",
            "score": 0.30731069923158194
          },
          "MedQA - # eval": {
            "description": "min=503, mean=503, max=503, sum=503 (1)",
            "tab": "General information",
            "score": 503.0
          },
          "MedQA - # train": {
            "description": "min=5, mean=5, max=5, sum=5 (1)",
            "tab": "General information",
            "score": 5.0
          },
          "MedQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MedQA - # prompt tokens": {
            "description": "min=1009.05, mean=1009.05, max=1009.05, sum=1009.05 (1)",
            "tab": "General information",
            "score": 1009.0497017892644
          },
          "MedQA - # output tokens": {
            "description": "min=1, mean=1, max=1, sum=1 (1)",
            "tab": "General information",
            "score": 1.0
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "WMT 2014",
      "source_data": {
        "dataset_name": "WMT 2014",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/lite/benchmark_output/releases/v1.13.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "BLEU-4 on WMT 2014",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.225,
        "details": {
          "description": "min=0.18, mean=0.225, max=0.267, sum=1.125 (5)",
          "tab": "Accuracy",
          "WMT 2014 - Observed inference time (s)": {
            "description": "min=0.725, mean=0.768, max=0.804, sum=3.841 (5)",
            "tab": "Efficiency",
            "score": 0.7681678841877538
          },
          "WMT 2014 - # eval": {
            "description": "min=503, mean=568.8, max=832, sum=2844 (5)",
            "tab": "General information",
            "score": 568.8
          },
          "WMT 2014 - # train": {
            "description": "min=1, mean=1, max=1, sum=5 (5)",
            "tab": "General information",
            "score": 1.0
          },
          "WMT 2014 - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (5)",
            "tab": "General information",
            "score": 0.0
          },
          "WMT 2014 - # prompt tokens": {
            "description": "min=69.529, mean=105.006, max=128.497, sum=525.028 (5)",
            "tab": "General information",
            "score": 105.00557042361216
          },
          "WMT 2014 - # output tokens": {
            "description": "min=23.809, mean=25.367, max=25.988, sum=126.835 (5)",
            "tab": "General information",
            "score": 25.366906254779018
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "language_pair": [
            "cs-en",
            "de-en",
            "fr-en",
            "hi-en",
            "ru-en"
          ]
        }
      }
    }
  ]
}