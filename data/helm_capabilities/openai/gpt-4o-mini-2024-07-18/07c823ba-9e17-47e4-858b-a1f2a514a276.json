{
  "schema_version": "0.2.0",
  "evaluation_id": "helm_capabilities/openai_gpt-4o-mini-2024-07-18/1770835969.095764",
  "retrieved_timestamp": "1770835969.095764",
  "source_metadata": {
    "source_name": "helm_capabilities",
    "source_type": "documentation",
    "source_organization_name": "crfm",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "GPT-4o mini 2024-07-18",
    "id": "openai/gpt-4o-mini-2024-07-18",
    "developer": "openai",
    "inference_platform": "unknown"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Mean score",
      "source_data": {
        "dataset_name": "helm_capabilities",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/capabilities/benchmark_output/releases/v1.15.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "The mean of the scores from all columns.",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.565,
        "details": {
          "tab": "Accuracy",
          "Mean score - Efficiency": {
            "description": null,
            "tab": "Efficiency",
            "score": 10.41176955262334
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "MMLU-Pro",
      "source_data": {
        "dataset_name": "MMLU-Pro",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/capabilities/benchmark_output/releases/v1.15.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "COT correct on MMLU-Pro",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.603,
        "details": {
          "description": "min=0.603, mean=0.603, max=0.603, sum=0.603 (1)",
          "tab": "Accuracy",
          "MMLU-Pro - Observed inference time (s)": {
            "description": "min=6.572, mean=6.572, max=6.572, sum=6.572 (1)",
            "tab": "Efficiency",
            "score": 6.57206253027916
          },
          "MMLU-Pro - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "MMLU-Pro - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU-Pro - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "MMLU-Pro - # prompt tokens": {
            "description": "min=228.569, mean=228.569, max=228.569, sum=228.569 (1)",
            "tab": "General information",
            "score": 228.569
          },
          "MMLU-Pro - # output tokens": {
            "description": "min=334.86, mean=334.86, max=334.86, sum=334.86 (1)",
            "tab": "General information",
            "score": 334.86
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": "all",
          "use_chain_of_thought": "true",
          "use_few_shot": "false"
        }
      }
    },
    {
      "evaluation_name": "GPQA",
      "source_data": {
        "dataset_name": "GPQA",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/capabilities/benchmark_output/releases/v1.15.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "COT correct on GPQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.368,
        "details": {
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "tab": "Accuracy",
          "GPQA - Observed inference time (s)": {
            "description": "min=8.814, mean=8.814, max=8.814, sum=8.814 (1)",
            "tab": "Efficiency",
            "score": 8.813848996910814
          },
          "GPQA - # eval": {
            "description": "min=446, mean=446, max=446, sum=446 (1)",
            "tab": "General information",
            "score": 446.0
          },
          "GPQA - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GPQA - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "GPQA - # prompt tokens": {
            "description": "min=248.152, mean=248.152, max=248.152, sum=248.152 (1)",
            "tab": "General information",
            "score": 248.152466367713
          },
          "GPQA - # output tokens": {
            "description": "min=489.226, mean=489.226, max=489.226, sum=489.226 (1)",
            "tab": "General information",
            "score": 489.22645739910314
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": "gpqa_main",
          "use_chain_of_thought": "true",
          "use_few_shot": "false"
        }
      }
    },
    {
      "evaluation_name": "IFEval",
      "source_data": {
        "dataset_name": "IFEval",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/capabilities/benchmark_output/releases/v1.15.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "IFEval Strict Acc on IFEval",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.782,
        "details": {
          "description": "min=0.782, mean=0.782, max=0.782, sum=0.782 (1)",
          "tab": "Accuracy",
          "IFEval - Observed inference time (s)": {
            "description": "min=5.963, mean=5.963, max=5.963, sum=5.963 (1)",
            "tab": "Efficiency",
            "score": 5.963314282916169
          },
          "IFEval - # eval": {
            "description": "min=541, mean=541, max=541, sum=541 (1)",
            "tab": "General information",
            "score": 541.0
          },
          "IFEval - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "IFEval - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "IFEval - # prompt tokens": {
            "description": "min=45.671, mean=45.671, max=45.671, sum=45.671 (1)",
            "tab": "General information",
            "score": 45.67097966728281
          },
          "IFEval - # output tokens": {
            "description": "min=314.919, mean=314.919, max=314.919, sum=314.919 (1)",
            "tab": "General information",
            "score": 314.91866913123846
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    },
    {
      "evaluation_name": "WildBench",
      "source_data": {
        "dataset_name": "WildBench",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/capabilities/benchmark_output/releases/v1.15.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "WB Score on WildBench",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.791,
        "details": {
          "description": "min=0.791, mean=0.791, max=0.791, sum=0.791 (1)",
          "tab": "Accuracy",
          "WildBench - Observed inference time (s)": {
            "description": "min=13.996, mean=13.996, max=13.996, sum=13.996 (1)",
            "tab": "Efficiency",
            "score": 13.996195561885834
          },
          "WildBench - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "WildBench - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "WildBench - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "WildBench - # prompt tokens": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "WildBench - # output tokens": {
            "description": "min=809.307, mean=809.307, max=809.307, sum=809.307 (1)",
            "tab": "General information",
            "score": 809.307
          }
        }
      },
      "generation_config": {
        "additional_details": {
          "subset": "v2"
        }
      }
    },
    {
      "evaluation_name": "Omni-MATH",
      "source_data": {
        "dataset_name": "Omni-MATH",
        "source_type": "url",
        "url": [
          "https://storage.googleapis.com/crfm-helm-public/capabilities/benchmark_output/releases/v1.15.0/groups/core_scenarios.json"
        ]
      },
      "metric_config": {
        "evaluation_description": "Acc on Omni-MATH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0.0,
        "max_score": 1.0
      },
      "score_details": {
        "score": 0.28,
        "details": {
          "description": "min=0.28, mean=0.28, max=0.28, sum=0.28 (1)",
          "tab": "Accuracy",
          "Omni-MATH - Observed inference time (s)": {
            "description": "min=16.713, mean=16.713, max=16.713, sum=16.713 (1)",
            "tab": "Efficiency",
            "score": 16.713426391124724
          },
          "Omni-MATH - # eval": {
            "description": "min=1000, mean=1000, max=1000, sum=1000 (1)",
            "tab": "General information",
            "score": 1000.0
          },
          "Omni-MATH - # train": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "Omni-MATH - truncated": {
            "description": "min=0, mean=0, max=0, sum=0 (1)",
            "tab": "General information",
            "score": 0.0
          },
          "Omni-MATH - # prompt tokens": {
            "description": "min=109.623, mean=109.623, max=109.623, sum=109.623 (1)",
            "tab": "General information",
            "score": 109.623
          },
          "Omni-MATH - # output tokens": {
            "description": "min=863.417, mean=863.417, max=863.417, sum=863.417 (1)",
            "tab": "General information",
            "score": 863.417
          }
        }
      },
      "generation_config": {
        "additional_details": {}
      }
    }
  ]
}