## Automatic Evaluation Log Converters
A collection of scripts to convert evaluation logs from local runs from evaluation frameworks (e.g., `Inspect AI` and `lm-eval-harness`). 

### Installation
- Install the required dependencies:

```bash
uv sync
```

### Inspect

The conversion script from `Inspect AI` to the unified schema can be run using `eval_converters/inspect/__main__.py`.

Using the `--log_path` argument, you can choose one of three ways to specify evaluations to convert:
- Provide an `Inspect AI` evaluation log with the `.eval` extension (e.g., `2026-02-07T11-26-57+00-00_gaia_4V8zHbbRKpU5Yv2BMoBcjE.eval`)
- Provide an `Inspect AI` evaluation log with the `.json` extension (e.g., `2026-02-07T11-26-57+00-00_gaia_4V8zHbbRKpU5Yv2BMoBcjE.json`)
- Provide a directory containing multiple `Inspect AI` evaluation logs

The exact command for converting an example evaluation log is:

```bash
uv run python3 -m eval_converters.inspect --log_path tests/data/inspect/2026-02-07T11-26-57+00-00_gaia_4V8zHbbRKpU5Yv2BMoBcjE.json
```


Full manual for conversion of your own Inspect evaluation log into unified is available below:

```bash
usage: __main__.py [-h] [--log_path LOG_PATH] [--output_dir OUTPUT_DIR]
                   [--source_organization_name SOURCE_ORGANIZATION_NAME]
                   [--evaluator_relationship {first_party,third_party,collaborative,other}]
                   [--source_organization_url SOURCE_ORGANIZATION_URL]
                   [--source_organization_logo_url SOURCE_ORGANIZATION_LOGO_URL]
                   [--eval_library_name EVAL_LIBRARY_NAME]
                   [--eval_library_version EVAL_LIBRARY_VERSION]

options:
  -h, --help            show this help message and exit
  --log_path LOG_PATH   Inspect evalaution log file with extension eval or
                        json.
  --output_dir OUTPUT_DIR
  --source_organization_name SOURCE_ORGANIZATION_NAME
                        Orgnization which pushed evaluation to the every-eval-
                        ever.
  --evaluator_relationship {first_party,third_party,collaborative,other}
                        Relationship of evaluation author to the model
  --source_organization_url SOURCE_ORGANIZATION_URL
  --source_organization_logo_url SOURCE_ORGANIZATION_LOGO_URL
  --eval_library_name EVAL_LIBRARY_NAME
                        Name of the evaluation library (e.g. inspect_ai,
                        lm_eval, helm)
  --eval_library_version EVAL_LIBRARY_VERSION
                        Version of the evaluation library
```

### HELM

You can convert HELM evaluation log into unified schema via `eval_converters/helm/__main__.py`. For example:

```bash
uv run python3 -m eval_converters.helm --log_path tests/data/helm/commonsense:dataset=hellaswag,method=multiple_choice_joint,model=eleutherai_pythia-1b-v0
```

The automatic conversion script requires following files generated by HELM to work correctly:
- per_instance_stats.json
- run_spec.json
- scenario_state.json
- scenario.json
- stats.json

Full manual for conversion of your own HELM evaluation log into unified is available below:

```bash
usage: __main__.py [-h] [--log_path LOG_PATH] [--output_dir OUTPUT_DIR]
                   [--source_organization_name SOURCE_ORGANIZATION_NAME]
                   [--evaluator_relationship {first_party,third_party,collaborative,other}]
                   [--source_organization_url SOURCE_ORGANIZATION_URL]
                   [--source_organization_logo_url SOURCE_ORGANIZATION_LOGO_URL]
                   [--eval_library_name EVAL_LIBRARY_NAME]
                   [--eval_library_version EVAL_LIBRARY_VERSION]

options:
  -h, --help            show this help message and exit
  --log_path LOG_PATH   Path to directory with single evaluaion or multiple
                        evaluations to convert
  --output_dir OUTPUT_DIR
  --source_organization_name SOURCE_ORGANIZATION_NAME
                        Orgnization which pushed evaluation.
  --evaluator_relationship {first_party,third_party,collaborative,other}
                        Relationship of evaluation author to the model
  --source_organization_url SOURCE_ORGANIZATION_URL
  --source_organization_logo_url SOURCE_ORGANIZATION_LOGO_URL
  --eval_library_name EVAL_LIBRARY_NAME
                        Name of the evaluation library (e.g. inspect_ai,
                        lm_eval, helm)
  --eval_library_version EVAL_LIBRARY_VERSION
                        Version of the evaluation library
```

## lm-eval-harness

The conversion script from `lm-eval-harness` evaluation logs to the unified schema can be run using `eval_converters/lm_eval/__main__.py`.

Using the `--log_path` argument, you can run a command like this:

```bash
uv run python -m eval_converters.lm_eval --log_path tests/data/lm_eval/results_2026-01-21T03-44-18.458309.json
```


Full manual for conversion of your own lm-eval evaluation log into unified is available below:

```bash
usage: __main__.py [-h] --log_path LOG_PATH [--output_dir OUTPUT_DIR]
                   [--source_organization_name SOURCE_ORGANIZATION_NAME]
                   [--evaluator_relationship {first_party,third_party,collaborative,other}]
                   [--source_organization_url SOURCE_ORGANIZATION_URL]
                   [--source_organization_logo_url SOURCE_ORGANIZATION_LOGO_URL]
                   [--include_samples] [--inference_engine INFERENCE_ENGINE]
                   [--inference_engine_version INFERENCE_ENGINE_VERSION]
                   [--eval_library_name EVAL_LIBRARY_NAME]
                   [--eval_library_version EVAL_LIBRARY_VERSION]

Convert lm-evaluation-harness output to every_eval_ever format

options:
  -h, --help            show this help message and exit
  --log_path LOG_PATH   Path to results JSON file or directory containing
                        results files
  --output_dir OUTPUT_DIR
                        Output directory for converted files
  --source_organization_name SOURCE_ORGANIZATION_NAME
                        Name of the organization that ran the evaluation
  --evaluator_relationship {first_party,third_party,collaborative,other}
                        Relationship of the evaluator to the model
  --source_organization_url SOURCE_ORGANIZATION_URL
                        URL of the source organization
  --source_organization_logo_url SOURCE_ORGANIZATION_LOGO_URL
                        Logo of the source organization
  --include_samples     Include instance-level sample data (requires
                        --log_samples in original eval)
  --inference_engine INFERENCE_ENGINE
                        Override inference engine name (e.g. 'vllm',
                        'transformers'). Auto-detected from model type when
                        possible.
  --inference_engine_version INFERENCE_ENGINE_VERSION
                        Inference engine version (e.g. '0.6.0'). Not available
                        from lm-eval logs, so must be provided manually.
  --eval_library_name EVAL_LIBRARY_NAME
                        Name of the evaluation library (e.g. inspect_ai,
                        lm_eval, helm)
  --eval_library_version EVAL_LIBRARY_VERSION
                        Version of the evaluation library
```